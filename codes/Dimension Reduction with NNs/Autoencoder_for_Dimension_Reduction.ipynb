{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Filename: Autoencoder_for_Dimension_Reduction.ipynb\n",
    "#\n",
    "# Purpose: To learn an autoencoder for dimension reduction of tf-idf feature vectors\n",
    "#\n",
    "# Author(s): Bobby (Robert) Lumpkin\n",
    "#\n",
    "# Library Dependencies: numpy, pandas, tensorflow, bpmll\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Learning for TF-IDF Feature Vectors (Non-linear Dimension Reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.adapt import MLkNN\n",
    "import sys\n",
    "sys.path.append('../ThresholdFunctionLearning')    ## Append path to the ThresholdFunctionLearning directory to the interpreters\n",
    "                                                   ## search path\n",
    "from threshold_learning import predict_test_labels_binary    ## Import the 'predict_test_labels_binary()' function from the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>threats/impacts</th>\n",
       "      <th>responses/actions</th>\n",
       "      <th>severity</th>\n",
       "      <th>susceptibility</th>\n",
       "      <th>self-efficacy</th>\n",
       "      <th>external-efficacy</th>\n",
       "      <th>response efficacy</th>\n",
       "      <th>public health</th>\n",
       "      <th>...</th>\n",
       "      <th>prosper</th>\n",
       "      <th>preview</th>\n",
       "      <th>moor</th>\n",
       "      <th>coverag</th>\n",
       "      <th>glow</th>\n",
       "      <th>profil</th>\n",
       "      <th>clash</th>\n",
       "      <th>incumb</th>\n",
       "      <th>frequent</th>\n",
       "      <th>unfound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>214236</td>\n",
       "      <td>MURPHY: Again Martha we are defacto staying at...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214232</td>\n",
       "      <td>GOV. PHIL MURPHY, (D-NJ): Yes. Good to be back...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214266</td>\n",
       "      <td>BEAUMONT (ON SCREEN UPPER LEFT - \"FRIDAY MARCH...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214246</td>\n",
       "      <td>But in the meantime, my message to Louisiana i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214238</td>\n",
       "      <td>MURPHY: Yeah listen, we had gotten another shi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   para_id                                          full_text  \\\n",
       "0   214236  MURPHY: Again Martha we are defacto staying at...   \n",
       "1   214232  GOV. PHIL MURPHY, (D-NJ): Yes. Good to be back...   \n",
       "2   214266  BEAUMONT (ON SCREEN UPPER LEFT - \"FRIDAY MARCH...   \n",
       "3   214246  But in the meantime, my message to Louisiana i...   \n",
       "4   214238  MURPHY: Yeah listen, we had gotten another shi...   \n",
       "\n",
       "   threats/impacts  responses/actions  severity  susceptibility  \\\n",
       "0                1                  1         0               1   \n",
       "1                1                  1         1               1   \n",
       "2                0                  1         0               0   \n",
       "3                1                  1         1               0   \n",
       "4                0                  1         0               0   \n",
       "\n",
       "   self-efficacy  external-efficacy  response efficacy  public health  ...  \\\n",
       "0              1                  0                  1              1  ...   \n",
       "1              1                  0                  1              1  ...   \n",
       "2              0                  1                  0              1  ...   \n",
       "3              1                  0                  1              1  ...   \n",
       "4              0                  1                  0              1  ...   \n",
       "\n",
       "   prosper  preview  moor  coverag  glow  profil  clash  incumb  frequent  \\\n",
       "0      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "1      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "2      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "3      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "4      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "\n",
       "  unfound  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "\n",
       "[5 rows x 2119 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load 'content_paragraphs_ready.csv' into a pandas dataframe\n",
    "data_filepath = \"..\\..\\dataset\\content_paragraphs_ready.csv\"\n",
    "paragraph_data = pd.read_csv(data_filepath)\n",
    "paragraph_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>murphi</th>\n",
       "      <th>martha</th>\n",
       "      <th>defacto</th>\n",
       "      <th>stay</th>\n",
       "      <th>home</th>\n",
       "      <th>state</th>\n",
       "      <th>million</th>\n",
       "      <th>us</th>\n",
       "      <th>youï¿½</th>\n",
       "      <th>...</th>\n",
       "      <th>susceptibility</th>\n",
       "      <th>self-efficacy</th>\n",
       "      <th>external-efficacy</th>\n",
       "      <th>response efficacy</th>\n",
       "      <th>public health</th>\n",
       "      <th>economy</th>\n",
       "      <th>education</th>\n",
       "      <th>political evaluation</th>\n",
       "      <th>racial conflict</th>\n",
       "      <th>international ralations/foreign policies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text1</td>\n",
       "      <td>1.684247</td>\n",
       "      <td>1.348455</td>\n",
       "      <td>2.161368</td>\n",
       "      <td>3.118616</td>\n",
       "      <td>3.016311</td>\n",
       "      <td>0.91833</td>\n",
       "      <td>1.207125</td>\n",
       "      <td>1.383217</td>\n",
       "      <td>1.763428</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text2</td>\n",
       "      <td>1.684247</td>\n",
       "      <td>1.348455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.67332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.763428</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text5</td>\n",
       "      <td>1.684247</td>\n",
       "      <td>1.348455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.383217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id    murphi    martha   defacto      stay      home    state   million  \\\n",
       "0  text1  1.684247  1.348455  2.161368  3.118616  3.016311  0.91833  1.207125   \n",
       "1  text2  1.684247  1.348455  0.000000  0.000000  0.000000  3.67332  0.000000   \n",
       "2  text3  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "3  text4  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "4  text5  1.684247  1.348455  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "\n",
       "         us      youï¿½  ...  susceptibility  self-efficacy  external-efficacy  \\\n",
       "0  1.383217  1.763428  ...               1              1                  0   \n",
       "1  0.000000  1.763428  ...               1              1                  0   \n",
       "2  0.000000  0.000000  ...               0              0                  1   \n",
       "3  0.000000  0.000000  ...               0              1                  0   \n",
       "4  1.383217  0.000000  ...               0              0                  1   \n",
       "\n",
       "   response efficacy  public health  economy  education  political evaluation  \\\n",
       "0                  1              1        0          0                     0   \n",
       "1                  1              1        0          0                     0   \n",
       "2                  0              1        0          0                     0   \n",
       "3                  1              1        0          0                     0   \n",
       "4                  0              1        0          0                     0   \n",
       "\n",
       "   racial conflict  international ralations/foreign policies  \n",
       "0                0                                         0  \n",
       "1                0                                         0  \n",
       "2                0                                         0  \n",
       "3                0                                         0  \n",
       "4                0                                         0  \n",
       "\n",
       "[5 rows x 2108 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Keep only 'doc_id', the label columns, and the tf-idf columns\n",
    "tfidf_colnames = list(paragraph_data.columns[25:])\n",
    "label_columns = list(paragraph_data.columns[2:15])\n",
    "cols_toKeep = ['doc_id']\n",
    "cols_toKeep.extend(tfidf_colnames)\n",
    "cols_toKeep.extend(label_columns)\n",
    "paragraph_data = paragraph_data[cols_toKeep]\n",
    "paragraph_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the X and Y train and test matrices\n",
    "X = paragraph_data[tfidf_colnames].to_numpy().astype(float)\n",
    "Y = paragraph_data[label_columns].to_numpy().astype(float)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2094"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 62\n",
    "visible_dim = X.shape[1]\n",
    "\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, latent_dim, visible_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim   \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(latent_dim * 2, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(latent_dim, activation = 'relu'),\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(latent_dim * 2, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(visible_dim, activation = 'sigmoid'),\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim, visible_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer = 'adam', loss = tf.keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0429 - val_loss: 0.0410\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0429 - val_loss: 0.0411\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0429 - val_loss: 0.0410\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0429 - val_loss: 0.0410\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0428 - val_loss: 0.0411\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0428 - val_loss: 0.0410\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.0427 - val_loss: 0.0410\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0427 - val_loss: 0.0410\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0427 - val_loss: 0.0410\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0426 - val_loss: 0.0410\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0426 - val_loss: 0.0410\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0425 - val_loss: 0.0410\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0425 - val_loss: 0.0410\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0425 - val_loss: 0.0410\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0424 - val_loss: 0.0410\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0424 - val_loss: 0.0410\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0423 - val_loss: 0.0409\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0422 - val_loss: 0.0410\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0423 - val_loss: 0.0410\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0422 - val_loss: 0.0410\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0421 - val_loss: 0.0409\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0421 - val_loss: 0.0409\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0421 - val_loss: 0.0409\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0420 - val_loss: 0.0409\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0420 - val_loss: 0.0409\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0420 - val_loss: 0.0409\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0419 - val_loss: 0.0409\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0418 - val_loss: 0.0409\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0418 - val_loss: 0.0409\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0418 - val_loss: 0.0409\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0418 - val_loss: 0.0409\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0417 - val_loss: 0.0409\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0417 - val_loss: 0.0409\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0416 - val_loss: 0.0409\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0415 - val_loss: 0.0408\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0415 - val_loss: 0.0408\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0414 - val_loss: 0.0409\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0414 - val_loss: 0.0408\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0414 - val_loss: 0.0408\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0414 - val_loss: 0.0408\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0414 - val_loss: 0.0408\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0414 - val_loss: 0.0408\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0413 - val_loss: 0.0408\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0412 - val_loss: 0.0408\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0412 - val_loss: 0.0408\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0412 - val_loss: 0.0408\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0411 - val_loss: 0.0408\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0411 - val_loss: 0.0408\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0411 - val_loss: 0.0408\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0410 - val_loss: 0.0408\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0410 - val_loss: 0.0408\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0410 - val_loss: 0.0408\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0409 - val_loss: 0.0408\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0409 - val_loss: 0.0408\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0409 - val_loss: 0.0408\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0409 - val_loss: 0.0408\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0408 - val_loss: 0.0408\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0408 - val_loss: 0.0407\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0408 - val_loss: 0.0408\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0408 - val_loss: 0.0407\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0407 - val_loss: 0.0407\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0407 - val_loss: 0.0408\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0407 - val_loss: 0.0407\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0406 - val_loss: 0.0407\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0407 - val_loss: 0.0407\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0406 - val_loss: 0.0407\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0406 - val_loss: 0.0407\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0406 - val_loss: 0.0407\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0406 - val_loss: 0.0407\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0406 - val_loss: 0.0407\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0405 - val_loss: 0.0407\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0405 - val_loss: 0.0407\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0405 - val_loss: 0.0407\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0405 - val_loss: 0.0407\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0405 - val_loss: 0.0407\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0404 - val_loss: 0.0407\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0404 - val_loss: 0.0407\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0404 - val_loss: 0.0407\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0404 - val_loss: 0.0407\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0404 - val_loss: 0.0407\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0404 - val_loss: 0.0407\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0404 - val_loss: 0.0407\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0404 - val_loss: 0.0407\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0404 - val_loss: 0.0407\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.0402 - val_loss: 0.0407\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0403 - val_loss: 0.0407\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0402 - val_loss: 0.0407\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0402 - val_loss: 0.0407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16833950cd0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs = 200,\n",
    "                shuffle = True,\n",
    "                validation_data = (X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tfidfs = autoencoder.encoder(X).numpy()\n",
    "decoded_tfidfs = autoencoder.decoder(encoded_tfidfs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 62)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_tfidfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write the encoded data to a csv file\n",
    "encoded_df = pd.DataFrame(encoded_tfidfs)\n",
    "encoded_df.to_csv('encoded_tfidfs.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Encoded Data for ML-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train, encoded_test, Y_train, Y_test = train_test_split(encoded_tfidfs, Y, test_size = 0.33, random_state = 321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_neighbors=3 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hamming loss for the training data is 0.093\n",
      "The Hamming loss for the test data is 0.174\n"
     ]
    }
   ],
   "source": [
    "classifier = MLkNN(k = 3)\n",
    "classifier.fit(encoded_train, Y_train)\n",
    "y_train_pred = classifier.predict(encoded_train)\n",
    "y_train_pred_array = y_train_pred.toarray()\n",
    "y_test_pred = classifier.predict(encoded_test)\n",
    "y_test_pred_array = y_test_pred.toarray()\n",
    "\n",
    "print (f\"The Hamming loss for the training data is {metrics.hamming_loss(Y_train, y_train_pred_array):.3f}\")\n",
    "print (f\"The Hamming loss for the test data is {metrics.hamming_loss(Y_test, y_test_pred_array):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'k': range(1,5), 's': [1.0, 1.5, 2.5, 5.0, 10.0]}  \n",
    "#By default, the Hamming loss as an option is not provided in the scoring string options. So, we will make the Hamming loss funciton as a scorer and use that. \n",
    "hamming_scorer = metrics.make_scorer(metrics.hamming_loss)\n",
    "\n",
    "clf = GridSearchCV(MLkNN(), parameters, scoring = hamming_scorer, cv = 5, verbose = 1)\n",
    "clf.fit(encoded_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Params</th>\n",
       "      <th>Mean out-of-bag Hamming loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'k': 1, 's': 1.0}</td>\n",
       "      <td>0.192723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'k': 1, 's': 1.5}</td>\n",
       "      <td>0.192723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'k': 1, 's': 2.5}</td>\n",
       "      <td>0.192723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'k': 1, 's': 5.0}</td>\n",
       "      <td>0.192723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'k': 1, 's': 10.0}</td>\n",
       "      <td>0.192723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'k': 2, 's': 1.0}</td>\n",
       "      <td>0.203810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'k': 2, 's': 1.5}</td>\n",
       "      <td>0.201837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'k': 2, 's': 2.5}</td>\n",
       "      <td>0.200260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'k': 2, 's': 5.0}</td>\n",
       "      <td>0.188394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'k': 2, 's': 10.0}</td>\n",
       "      <td>0.183671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'k': 3, 's': 1.0}</td>\n",
       "      <td>0.171390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'k': 3, 's': 1.5}</td>\n",
       "      <td>0.171390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'k': 3, 's': 2.5}</td>\n",
       "      <td>0.171390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'k': 3, 's': 5.0}</td>\n",
       "      <td>0.172189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'k': 3, 's': 10.0}</td>\n",
       "      <td>0.170964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'k': 4, 's': 1.0}</td>\n",
       "      <td>0.183629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'k': 4, 's': 1.5}</td>\n",
       "      <td>0.181262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'k': 4, 's': 2.5}</td>\n",
       "      <td>0.181252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'k': 4, 's': 5.0}</td>\n",
       "      <td>0.177702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'k': 4, 's': 10.0}</td>\n",
       "      <td>0.177318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Params  Mean out-of-bag Hamming loss\n",
       "0    {'k': 1, 's': 1.0}                      0.192723\n",
       "1    {'k': 1, 's': 1.5}                      0.192723\n",
       "2    {'k': 1, 's': 2.5}                      0.192723\n",
       "3    {'k': 1, 's': 5.0}                      0.192723\n",
       "4   {'k': 1, 's': 10.0}                      0.192723\n",
       "5    {'k': 2, 's': 1.0}                      0.203810\n",
       "6    {'k': 2, 's': 1.5}                      0.201837\n",
       "7    {'k': 2, 's': 2.5}                      0.200260\n",
       "8    {'k': 2, 's': 5.0}                      0.188394\n",
       "9   {'k': 2, 's': 10.0}                      0.183671\n",
       "10   {'k': 3, 's': 1.0}                      0.171390\n",
       "11   {'k': 3, 's': 1.5}                      0.171390\n",
       "12   {'k': 3, 's': 2.5}                      0.171390\n",
       "13   {'k': 3, 's': 5.0}                      0.172189\n",
       "14  {'k': 3, 's': 10.0}                      0.170964\n",
       "15   {'k': 4, 's': 1.0}                      0.183629\n",
       "16   {'k': 4, 's': 1.5}                      0.181262\n",
       "17   {'k': 4, 's': 2.5}                      0.181252\n",
       "18   {'k': 4, 's': 5.0}                      0.177702\n",
       "19  {'k': 4, 's': 10.0}                      0.177318"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 3, 's': 10.0} 0.1709643932315997\n"
     ]
    }
   ],
   "source": [
    "best_index = np.argmin(clf.cv_results_[\"mean_test_score\"])\n",
    "best_parameters = clf.cv_results_[\"params\"][best_index]\n",
    "\n",
    "df_CV = pd.DataFrame(columns=[\"Params\", \"Mean out-of-bag Hamming loss\"])\n",
    "df_CV[\"Params\"] = clf.cv_results_[\"params\"]\n",
    "df_CV[ \"Mean out-of-bag Hamming loss\"] = clf.cv_results_[\"mean_test_score\"]\n",
    "display(df_CV)\n",
    "print(best_parameters, np.min(clf.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_neighbors=3 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: The Hamming loss training data is 0.103\n",
      "Best parameters: The Hamming loss test data is 0.175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_neighbors=3 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters with threshold function learning: Hamming loss Test set is 0.21394230769230768\n"
     ]
    }
   ],
   "source": [
    "#Threshold learning\n",
    "#using the best parameters from the cross-validation with original threshold. \n",
    "classifier_best = MLkNN(k = 3, s = 10)\n",
    "classifier_best.fit(encoded_train, Y_train)\n",
    "y_train_pred_best = classifier_best.predict(encoded_train)\n",
    "y_train_pred_best_array = y_train_pred_best.toarray()\n",
    "y_test_pred_best = classifier_best.predict(encoded_test)\n",
    "y_test_pred_best_array = y_test_pred_best.toarray()\n",
    "\n",
    "print (f\"Best parameters: The Hamming loss training data is {metrics.hamming_loss(Y_train, y_train_pred_best_array):.3f}\")\n",
    "print (f\"Best parameters: The Hamming loss test data is {metrics.hamming_loss(Y_test, y_test_pred_best_array):.3f}\")\n",
    "\n",
    "#using the best parameters\n",
    "classifier_CV = MLkNN(k = 3, s = 2.5)\n",
    "classifier_CV.fit(encoded_train, Y_train)\n",
    "y_train_pred_proba = classifier_CV.predict_proba(encoded_train)\n",
    "y_train_pred_proba_array = y_train_pred_proba.toarray()\n",
    "y_test_pred_proba = classifier_CV.predict_proba(encoded_test)\n",
    "y_test_pred_proba_array = y_test_pred_proba.toarray()\n",
    "\n",
    "t_range = (0, 1)\n",
    "\n",
    "test_labels_binary, threshold_function = predict_test_labels_binary(y_train_pred_proba_array, Y_train, y_test_pred_proba_array, t_range)\n",
    "print (f\"Best parameters with threshold function learning: Hamming loss Test set is {metrics.hamming_loss(Y_test, test_labels_binary)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
