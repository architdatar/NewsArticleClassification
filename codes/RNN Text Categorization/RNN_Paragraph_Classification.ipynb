{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Filename: RNN_Paragraph_Classification.ipynb\n",
    "#\n",
    "# Purpose: Multi-label Text-categorization, using recurrent neural networks, for paragraph-level\n",
    "#          data as part of STAT 6500 final project.\n",
    "\n",
    "# Author(s): Bobby (Robert) Lumpkin\n",
    "#\n",
    "# Library Dependencies: numpy, pandas, tensorflow, bpmll\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph Classification Using RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from bpmll import bp_mll_loss\n",
    "import sklearn_json as skljson\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import sys\n",
    "sys.path.append('../ThresholdFunctionLearning')    ## Append path to the ThresholdFunctionLearning directory to the interpreters\n",
    "                                                   ## search path\n",
    "from threshold_learning import predict_test_labels_binary    ## Import the 'predict_test_labels_binary()' function from the \n",
    "                                                             ## threshold_learning library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>threats/impacts</th>\n",
       "      <th>responses/actions</th>\n",
       "      <th>severity</th>\n",
       "      <th>susceptibility</th>\n",
       "      <th>self-efficacy</th>\n",
       "      <th>external-efficacy</th>\n",
       "      <th>response efficacy</th>\n",
       "      <th>public health</th>\n",
       "      <th>...</th>\n",
       "      <th>prosper</th>\n",
       "      <th>preview</th>\n",
       "      <th>moor</th>\n",
       "      <th>coverag</th>\n",
       "      <th>glow</th>\n",
       "      <th>profil</th>\n",
       "      <th>clash</th>\n",
       "      <th>incumb</th>\n",
       "      <th>frequent</th>\n",
       "      <th>unfound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>214236</td>\n",
       "      <td>MURPHY: Again Martha we are defacto staying at...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214232</td>\n",
       "      <td>GOV. PHIL MURPHY, (D-NJ): Yes. Good to be back...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214266</td>\n",
       "      <td>BEAUMONT (ON SCREEN UPPER LEFT - \"FRIDAY MARCH...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214246</td>\n",
       "      <td>But in the meantime, my message to Louisiana i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214238</td>\n",
       "      <td>MURPHY: Yeah listen, we had gotten another shi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   para_id                                          full_text  \\\n",
       "0   214236  MURPHY: Again Martha we are defacto staying at...   \n",
       "1   214232  GOV. PHIL MURPHY, (D-NJ): Yes. Good to be back...   \n",
       "2   214266  BEAUMONT (ON SCREEN UPPER LEFT - \"FRIDAY MARCH...   \n",
       "3   214246  But in the meantime, my message to Louisiana i...   \n",
       "4   214238  MURPHY: Yeah listen, we had gotten another shi...   \n",
       "\n",
       "   threats/impacts  responses/actions  severity  susceptibility  \\\n",
       "0                1                  1         0               1   \n",
       "1                1                  1         1               1   \n",
       "2                0                  1         0               0   \n",
       "3                1                  1         1               0   \n",
       "4                0                  1         0               0   \n",
       "\n",
       "   self-efficacy  external-efficacy  response efficacy  public health  ...  \\\n",
       "0              1                  0                  1              1  ...   \n",
       "1              1                  0                  1              1  ...   \n",
       "2              0                  1                  0              1  ...   \n",
       "3              1                  0                  1              1  ...   \n",
       "4              0                  1                  0              1  ...   \n",
       "\n",
       "   prosper  preview  moor  coverag  glow  profil  clash  incumb  frequent  \\\n",
       "0      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "1      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "2      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "3      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "4      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "\n",
       "  unfound  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "\n",
       "[5 rows x 2119 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load 'content_paragraphs_ready.csv' into a pandas dataframe\n",
    "data_filepath = \"..\\..\\dataset\\content_paragraphs_ready.csv\"\n",
    "paragraph_data = pd.read_csv(data_filepath)\n",
    "paragraph_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>threats/impacts</th>\n",
       "      <th>responses/actions</th>\n",
       "      <th>severity</th>\n",
       "      <th>susceptibility</th>\n",
       "      <th>self-efficacy</th>\n",
       "      <th>external-efficacy</th>\n",
       "      <th>response efficacy</th>\n",
       "      <th>public health</th>\n",
       "      <th>economy</th>\n",
       "      <th>education</th>\n",
       "      <th>political evaluation</th>\n",
       "      <th>racial conflict</th>\n",
       "      <th>international ralations/foreign policies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>214236</td>\n",
       "      <td>MURPHY: Again Martha we are defacto staying at...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214232</td>\n",
       "      <td>GOV. PHIL MURPHY, (D-NJ): Yes. Good to be back...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214266</td>\n",
       "      <td>BEAUMONT (ON SCREEN UPPER LEFT - \"FRIDAY MARCH...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214246</td>\n",
       "      <td>But in the meantime, my message to Louisiana i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214238</td>\n",
       "      <td>MURPHY: Yeah listen, we had gotten another shi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   para_id                                          full_text  \\\n",
       "0   214236  MURPHY: Again Martha we are defacto staying at...   \n",
       "1   214232  GOV. PHIL MURPHY, (D-NJ): Yes. Good to be back...   \n",
       "2   214266  BEAUMONT (ON SCREEN UPPER LEFT - \"FRIDAY MARCH...   \n",
       "3   214246  But in the meantime, my message to Louisiana i...   \n",
       "4   214238  MURPHY: Yeah listen, we had gotten another shi...   \n",
       "\n",
       "   threats/impacts  responses/actions  severity  susceptibility  \\\n",
       "0                1                  1         0               1   \n",
       "1                1                  1         1               1   \n",
       "2                0                  1         0               0   \n",
       "3                1                  1         1               0   \n",
       "4                0                  1         0               0   \n",
       "\n",
       "   self-efficacy  external-efficacy  response efficacy  public health  \\\n",
       "0              1                  0                  1              1   \n",
       "1              1                  0                  1              1   \n",
       "2              0                  1                  0              1   \n",
       "3              1                  0                  1              1   \n",
       "4              0                  1                  0              1   \n",
       "\n",
       "   economy  education  political evaluation  racial conflict  \\\n",
       "0        0          0                     0                0   \n",
       "1        0          0                     0                0   \n",
       "2        0          0                     0                0   \n",
       "3        0          0                     0                0   \n",
       "4        0          0                     0                0   \n",
       "\n",
       "   international ralations/foreign policies  \n",
       "0                                         0  \n",
       "1                                         0  \n",
       "2                                         0  \n",
       "3                                         0  \n",
       "4                                         0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Keep only the paragraph id, paragraph text, and labels\n",
    "to_keep = paragraph_data.columns[0:15]\n",
    "#to_keep\n",
    "paragraph_data = paragraph_data[to_keep]\n",
    "paragraph_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "# https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate/34294022\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"https?://(\\S+|www)\\.\\S+\")\n",
    "for t in paragraph_data.full_text:\n",
    "    matches = pattern.findall(t)\n",
    "    for match in matches:\n",
    "        print(t)\n",
    "        print(match)\n",
    "        print(pattern.sub(r\"\", t))\n",
    "    if len(matches) > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_data[\"full_text\"] = paragraph_data.full_text.map(remove_URL) # map(lambda x: remove_URL(x))\n",
    "paragraph_data[\"full_text\"] = paragraph_data.full_text.map(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine\n",
    "# has been programmed to ignore, both when indexing entries for searching and when retrieving them \n",
    "# as the result of a search query.\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_data[\"full_text\"] = paragraph_data.full_text.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "counter = counter_word(paragraph_data.full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 69), ('people', 61), ('trump', 49), ('new', 48), ('tests', 47)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unique_words = len(counter)\n",
    "counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2711"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the X and Y train and test matrices\n",
    "covariate_cols = ['full_text']\n",
    "label_cols = paragraph_data.columns.difference(['para_id'] + covariate_cols)\n",
    "\n",
    "X = paragraph_data.full_text.to_numpy()\n",
    "Y = paragraph_data[label_cols].to_numpy().astype(float)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "random.seed(123)\n",
    "\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words = num_unique_words)\n",
    "tokenizer.fit_on_texts(X_train) # fit only to training\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "johnson asked 70 local savannah religious leaders keep worship centers closed none leaders said would reopen johnson told religious leaders understood financial burden religious institutions closed said reach god without going building”\n",
      "[478, 72, 829, 830, 831, 316, 178, 114, 832, 228, 229, 833, 178, 1, 23, 230, 478, 57, 316, 178, 834, 144, 835, 316, 836, 229, 1, 479, 837, 179, 3, 838]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_par_length = 0\n",
    "for par in train_sequences:\n",
    "    if len(par) > max_par_length:\n",
    "        max_par_length = len(par)\n",
    "        \n",
    "max_par_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((194, 100), (96, 100))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Max number of words in a sequence\n",
    "max_length = 100\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen = max_length, padding = \"post\", truncating = \"post\")\n",
    "test_padded = pad_sequences(test_sequences, maxlen = max_length, padding = \"post\", truncating = \"post\")\n",
    "train_padded.shape, test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export train and test data to json file\n",
    "RNN_data_dict = {'train_padded' : train_padded.tolist(), \n",
    "                 'test_padded' : test_padded.tolist(),\n",
    "                 'Y_train' : Y_train.tolist(), \n",
    "                 'Y_test' : Y_test.tolist()}\n",
    "\n",
    "RNN_data_dict_json = json.dumps(RNN_data_dict)\n",
    "with open(\"RNN_data_dict.json\", \"w\") as outfile: \n",
    "    json.dump(RNN_data_dict_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reversing the indices\n",
    "# flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n",
    "\n",
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[478, 72, 829, 830, 831, 316, 178, 114, 832, 228, 229, 833, 178, 1, 23, 230, 478, 57, 316, 178, 834, 144, 835, 316, 836, 229, 1, 479, 837, 179, 3, 838]\n",
      "johnson asked 70 local savannah religious leaders keep worship centers closed none leaders said would reopen johnson told religious leaders understood financial burden religious institutions closed said reach god without going building”\n"
     ]
    }
   ],
   "source": [
    "decoded_text = decode(train_sequences[0])\n",
    "\n",
    "print(train_sequences[0])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the simple RNN architecture\n",
    "num_labels = len(label_cols)\n",
    "\n",
    "model_simpleRNN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(num_unique_words, 32, input_length = max_length),\n",
    "    tf.keras.layers.SimpleRNN(16, return_sequences = False, return_state = False),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_labels, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "metric = tfa.metrics.HammingLoss(mode = 'multilabel', threshold = 0.5)\n",
    "model_simpleRNN.compile(loss = 'categorical_crossentropy', optimizer = optim, metrics = metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 - 2s - loss: 7.6402 - hamming_loss: 0.4508 - val_loss: 7.8881 - val_hamming_loss: 0.4423\n",
      "Epoch 2/100\n",
      "7/7 - 0s - loss: 7.6057 - hamming_loss: 0.4374 - val_loss: 7.8699 - val_hamming_loss: 0.4503\n",
      "Epoch 3/100\n",
      "7/7 - 0s - loss: 7.5747 - hamming_loss: 0.4302 - val_loss: 7.8509 - val_hamming_loss: 0.4551\n",
      "Epoch 4/100\n",
      "7/7 - 0s - loss: 7.5452 - hamming_loss: 0.4266 - val_loss: 7.8327 - val_hamming_loss: 0.4503\n",
      "Epoch 5/100\n",
      "7/7 - 0s - loss: 7.5174 - hamming_loss: 0.4207 - val_loss: 7.8150 - val_hamming_loss: 0.4407\n",
      "Epoch 6/100\n",
      "7/7 - 0s - loss: 7.4897 - hamming_loss: 0.4092 - val_loss: 7.7966 - val_hamming_loss: 0.4327\n",
      "Epoch 7/100\n",
      "7/7 - 0s - loss: 7.4609 - hamming_loss: 0.4033 - val_loss: 7.7773 - val_hamming_loss: 0.4359\n",
      "Epoch 8/100\n",
      "7/7 - 0s - loss: 7.4315 - hamming_loss: 0.3973 - val_loss: 7.7573 - val_hamming_loss: 0.4303\n",
      "Epoch 9/100\n",
      "7/7 - 0s - loss: 7.4005 - hamming_loss: 0.3894 - val_loss: 7.7378 - val_hamming_loss: 0.4239\n",
      "Epoch 10/100\n",
      "7/7 - 0s - loss: 7.3696 - hamming_loss: 0.3882 - val_loss: 7.7201 - val_hamming_loss: 0.4143\n",
      "Epoch 11/100\n",
      "7/7 - 0s - loss: 7.3370 - hamming_loss: 0.3918 - val_loss: 7.6999 - val_hamming_loss: 0.4095\n",
      "Epoch 12/100\n",
      "7/7 - 0s - loss: 7.3020 - hamming_loss: 0.3870 - val_loss: 7.6820 - val_hamming_loss: 0.4079\n",
      "Epoch 13/100\n",
      "7/7 - 0s - loss: 7.2682 - hamming_loss: 0.3846 - val_loss: 7.6658 - val_hamming_loss: 0.4079\n",
      "Epoch 14/100\n",
      "7/7 - 0s - loss: 7.2312 - hamming_loss: 0.3862 - val_loss: 7.6518 - val_hamming_loss: 0.4071\n",
      "Epoch 15/100\n",
      "7/7 - 0s - loss: 7.1934 - hamming_loss: 0.3862 - val_loss: 7.6442 - val_hamming_loss: 0.4054\n",
      "Epoch 16/100\n",
      "7/7 - 0s - loss: 7.1578 - hamming_loss: 0.3902 - val_loss: 7.6425 - val_hamming_loss: 0.3982\n",
      "Epoch 17/100\n",
      "7/7 - 0s - loss: 7.1240 - hamming_loss: 0.3914 - val_loss: 7.6437 - val_hamming_loss: 0.3838\n",
      "Epoch 18/100\n",
      "7/7 - 0s - loss: 7.0898 - hamming_loss: 0.4001 - val_loss: 7.6363 - val_hamming_loss: 0.3918\n",
      "Epoch 19/100\n",
      "7/7 - 0s - loss: 7.0493 - hamming_loss: 0.4076 - val_loss: 7.6261 - val_hamming_loss: 0.3894\n",
      "Epoch 20/100\n",
      "7/7 - 0s - loss: 7.0058 - hamming_loss: 0.4080 - val_loss: 7.6140 - val_hamming_loss: 0.3902\n",
      "Epoch 21/100\n",
      "7/7 - 0s - loss: 6.9725 - hamming_loss: 0.4096 - val_loss: 7.6129 - val_hamming_loss: 0.3990\n",
      "Epoch 22/100\n",
      "7/7 - 0s - loss: 6.9464 - hamming_loss: 0.4100 - val_loss: 7.6038 - val_hamming_loss: 0.4046\n",
      "Epoch 23/100\n",
      "7/7 - 0s - loss: 6.9113 - hamming_loss: 0.4100 - val_loss: 7.5920 - val_hamming_loss: 0.4038\n",
      "Epoch 24/100\n",
      "7/7 - 0s - loss: 6.8791 - hamming_loss: 0.4064 - val_loss: 7.5956 - val_hamming_loss: 0.4071\n",
      "Epoch 25/100\n",
      "7/7 - 0s - loss: 6.8521 - hamming_loss: 0.4084 - val_loss: 7.5851 - val_hamming_loss: 0.4062\n",
      "Epoch 26/100\n",
      "7/7 - 0s - loss: 6.8225 - hamming_loss: 0.4088 - val_loss: 7.5739 - val_hamming_loss: 0.4127\n",
      "Epoch 27/100\n",
      "7/7 - 0s - loss: 6.7947 - hamming_loss: 0.4108 - val_loss: 7.5546 - val_hamming_loss: 0.4111\n",
      "Epoch 28/100\n",
      "7/7 - 0s - loss: 6.7708 - hamming_loss: 0.4128 - val_loss: 7.5508 - val_hamming_loss: 0.4167\n",
      "Epoch 29/100\n",
      "7/7 - 0s - loss: 6.7456 - hamming_loss: 0.4159 - val_loss: 7.5502 - val_hamming_loss: 0.4215\n",
      "Epoch 30/100\n",
      "7/7 - 0s - loss: 6.7266 - hamming_loss: 0.4179 - val_loss: 7.5472 - val_hamming_loss: 0.4223\n",
      "Epoch 31/100\n",
      "7/7 - 0s - loss: 6.6983 - hamming_loss: 0.4159 - val_loss: 7.5419 - val_hamming_loss: 0.4223\n",
      "Epoch 32/100\n",
      "7/7 - 0s - loss: 6.6835 - hamming_loss: 0.4195 - val_loss: 7.5238 - val_hamming_loss: 0.4191\n",
      "Epoch 33/100\n",
      "7/7 - 0s - loss: 6.6588 - hamming_loss: 0.4120 - val_loss: 7.4841 - val_hamming_loss: 0.4223\n",
      "Epoch 34/100\n",
      "7/7 - 0s - loss: 6.6412 - hamming_loss: 0.4116 - val_loss: 7.4807 - val_hamming_loss: 0.4175\n",
      "Epoch 35/100\n",
      "7/7 - 0s - loss: 6.6281 - hamming_loss: 0.4140 - val_loss: 7.4903 - val_hamming_loss: 0.4183\n",
      "Epoch 36/100\n",
      "7/7 - 0s - loss: 6.6235 - hamming_loss: 0.4282 - val_loss: 7.4856 - val_hamming_loss: 0.4135\n",
      "Epoch 37/100\n",
      "7/7 - 0s - loss: 6.5931 - hamming_loss: 0.4223 - val_loss: 7.4716 - val_hamming_loss: 0.4183\n",
      "Epoch 38/100\n",
      "7/7 - 0s - loss: 6.5774 - hamming_loss: 0.4132 - val_loss: 7.4726 - val_hamming_loss: 0.4255\n",
      "Epoch 39/100\n",
      "7/7 - 0s - loss: 6.5698 - hamming_loss: 0.4120 - val_loss: 7.4780 - val_hamming_loss: 0.4239\n",
      "Epoch 40/100\n",
      "7/7 - 0s - loss: 6.5600 - hamming_loss: 0.4239 - val_loss: 7.4679 - val_hamming_loss: 0.4247\n",
      "Epoch 41/100\n",
      "7/7 - 0s - loss: 6.5343 - hamming_loss: 0.4191 - val_loss: 7.4617 - val_hamming_loss: 0.4239\n",
      "Epoch 42/100\n",
      "7/7 - 0s - loss: 6.5255 - hamming_loss: 0.4191 - val_loss: 7.4636 - val_hamming_loss: 0.4183\n",
      "Epoch 43/100\n",
      "7/7 - 0s - loss: 6.5114 - hamming_loss: 0.4191 - val_loss: 7.4507 - val_hamming_loss: 0.4207\n",
      "Epoch 44/100\n",
      "7/7 - 0s - loss: 6.4979 - hamming_loss: 0.4171 - val_loss: 7.4548 - val_hamming_loss: 0.4239\n",
      "Epoch 45/100\n",
      "7/7 - 0s - loss: 6.4949 - hamming_loss: 0.4203 - val_loss: 7.4542 - val_hamming_loss: 0.4223\n",
      "Epoch 46/100\n",
      "7/7 - 0s - loss: 6.4754 - hamming_loss: 0.4247 - val_loss: 7.4551 - val_hamming_loss: 0.4239\n",
      "Epoch 47/100\n",
      "7/7 - 0s - loss: 6.4688 - hamming_loss: 0.4239 - val_loss: 7.4516 - val_hamming_loss: 0.4207\n",
      "Epoch 48/100\n",
      "7/7 - 0s - loss: 6.4544 - hamming_loss: 0.4120 - val_loss: 7.4336 - val_hamming_loss: 0.4167\n",
      "Epoch 49/100\n",
      "7/7 - 0s - loss: 6.4407 - hamming_loss: 0.4159 - val_loss: 7.4355 - val_hamming_loss: 0.4199\n",
      "Epoch 50/100\n",
      "7/7 - 0s - loss: 6.4422 - hamming_loss: 0.4251 - val_loss: 7.4273 - val_hamming_loss: 0.4183\n",
      "Epoch 51/100\n",
      "7/7 - 0s - loss: 6.4242 - hamming_loss: 0.4151 - val_loss: 7.4215 - val_hamming_loss: 0.4127\n",
      "Epoch 52/100\n",
      "7/7 - 0s - loss: 6.4153 - hamming_loss: 0.4171 - val_loss: 7.4319 - val_hamming_loss: 0.4167\n",
      "Epoch 53/100\n",
      "7/7 - 0s - loss: 6.4219 - hamming_loss: 0.4159 - val_loss: 7.4333 - val_hamming_loss: 0.4191\n",
      "Epoch 54/100\n",
      "7/7 - 0s - loss: 6.4034 - hamming_loss: 0.4171 - val_loss: 7.4187 - val_hamming_loss: 0.4127\n",
      "Epoch 55/100\n",
      "7/7 - 0s - loss: 6.3906 - hamming_loss: 0.4159 - val_loss: 7.4263 - val_hamming_loss: 0.4183\n",
      "Epoch 56/100\n",
      "7/7 - 0s - loss: 6.3899 - hamming_loss: 0.4183 - val_loss: 7.4259 - val_hamming_loss: 0.4207\n",
      "Epoch 57/100\n",
      "7/7 - 0s - loss: 6.3792 - hamming_loss: 0.4175 - val_loss: 7.4204 - val_hamming_loss: 0.4207\n",
      "Epoch 58/100\n",
      "7/7 - 0s - loss: 6.3728 - hamming_loss: 0.4199 - val_loss: 7.4164 - val_hamming_loss: 0.4207\n",
      "Epoch 59/100\n",
      "7/7 - 0s - loss: 6.3600 - hamming_loss: 0.4203 - val_loss: 7.4176 - val_hamming_loss: 0.4223\n",
      "Epoch 60/100\n",
      "7/7 - 0s - loss: 6.3628 - hamming_loss: 0.4195 - val_loss: 7.4111 - val_hamming_loss: 0.4199\n",
      "Epoch 61/100\n",
      "7/7 - 0s - loss: 6.3415 - hamming_loss: 0.4155 - val_loss: 7.4117 - val_hamming_loss: 0.4199\n",
      "Epoch 62/100\n",
      "7/7 - 0s - loss: 6.3439 - hamming_loss: 0.4179 - val_loss: 7.4134 - val_hamming_loss: 0.4207\n",
      "Epoch 63/100\n",
      "7/7 - 0s - loss: 6.3302 - hamming_loss: 0.4179 - val_loss: 7.4112 - val_hamming_loss: 0.4231\n",
      "Epoch 64/100\n",
      "7/7 - 0s - loss: 6.3242 - hamming_loss: 0.4179 - val_loss: 7.4122 - val_hamming_loss: 0.4239\n",
      "Epoch 65/100\n",
      "7/7 - 0s - loss: 6.3162 - hamming_loss: 0.4179 - val_loss: 7.4099 - val_hamming_loss: 0.4215\n",
      "Epoch 66/100\n",
      "7/7 - 0s - loss: 6.3104 - hamming_loss: 0.4211 - val_loss: 7.4153 - val_hamming_loss: 0.4263\n",
      "Epoch 67/100\n",
      "7/7 - 0s - loss: 6.3092 - hamming_loss: 0.4195 - val_loss: 7.3989 - val_hamming_loss: 0.4239\n",
      "Epoch 68/100\n",
      "7/7 - 0s - loss: 6.2889 - hamming_loss: 0.4199 - val_loss: 7.4061 - val_hamming_loss: 0.4247\n",
      "Epoch 69/100\n",
      "7/7 - 0s - loss: 6.2999 - hamming_loss: 0.4191 - val_loss: 7.3916 - val_hamming_loss: 0.4215\n",
      "Epoch 70/100\n",
      "7/7 - 0s - loss: 6.2779 - hamming_loss: 0.4179 - val_loss: 7.4016 - val_hamming_loss: 0.4287\n",
      "Epoch 71/100\n",
      "7/7 - 0s - loss: 6.2848 - hamming_loss: 0.4223 - val_loss: 7.3873 - val_hamming_loss: 0.4271\n",
      "Epoch 72/100\n",
      "7/7 - 0s - loss: 6.2669 - hamming_loss: 0.4179 - val_loss: 7.3946 - val_hamming_loss: 0.4343\n",
      "Epoch 73/100\n",
      "7/7 - 0s - loss: 6.2755 - hamming_loss: 0.4235 - val_loss: 7.3925 - val_hamming_loss: 0.4359\n",
      "Epoch 74/100\n",
      "7/7 - 0s - loss: 6.2557 - hamming_loss: 0.4215 - val_loss: 7.3885 - val_hamming_loss: 0.4359\n",
      "Epoch 75/100\n",
      "7/7 - 0s - loss: 6.2465 - hamming_loss: 0.4227 - val_loss: 7.3935 - val_hamming_loss: 0.4359\n",
      "Epoch 76/100\n",
      "7/7 - 0s - loss: 6.2469 - hamming_loss: 0.4207 - val_loss: 7.3912 - val_hamming_loss: 0.4375\n",
      "Epoch 77/100\n",
      "7/7 - 0s - loss: 6.2417 - hamming_loss: 0.4195 - val_loss: 7.4009 - val_hamming_loss: 0.4391\n",
      "Epoch 78/100\n",
      "7/7 - 0s - loss: 6.2399 - hamming_loss: 0.4207 - val_loss: 7.3896 - val_hamming_loss: 0.4439\n",
      "Epoch 79/100\n",
      "7/7 - 0s - loss: 6.2253 - hamming_loss: 0.4231 - val_loss: 7.3992 - val_hamming_loss: 0.4415\n",
      "Epoch 80/100\n",
      "7/7 - 0s - loss: 6.2249 - hamming_loss: 0.4223 - val_loss: 7.3758 - val_hamming_loss: 0.4447\n",
      "Epoch 81/100\n",
      "7/7 - 0s - loss: 6.2075 - hamming_loss: 0.4211 - val_loss: 7.3883 - val_hamming_loss: 0.4471\n",
      "Epoch 82/100\n",
      "7/7 - 0s - loss: 6.2184 - hamming_loss: 0.4255 - val_loss: 7.3615 - val_hamming_loss: 0.4455\n",
      "Epoch 83/100\n",
      "7/7 - 0s - loss: 6.2053 - hamming_loss: 0.4211 - val_loss: 7.3776 - val_hamming_loss: 0.4479\n",
      "Epoch 84/100\n",
      "7/7 - 0s - loss: 6.2032 - hamming_loss: 0.4223 - val_loss: 7.3575 - val_hamming_loss: 0.4479\n",
      "Epoch 85/100\n",
      "7/7 - 0s - loss: 6.1825 - hamming_loss: 0.4179 - val_loss: 7.3701 - val_hamming_loss: 0.4503\n",
      "Epoch 86/100\n",
      "7/7 - 0s - loss: 6.2000 - hamming_loss: 0.4215 - val_loss: 7.3567 - val_hamming_loss: 0.4463\n",
      "Epoch 87/100\n",
      "7/7 - 0s - loss: 6.1912 - hamming_loss: 0.4155 - val_loss: 7.3741 - val_hamming_loss: 0.4543\n",
      "Epoch 88/100\n",
      "7/7 - 0s - loss: 6.2011 - hamming_loss: 0.4259 - val_loss: 7.3541 - val_hamming_loss: 0.4551\n",
      "Epoch 89/100\n",
      "7/7 - 0s - loss: 6.1791 - hamming_loss: 0.4171 - val_loss: 7.3695 - val_hamming_loss: 0.4567\n",
      "Epoch 90/100\n",
      "7/7 - 0s - loss: 6.1847 - hamming_loss: 0.4259 - val_loss: 7.3412 - val_hamming_loss: 0.4495\n",
      "Epoch 91/100\n",
      "7/7 - 0s - loss: 6.1621 - hamming_loss: 0.4227 - val_loss: 7.3568 - val_hamming_loss: 0.4535\n",
      "Epoch 92/100\n",
      "7/7 - 0s - loss: 6.1771 - hamming_loss: 0.4231 - val_loss: 7.3459 - val_hamming_loss: 0.4535\n",
      "Epoch 93/100\n",
      "7/7 - 0s - loss: 6.1550 - hamming_loss: 0.4219 - val_loss: 7.3524 - val_hamming_loss: 0.4511\n",
      "Epoch 94/100\n",
      "7/7 - 0s - loss: 6.1640 - hamming_loss: 0.4262 - val_loss: 7.3547 - val_hamming_loss: 0.4527\n",
      "Epoch 95/100\n",
      "7/7 - 0s - loss: 6.1497 - hamming_loss: 0.4239 - val_loss: 7.3578 - val_hamming_loss: 0.4567\n",
      "Epoch 96/100\n",
      "7/7 - 0s - loss: 6.1560 - hamming_loss: 0.4286 - val_loss: 7.3705 - val_hamming_loss: 0.4583\n",
      "Epoch 97/100\n",
      "7/7 - 0s - loss: 6.1603 - hamming_loss: 0.4294 - val_loss: 7.3544 - val_hamming_loss: 0.4575\n",
      "Epoch 98/100\n",
      "7/7 - 0s - loss: 6.1501 - hamming_loss: 0.4274 - val_loss: 7.3633 - val_hamming_loss: 0.4599\n",
      "Epoch 99/100\n",
      "7/7 - 0s - loss: 6.1585 - hamming_loss: 0.4286 - val_loss: 7.3516 - val_hamming_loss: 0.4591\n",
      "Epoch 100/100\n",
      "7/7 - 0s - loss: 6.1373 - hamming_loss: 0.4247 - val_loss: 7.3472 - val_hamming_loss: 0.4583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e1f626730>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "model_simpleRNN.fit(train_padded, Y_train, epochs = 100, validation_data = (test_padded, Y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23557692307692307"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Learn a Threshold Function\n",
    "Y_train_pred = model_simpleRNN.predict(train_padded)\n",
    "Y_test_pred = model_simpleRNN.predict(test_padded)\n",
    "t_range = (0, 1)\n",
    "\n",
    "test_labels_binary, threshold_function = predict_test_labels_binary(Y_train_pred, Y_train, Y_test_pred, t_range)\n",
    "metrics.hamming_loss(Y_test, test_labels_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_binary[0,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the LSTM RNN architecture\n",
    "num_labels = len(label_cols)\n",
    "\n",
    "model_LSTM = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(num_unique_words, 32, input_length = max_length),\n",
    "    tf.keras.layers.LSTM(16, return_sequences = False, return_state = False),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_labels, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(lr=0.001)\n",
    "#optim_func_LSTM = tf.keras.optimizers.Adagrad(\n",
    "#    learning_rate = 0.001, initial_accumulator_value = 0.1, epsilon = 1e-07,\n",
    "#    name = 'Adagrad')\n",
    "\n",
    "metric = tfa.metrics.HammingLoss(mode = 'multilabel', threshold = 0.5)\n",
    "model_LSTM.compile(loss = 'categorical_crossentropy', optimizer = optim, metrics = metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 - 2s - loss: 7.6843 - hamming_loss: 0.5381 - val_loss: 7.9091 - val_hamming_loss: 0.4639\n",
      "Epoch 2/100\n",
      "7/7 - 0s - loss: 7.6235 - hamming_loss: 0.5052 - val_loss: 7.8395 - val_hamming_loss: 0.5393\n",
      "Epoch 3/100\n",
      "7/7 - 0s - loss: 7.5496 - hamming_loss: 0.5551 - val_loss: 7.7410 - val_hamming_loss: 0.5393\n",
      "Epoch 4/100\n",
      "7/7 - 0s - loss: 7.4440 - hamming_loss: 0.5551 - val_loss: 7.5849 - val_hamming_loss: 0.5393\n",
      "Epoch 5/100\n",
      "7/7 - 0s - loss: 7.2851 - hamming_loss: 0.5551 - val_loss: 7.3702 - val_hamming_loss: 0.5777\n",
      "Epoch 6/100\n",
      "7/7 - 0s - loss: 7.0946 - hamming_loss: 0.5440 - val_loss: 7.1429 - val_hamming_loss: 0.5024\n",
      "Epoch 7/100\n",
      "7/7 - 0s - loss: 6.9162 - hamming_loss: 0.5186 - val_loss: 6.9666 - val_hamming_loss: 0.5024\n",
      "Epoch 8/100\n",
      "7/7 - 0s - loss: 6.7968 - hamming_loss: 0.5186 - val_loss: 6.8954 - val_hamming_loss: 0.5024\n",
      "Epoch 9/100\n",
      "7/7 - 0s - loss: 6.7634 - hamming_loss: 0.5186 - val_loss: 6.8984 - val_hamming_loss: 0.5024\n",
      "Epoch 10/100\n",
      "7/7 - 0s - loss: 6.7741 - hamming_loss: 0.5186 - val_loss: 6.9197 - val_hamming_loss: 0.5024\n",
      "Epoch 11/100\n",
      "7/7 - 0s - loss: 6.7983 - hamming_loss: 0.5186 - val_loss: 6.9437 - val_hamming_loss: 0.5024\n",
      "Epoch 12/100\n",
      "7/7 - 0s - loss: 6.8210 - hamming_loss: 0.5385 - val_loss: 6.9651 - val_hamming_loss: 0.5473\n",
      "Epoch 13/100\n",
      "7/7 - 0s - loss: 6.8543 - hamming_loss: 0.5472 - val_loss: 6.9933 - val_hamming_loss: 0.5473\n",
      "Epoch 14/100\n",
      "7/7 - 0s - loss: 6.8938 - hamming_loss: 0.5472 - val_loss: 7.0285 - val_hamming_loss: 0.5473\n",
      "Epoch 15/100\n",
      "7/7 - 0s - loss: 6.9310 - hamming_loss: 0.5472 - val_loss: 7.0545 - val_hamming_loss: 0.5473\n",
      "Epoch 16/100\n",
      "7/7 - 0s - loss: 6.9593 - hamming_loss: 0.5472 - val_loss: 7.0882 - val_hamming_loss: 0.5473\n",
      "Epoch 17/100\n",
      "7/7 - 0s - loss: 7.0100 - hamming_loss: 0.5472 - val_loss: 7.1381 - val_hamming_loss: 0.5473\n",
      "Epoch 18/100\n",
      "7/7 - 0s - loss: 7.0590 - hamming_loss: 0.5472 - val_loss: 7.1815 - val_hamming_loss: 0.6098\n",
      "Epoch 19/100\n",
      "7/7 - 0s - loss: 7.0997 - hamming_loss: 0.6170 - val_loss: 7.2089 - val_hamming_loss: 0.6098\n",
      "Epoch 20/100\n",
      "7/7 - 0s - loss: 7.1256 - hamming_loss: 0.6170 - val_loss: 7.2256 - val_hamming_loss: 0.6098\n",
      "Epoch 21/100\n",
      "7/7 - 0s - loss: 7.1425 - hamming_loss: 0.6170 - val_loss: 7.2461 - val_hamming_loss: 0.6098\n",
      "Epoch 22/100\n",
      "7/7 - 0s - loss: 7.1635 - hamming_loss: 0.6170 - val_loss: 7.2692 - val_hamming_loss: 0.6098\n",
      "Epoch 23/100\n",
      "7/7 - 0s - loss: 7.1823 - hamming_loss: 0.6170 - val_loss: 7.2932 - val_hamming_loss: 0.6098\n",
      "Epoch 24/100\n",
      "7/7 - 0s - loss: 7.2033 - hamming_loss: 0.6170 - val_loss: 7.3229 - val_hamming_loss: 0.6098\n",
      "Epoch 25/100\n",
      "7/7 - 0s - loss: 7.2317 - hamming_loss: 0.6170 - val_loss: 7.3518 - val_hamming_loss: 0.6098\n",
      "Epoch 26/100\n",
      "7/7 - 0s - loss: 7.2490 - hamming_loss: 0.6170 - val_loss: 7.3610 - val_hamming_loss: 0.6098\n",
      "Epoch 27/100\n",
      "7/7 - 0s - loss: 7.2550 - hamming_loss: 0.6170 - val_loss: 7.3680 - val_hamming_loss: 0.6098\n",
      "Epoch 28/100\n",
      "7/7 - 0s - loss: 7.2537 - hamming_loss: 0.6170 - val_loss: 7.3690 - val_hamming_loss: 0.6098\n",
      "Epoch 29/100\n",
      "7/7 - 0s - loss: 7.2505 - hamming_loss: 0.6170 - val_loss: 7.3745 - val_hamming_loss: 0.6098\n",
      "Epoch 30/100\n",
      "7/7 - 0s - loss: 7.2510 - hamming_loss: 0.6170 - val_loss: 7.3871 - val_hamming_loss: 0.6098\n",
      "Epoch 31/100\n",
      "7/7 - 0s - loss: 7.2563 - hamming_loss: 0.6170 - val_loss: 7.4025 - val_hamming_loss: 0.6098\n",
      "Epoch 32/100\n",
      "7/7 - 0s - loss: 7.2697 - hamming_loss: 0.6170 - val_loss: 7.4241 - val_hamming_loss: 0.6098\n",
      "Epoch 33/100\n",
      "7/7 - 0s - loss: 7.2826 - hamming_loss: 0.6170 - val_loss: 7.4378 - val_hamming_loss: 0.6098\n",
      "Epoch 34/100\n",
      "7/7 - 0s - loss: 7.2896 - hamming_loss: 0.6170 - val_loss: 7.4480 - val_hamming_loss: 0.6098\n",
      "Epoch 35/100\n",
      "7/7 - 0s - loss: 7.3011 - hamming_loss: 0.6170 - val_loss: 7.4657 - val_hamming_loss: 0.6098\n",
      "Epoch 36/100\n",
      "7/7 - 0s - loss: 7.3188 - hamming_loss: 0.6170 - val_loss: 7.4835 - val_hamming_loss: 0.6098\n",
      "Epoch 37/100\n",
      "7/7 - 0s - loss: 7.3306 - hamming_loss: 0.6170 - val_loss: 7.4944 - val_hamming_loss: 0.6098\n",
      "Epoch 38/100\n",
      "7/7 - 0s - loss: 7.3442 - hamming_loss: 0.6170 - val_loss: 7.5126 - val_hamming_loss: 0.6098\n",
      "Epoch 39/100\n",
      "7/7 - 0s - loss: 7.3585 - hamming_loss: 0.6170 - val_loss: 7.5231 - val_hamming_loss: 0.6098\n",
      "Epoch 40/100\n",
      "7/7 - 0s - loss: 7.3684 - hamming_loss: 0.6178 - val_loss: 7.5278 - val_hamming_loss: 0.6851\n",
      "Epoch 41/100\n",
      "7/7 - 0s - loss: 7.3715 - hamming_loss: 0.6931 - val_loss: 7.5355 - val_hamming_loss: 0.6851\n",
      "Epoch 42/100\n",
      "7/7 - 0s - loss: 7.3801 - hamming_loss: 0.6931 - val_loss: 7.5495 - val_hamming_loss: 0.6851\n",
      "Epoch 43/100\n",
      "7/7 - 0s - loss: 7.3839 - hamming_loss: 0.6931 - val_loss: 7.5553 - val_hamming_loss: 0.6851\n",
      "Epoch 44/100\n",
      "7/7 - 0s - loss: 7.3863 - hamming_loss: 0.6931 - val_loss: 7.5633 - val_hamming_loss: 0.6851\n",
      "Epoch 45/100\n",
      "7/7 - 0s - loss: 7.3893 - hamming_loss: 0.6931 - val_loss: 7.5676 - val_hamming_loss: 0.6851\n",
      "Epoch 46/100\n",
      "7/7 - 0s - loss: 7.3850 - hamming_loss: 0.6931 - val_loss: 7.5598 - val_hamming_loss: 0.6851\n",
      "Epoch 47/100\n",
      "7/7 - 0s - loss: 7.3798 - hamming_loss: 0.6931 - val_loss: 7.5632 - val_hamming_loss: 0.6851\n",
      "Epoch 48/100\n",
      "7/7 - 0s - loss: 7.3809 - hamming_loss: 0.6931 - val_loss: 7.5732 - val_hamming_loss: 0.6851\n",
      "Epoch 49/100\n",
      "7/7 - 0s - loss: 7.3874 - hamming_loss: 0.6931 - val_loss: 7.5852 - val_hamming_loss: 0.6851\n",
      "Epoch 50/100\n",
      "7/7 - 0s - loss: 7.4017 - hamming_loss: 0.6931 - val_loss: 7.6056 - val_hamming_loss: 0.6851\n",
      "Epoch 51/100\n",
      "7/7 - 0s - loss: 7.4149 - hamming_loss: 0.6931 - val_loss: 7.6211 - val_hamming_loss: 0.6851\n",
      "Epoch 52/100\n",
      "7/7 - 0s - loss: 7.4262 - hamming_loss: 0.6931 - val_loss: 7.6356 - val_hamming_loss: 0.6851\n",
      "Epoch 53/100\n",
      "7/7 - 0s - loss: 7.4302 - hamming_loss: 0.6931 - val_loss: 7.6374 - val_hamming_loss: 0.6851\n",
      "Epoch 54/100\n",
      "7/7 - 0s - loss: 7.4322 - hamming_loss: 0.6931 - val_loss: 7.6457 - val_hamming_loss: 0.6851\n",
      "Epoch 55/100\n",
      "7/7 - 0s - loss: 7.4377 - hamming_loss: 0.6931 - val_loss: 7.6515 - val_hamming_loss: 0.6851\n",
      "Epoch 56/100\n",
      "7/7 - 0s - loss: 7.4384 - hamming_loss: 0.6931 - val_loss: 7.6531 - val_hamming_loss: 0.6851\n",
      "Epoch 57/100\n",
      "7/7 - 0s - loss: 7.4493 - hamming_loss: 0.6931 - val_loss: 7.6747 - val_hamming_loss: 0.6851\n",
      "Epoch 58/100\n",
      "7/7 - 0s - loss: 7.4684 - hamming_loss: 0.6931 - val_loss: 7.6977 - val_hamming_loss: 0.6851\n",
      "Epoch 59/100\n",
      "7/7 - 0s - loss: 7.4884 - hamming_loss: 0.6931 - val_loss: 7.7173 - val_hamming_loss: 0.6851\n",
      "Epoch 60/100\n",
      "7/7 - 0s - loss: 7.4979 - hamming_loss: 0.6931 - val_loss: 7.7261 - val_hamming_loss: 0.6851\n",
      "Epoch 61/100\n",
      "7/7 - 0s - loss: 7.5072 - hamming_loss: 0.6931 - val_loss: 7.7416 - val_hamming_loss: 0.6851\n",
      "Epoch 62/100\n",
      "7/7 - 0s - loss: 7.5155 - hamming_loss: 0.6931 - val_loss: 7.7490 - val_hamming_loss: 0.6851\n",
      "Epoch 63/100\n",
      "7/7 - 0s - loss: 7.5205 - hamming_loss: 0.6931 - val_loss: 7.7560 - val_hamming_loss: 0.6851\n",
      "Epoch 64/100\n",
      "7/7 - 0s - loss: 7.5253 - hamming_loss: 0.6931 - val_loss: 7.7651 - val_hamming_loss: 0.6851\n",
      "Epoch 65/100\n",
      "7/7 - 0s - loss: 7.5354 - hamming_loss: 0.6931 - val_loss: 7.7788 - val_hamming_loss: 0.6851\n",
      "Epoch 66/100\n",
      "7/7 - 0s - loss: 7.5513 - hamming_loss: 0.6931 - val_loss: 7.7946 - val_hamming_loss: 0.6851\n",
      "Epoch 67/100\n",
      "7/7 - 0s - loss: 7.5561 - hamming_loss: 0.6931 - val_loss: 7.7956 - val_hamming_loss: 0.6851\n",
      "Epoch 68/100\n",
      "7/7 - 0s - loss: 7.5572 - hamming_loss: 0.6931 - val_loss: 7.8036 - val_hamming_loss: 0.6851\n",
      "Epoch 69/100\n",
      "7/7 - 0s - loss: 7.5617 - hamming_loss: 0.6931 - val_loss: 7.8136 - val_hamming_loss: 0.6851\n",
      "Epoch 70/100\n",
      "7/7 - 0s - loss: 7.5720 - hamming_loss: 0.6931 - val_loss: 7.8310 - val_hamming_loss: 0.6851\n",
      "Epoch 71/100\n",
      "7/7 - 0s - loss: 7.5864 - hamming_loss: 0.6931 - val_loss: 7.8483 - val_hamming_loss: 0.6851\n",
      "Epoch 72/100\n",
      "7/7 - 0s - loss: 7.6026 - hamming_loss: 0.6931 - val_loss: 7.8722 - val_hamming_loss: 0.6851\n",
      "Epoch 73/100\n",
      "7/7 - 0s - loss: 7.6222 - hamming_loss: 0.6931 - val_loss: 7.8973 - val_hamming_loss: 0.6851\n",
      "Epoch 74/100\n",
      "7/7 - 0s - loss: 7.6438 - hamming_loss: 0.6931 - val_loss: 7.9232 - val_hamming_loss: 0.6851\n",
      "Epoch 75/100\n",
      "7/7 - 0s - loss: 7.6676 - hamming_loss: 0.6931 - val_loss: 7.9466 - val_hamming_loss: 0.6851\n",
      "Epoch 76/100\n",
      "7/7 - 0s - loss: 7.6914 - hamming_loss: 0.6931 - val_loss: 7.9701 - val_hamming_loss: 0.6851\n",
      "Epoch 77/100\n",
      "7/7 - 0s - loss: 7.7044 - hamming_loss: 0.6931 - val_loss: 7.9741 - val_hamming_loss: 0.6851\n",
      "Epoch 78/100\n",
      "7/7 - 0s - loss: 7.7050 - hamming_loss: 0.6931 - val_loss: 7.9720 - val_hamming_loss: 0.6851\n",
      "Epoch 79/100\n",
      "7/7 - 0s - loss: 7.7086 - hamming_loss: 0.6931 - val_loss: 7.9795 - val_hamming_loss: 0.6851\n",
      "Epoch 80/100\n",
      "7/7 - 0s - loss: 7.7094 - hamming_loss: 0.6931 - val_loss: 7.9834 - val_hamming_loss: 0.6851\n",
      "Epoch 81/100\n",
      "7/7 - 0s - loss: 7.7136 - hamming_loss: 0.6931 - val_loss: 7.9980 - val_hamming_loss: 0.6851\n",
      "Epoch 82/100\n",
      "7/7 - 0s - loss: 7.7243 - hamming_loss: 0.6931 - val_loss: 8.0070 - val_hamming_loss: 0.6851\n",
      "Epoch 83/100\n",
      "7/7 - 0s - loss: 7.7311 - hamming_loss: 0.6931 - val_loss: 8.0128 - val_hamming_loss: 0.6851\n",
      "Epoch 84/100\n",
      "7/7 - 0s - loss: 7.7310 - hamming_loss: 0.6931 - val_loss: 8.0115 - val_hamming_loss: 0.6851\n",
      "Epoch 85/100\n",
      "7/7 - 0s - loss: 7.7306 - hamming_loss: 0.6931 - val_loss: 8.0195 - val_hamming_loss: 0.6851\n",
      "Epoch 86/100\n",
      "7/7 - 0s - loss: 7.7404 - hamming_loss: 0.6931 - val_loss: 8.0365 - val_hamming_loss: 0.6851\n",
      "Epoch 87/100\n",
      "7/7 - 0s - loss: 7.7535 - hamming_loss: 0.6931 - val_loss: 8.0503 - val_hamming_loss: 0.6851\n",
      "Epoch 88/100\n",
      "7/7 - 0s - loss: 7.7629 - hamming_loss: 0.6931 - val_loss: 8.0649 - val_hamming_loss: 0.6851\n",
      "Epoch 89/100\n",
      "7/7 - 0s - loss: 7.7698 - hamming_loss: 0.6931 - val_loss: 8.0765 - val_hamming_loss: 0.6851\n",
      "Epoch 90/100\n",
      "7/7 - 0s - loss: 7.7785 - hamming_loss: 0.6931 - val_loss: 8.0876 - val_hamming_loss: 0.6851\n",
      "Epoch 91/100\n",
      "7/7 - 0s - loss: 7.7812 - hamming_loss: 0.6931 - val_loss: 8.0925 - val_hamming_loss: 0.6851\n",
      "Epoch 92/100\n",
      "7/7 - 0s - loss: 7.7829 - hamming_loss: 0.6931 - val_loss: 8.1008 - val_hamming_loss: 0.6851\n",
      "Epoch 93/100\n",
      "7/7 - 0s - loss: 7.7858 - hamming_loss: 0.6931 - val_loss: 8.1070 - val_hamming_loss: 0.6851\n",
      "Epoch 94/100\n",
      "7/7 - 0s - loss: 7.7883 - hamming_loss: 0.6931 - val_loss: 8.1104 - val_hamming_loss: 0.6851\n",
      "Epoch 95/100\n",
      "7/7 - 0s - loss: 7.7843 - hamming_loss: 0.6931 - val_loss: 8.1074 - val_hamming_loss: 0.6851\n",
      "Epoch 96/100\n",
      "7/7 - 0s - loss: 7.7860 - hamming_loss: 0.6931 - val_loss: 8.1149 - val_hamming_loss: 0.6851\n",
      "Epoch 97/100\n",
      "7/7 - 0s - loss: 7.7836 - hamming_loss: 0.6931 - val_loss: 8.1092 - val_hamming_loss: 0.6851\n",
      "Epoch 98/100\n",
      "7/7 - 0s - loss: 7.7790 - hamming_loss: 0.6931 - val_loss: 8.1169 - val_hamming_loss: 0.6851\n",
      "Epoch 99/100\n",
      "7/7 - 0s - loss: 7.7847 - hamming_loss: 0.6931 - val_loss: 8.1312 - val_hamming_loss: 0.6851\n",
      "Epoch 100/100\n",
      "7/7 - 0s - loss: 7.7938 - hamming_loss: 0.6931 - val_loss: 8.1453 - val_hamming_loss: 0.6851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27102ea6df0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "model_LSTM.fit(train_padded, Y_train, epochs = 100, validation_data = (test_padded, Y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23798076923076922"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Learn a Threshold Function\n",
    "Y_train_pred = model_LSTM.predict(train_padded)\n",
    "Y_test_pred = model_LSTM.predict(test_padded)\n",
    "t_range = (0, 1)\n",
    "\n",
    "test_labels_binary, threshold_function = predict_test_labels_binary(Y_train_pred, Y_train, Y_test_pred, t_range)\n",
    "metrics.hamming_loss(Y_test, test_labels_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_binary[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model weights \n",
    "model_LSTM_filepath = 'Models/Cross_Entropy/LSTM_weights'\n",
    "model_LSTM.save_weights(model_LSTM_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the learned threshold function\n",
    "threshold_filepath = 'Models/Cross_Entropy/threshold_LSTM.json'\n",
    "skljson.to_json(threshold_function, threshold_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the LSTM RNN architecture\n",
    "num_labels = len(label_cols)\n",
    "\n",
    "model_biLSTM = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(num_unique_words, 32, input_length = max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences = False, return_state = False)),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_labels, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "#optim = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "optim = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate = 0.001, initial_accumulator_value = 0.1, epsilon = 1e-07,\n",
    "    name = 'Adagrad')\n",
    "\n",
    "#optim = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum = 0.8, epsilon=1e-07,)\n",
    "\n",
    "metric = tfa.metrics.HammingLoss(mode = 'multilabel', threshold = 0.5)\n",
    "model_biLSTM.compile(loss = 'categorical_crossentropy', optimizer = optim, metrics = metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 - 5s - loss: 7.6579 - hamming_loss: 0.5492 - val_loss: 7.9161 - val_hamming_loss: 0.5481\n",
      "Epoch 2/100\n",
      "7/7 - 0s - loss: 7.6446 - hamming_loss: 0.5420 - val_loss: 7.9055 - val_hamming_loss: 0.5553\n",
      "Epoch 3/100\n",
      "7/7 - 0s - loss: 7.6339 - hamming_loss: 0.5424 - val_loss: 7.8966 - val_hamming_loss: 0.5521\n",
      "Epoch 4/100\n",
      "7/7 - 0s - loss: 7.6247 - hamming_loss: 0.5385 - val_loss: 7.8892 - val_hamming_loss: 0.5497\n",
      "Epoch 5/100\n",
      "7/7 - 0s - loss: 7.6172 - hamming_loss: 0.5385 - val_loss: 7.8823 - val_hamming_loss: 0.5441\n",
      "Epoch 6/100\n",
      "7/7 - 0s - loss: 7.6100 - hamming_loss: 0.5317 - val_loss: 7.8756 - val_hamming_loss: 0.5361\n",
      "Epoch 7/100\n",
      "7/7 - 0s - loss: 7.6031 - hamming_loss: 0.5254 - val_loss: 7.8687 - val_hamming_loss: 0.5312\n",
      "Epoch 8/100\n",
      "7/7 - 0s - loss: 7.5961 - hamming_loss: 0.5202 - val_loss: 7.8634 - val_hamming_loss: 0.5264\n",
      "Epoch 9/100\n",
      "7/7 - 0s - loss: 7.5904 - hamming_loss: 0.5163 - val_loss: 7.8577 - val_hamming_loss: 0.5248\n",
      "Epoch 10/100\n",
      "7/7 - 0s - loss: 7.5844 - hamming_loss: 0.5139 - val_loss: 7.8522 - val_hamming_loss: 0.5232\n",
      "Epoch 11/100\n",
      "7/7 - 0s - loss: 7.5784 - hamming_loss: 0.5115 - val_loss: 7.8471 - val_hamming_loss: 0.5208\n",
      "Epoch 12/100\n",
      "7/7 - 0s - loss: 7.5730 - hamming_loss: 0.5107 - val_loss: 7.8414 - val_hamming_loss: 0.5200\n",
      "Epoch 13/100\n",
      "7/7 - 0s - loss: 7.5671 - hamming_loss: 0.5091 - val_loss: 7.8359 - val_hamming_loss: 0.5192\n",
      "Epoch 14/100\n",
      "7/7 - 0s - loss: 7.5616 - hamming_loss: 0.5083 - val_loss: 7.8317 - val_hamming_loss: 0.5184\n",
      "Epoch 15/100\n",
      "7/7 - 0s - loss: 7.5571 - hamming_loss: 0.5071 - val_loss: 7.8273 - val_hamming_loss: 0.5200\n",
      "Epoch 16/100\n",
      "7/7 - 0s - loss: 7.5522 - hamming_loss: 0.5071 - val_loss: 7.8217 - val_hamming_loss: 0.5168\n",
      "Epoch 17/100\n",
      "7/7 - 0s - loss: 7.5465 - hamming_loss: 0.5067 - val_loss: 7.8170 - val_hamming_loss: 0.5160\n",
      "Epoch 18/100\n",
      "7/7 - 0s - loss: 7.5416 - hamming_loss: 0.5067 - val_loss: 7.8121 - val_hamming_loss: 0.5176\n",
      "Epoch 19/100\n",
      "7/7 - 0s - loss: 7.5368 - hamming_loss: 0.5079 - val_loss: 7.8073 - val_hamming_loss: 0.5184\n",
      "Epoch 20/100\n",
      "7/7 - 0s - loss: 7.5316 - hamming_loss: 0.5079 - val_loss: 7.8026 - val_hamming_loss: 0.5152\n",
      "Epoch 21/100\n",
      "7/7 - 0s - loss: 7.5266 - hamming_loss: 0.5083 - val_loss: 7.7978 - val_hamming_loss: 0.5152\n",
      "Epoch 22/100\n",
      "7/7 - 0s - loss: 7.5215 - hamming_loss: 0.5079 - val_loss: 7.7930 - val_hamming_loss: 0.5144\n",
      "Epoch 23/100\n",
      "7/7 - 0s - loss: 7.5164 - hamming_loss: 0.5079 - val_loss: 7.7877 - val_hamming_loss: 0.5168\n",
      "Epoch 24/100\n",
      "7/7 - 0s - loss: 7.5110 - hamming_loss: 0.5095 - val_loss: 7.7823 - val_hamming_loss: 0.5152\n",
      "Epoch 25/100\n",
      "7/7 - 0s - loss: 7.5053 - hamming_loss: 0.5091 - val_loss: 7.7781 - val_hamming_loss: 0.5152\n",
      "Epoch 26/100\n",
      "7/7 - 0s - loss: 7.5008 - hamming_loss: 0.5095 - val_loss: 7.7730 - val_hamming_loss: 0.5088\n",
      "Epoch 27/100\n",
      "7/7 - 0s - loss: 7.4955 - hamming_loss: 0.5059 - val_loss: 7.7682 - val_hamming_loss: 0.5072\n",
      "Epoch 28/100\n",
      "7/7 - 0s - loss: 7.4903 - hamming_loss: 0.5059 - val_loss: 7.7635 - val_hamming_loss: 0.5048\n",
      "Epoch 29/100\n",
      "7/7 - 0s - loss: 7.4851 - hamming_loss: 0.5052 - val_loss: 7.7584 - val_hamming_loss: 0.5064\n",
      "Epoch 30/100\n",
      "7/7 - 0s - loss: 7.4794 - hamming_loss: 0.5059 - val_loss: 7.7529 - val_hamming_loss: 0.5040\n",
      "Epoch 31/100\n",
      "7/7 - 0s - loss: 7.4736 - hamming_loss: 0.5048 - val_loss: 7.7478 - val_hamming_loss: 0.5040\n",
      "Epoch 32/100\n",
      "7/7 - 0s - loss: 7.4681 - hamming_loss: 0.5040 - val_loss: 7.7424 - val_hamming_loss: 0.5016\n",
      "Epoch 33/100\n",
      "7/7 - 0s - loss: 7.4625 - hamming_loss: 0.5056 - val_loss: 7.7371 - val_hamming_loss: 0.5016\n",
      "Epoch 34/100\n",
      "7/7 - 0s - loss: 7.4564 - hamming_loss: 0.5056 - val_loss: 7.7311 - val_hamming_loss: 0.5008\n",
      "Epoch 35/100\n",
      "7/7 - 0s - loss: 7.4503 - hamming_loss: 0.5048 - val_loss: 7.7258 - val_hamming_loss: 0.5016\n",
      "Epoch 36/100\n",
      "7/7 - 0s - loss: 7.4447 - hamming_loss: 0.5052 - val_loss: 7.7204 - val_hamming_loss: 0.5016\n",
      "Epoch 37/100\n",
      "7/7 - 0s - loss: 7.4388 - hamming_loss: 0.5044 - val_loss: 7.7144 - val_hamming_loss: 0.5000\n",
      "Epoch 38/100\n",
      "7/7 - 0s - loss: 7.4324 - hamming_loss: 0.4996 - val_loss: 7.7089 - val_hamming_loss: 0.4992\n",
      "Epoch 39/100\n",
      "7/7 - 0s - loss: 7.4266 - hamming_loss: 0.4992 - val_loss: 7.7030 - val_hamming_loss: 0.4968\n",
      "Epoch 40/100\n",
      "7/7 - 0s - loss: 7.4204 - hamming_loss: 0.4972 - val_loss: 7.6969 - val_hamming_loss: 0.4936\n",
      "Epoch 41/100\n",
      "7/7 - 0s - loss: 7.4138 - hamming_loss: 0.4972 - val_loss: 7.6910 - val_hamming_loss: 0.4936\n",
      "Epoch 42/100\n",
      "7/7 - 0s - loss: 7.4073 - hamming_loss: 0.4964 - val_loss: 7.6854 - val_hamming_loss: 0.4904\n",
      "Epoch 43/100\n",
      "7/7 - 0s - loss: 7.4012 - hamming_loss: 0.4941 - val_loss: 7.6797 - val_hamming_loss: 0.4896\n",
      "Epoch 44/100\n",
      "7/7 - 0s - loss: 7.3949 - hamming_loss: 0.4937 - val_loss: 7.6734 - val_hamming_loss: 0.4880\n",
      "Epoch 45/100\n",
      "7/7 - 0s - loss: 7.3881 - hamming_loss: 0.4921 - val_loss: 7.6682 - val_hamming_loss: 0.4872\n",
      "Epoch 46/100\n",
      "7/7 - 0s - loss: 7.3825 - hamming_loss: 0.4929 - val_loss: 7.6620 - val_hamming_loss: 0.4880\n",
      "Epoch 47/100\n",
      "7/7 - 0s - loss: 7.3757 - hamming_loss: 0.4917 - val_loss: 7.6558 - val_hamming_loss: 0.4880\n",
      "Epoch 48/100\n",
      "7/7 - 0s - loss: 7.3687 - hamming_loss: 0.4917 - val_loss: 7.6491 - val_hamming_loss: 0.4872\n",
      "Epoch 49/100\n",
      "7/7 - 0s - loss: 7.3615 - hamming_loss: 0.4901 - val_loss: 7.6424 - val_hamming_loss: 0.4864\n",
      "Epoch 50/100\n",
      "7/7 - 0s - loss: 7.3543 - hamming_loss: 0.4901 - val_loss: 7.6363 - val_hamming_loss: 0.4840\n",
      "Epoch 51/100\n",
      "7/7 - 0s - loss: 7.3476 - hamming_loss: 0.4885 - val_loss: 7.6294 - val_hamming_loss: 0.4824\n",
      "Epoch 52/100\n",
      "7/7 - 0s - loss: 7.3400 - hamming_loss: 0.4881 - val_loss: 7.6239 - val_hamming_loss: 0.4800\n",
      "Epoch 53/100\n",
      "7/7 - 0s - loss: 7.3340 - hamming_loss: 0.4873 - val_loss: 7.6169 - val_hamming_loss: 0.4808\n",
      "Epoch 54/100\n",
      "7/7 - 0s - loss: 7.3265 - hamming_loss: 0.4873 - val_loss: 7.6107 - val_hamming_loss: 0.4800\n",
      "Epoch 55/100\n",
      "7/7 - 0s - loss: 7.3198 - hamming_loss: 0.4869 - val_loss: 7.6047 - val_hamming_loss: 0.4792\n",
      "Epoch 56/100\n",
      "7/7 - 0s - loss: 7.3133 - hamming_loss: 0.4861 - val_loss: 7.5975 - val_hamming_loss: 0.4808\n",
      "Epoch 57/100\n",
      "7/7 - 0s - loss: 7.3056 - hamming_loss: 0.4853 - val_loss: 7.5904 - val_hamming_loss: 0.4792\n",
      "Epoch 58/100\n",
      "7/7 - 0s - loss: 7.2978 - hamming_loss: 0.4849 - val_loss: 7.5833 - val_hamming_loss: 0.4784\n",
      "Epoch 59/100\n",
      "7/7 - 0s - loss: 7.2900 - hamming_loss: 0.4849 - val_loss: 7.5773 - val_hamming_loss: 0.4776\n",
      "Epoch 60/100\n",
      "7/7 - 0s - loss: 7.2831 - hamming_loss: 0.4845 - val_loss: 7.5706 - val_hamming_loss: 0.4800\n",
      "Epoch 61/100\n",
      "7/7 - 0s - loss: 7.2759 - hamming_loss: 0.4853 - val_loss: 7.5641 - val_hamming_loss: 0.4792\n",
      "Epoch 62/100\n",
      "7/7 - 0s - loss: 7.2689 - hamming_loss: 0.4849 - val_loss: 7.5579 - val_hamming_loss: 0.4768\n",
      "Epoch 63/100\n",
      "7/7 - 0s - loss: 7.2619 - hamming_loss: 0.4845 - val_loss: 7.5507 - val_hamming_loss: 0.4768\n",
      "Epoch 64/100\n",
      "7/7 - 0s - loss: 7.2538 - hamming_loss: 0.4841 - val_loss: 7.5440 - val_hamming_loss: 0.4752\n",
      "Epoch 65/100\n",
      "7/7 - 0s - loss: 7.2465 - hamming_loss: 0.4841 - val_loss: 7.5370 - val_hamming_loss: 0.4744\n",
      "Epoch 66/100\n",
      "7/7 - 0s - loss: 7.2387 - hamming_loss: 0.4841 - val_loss: 7.5315 - val_hamming_loss: 0.4744\n",
      "Epoch 67/100\n",
      "7/7 - 0s - loss: 7.2324 - hamming_loss: 0.4833 - val_loss: 7.5250 - val_hamming_loss: 0.4736\n",
      "Epoch 68/100\n",
      "7/7 - 0s - loss: 7.2253 - hamming_loss: 0.4826 - val_loss: 7.5186 - val_hamming_loss: 0.4736\n",
      "Epoch 69/100\n",
      "7/7 - 0s - loss: 7.2176 - hamming_loss: 0.4822 - val_loss: 7.5115 - val_hamming_loss: 0.4720\n",
      "Epoch 70/100\n",
      "7/7 - 0s - loss: 7.2101 - hamming_loss: 0.4818 - val_loss: 7.5048 - val_hamming_loss: 0.4712\n",
      "Epoch 71/100\n",
      "7/7 - 0s - loss: 7.2023 - hamming_loss: 0.4818 - val_loss: 7.4982 - val_hamming_loss: 0.4696\n",
      "Epoch 72/100\n",
      "7/7 - 0s - loss: 7.1949 - hamming_loss: 0.4810 - val_loss: 7.4913 - val_hamming_loss: 0.4679\n",
      "Epoch 73/100\n",
      "7/7 - 0s - loss: 7.1872 - hamming_loss: 0.4794 - val_loss: 7.4844 - val_hamming_loss: 0.4671\n",
      "Epoch 74/100\n",
      "7/7 - 0s - loss: 7.1791 - hamming_loss: 0.4774 - val_loss: 7.4782 - val_hamming_loss: 0.4679\n",
      "Epoch 75/100\n",
      "7/7 - 0s - loss: 7.1723 - hamming_loss: 0.4790 - val_loss: 7.4719 - val_hamming_loss: 0.4696\n",
      "Epoch 76/100\n",
      "7/7 - 0s - loss: 7.1655 - hamming_loss: 0.4806 - val_loss: 7.4660 - val_hamming_loss: 0.4679\n",
      "Epoch 77/100\n",
      "7/7 - 0s - loss: 7.1587 - hamming_loss: 0.4786 - val_loss: 7.4605 - val_hamming_loss: 0.4671\n",
      "Epoch 78/100\n",
      "7/7 - 0s - loss: 7.1525 - hamming_loss: 0.4778 - val_loss: 7.4550 - val_hamming_loss: 0.4671\n",
      "Epoch 79/100\n",
      "7/7 - 0s - loss: 7.1460 - hamming_loss: 0.4742 - val_loss: 7.4492 - val_hamming_loss: 0.4631\n",
      "Epoch 80/100\n",
      "7/7 - 0s - loss: 7.1387 - hamming_loss: 0.4699 - val_loss: 7.4432 - val_hamming_loss: 0.4567\n",
      "Epoch 81/100\n",
      "7/7 - 0s - loss: 7.1318 - hamming_loss: 0.4679 - val_loss: 7.4374 - val_hamming_loss: 0.4535\n",
      "Epoch 82/100\n",
      "7/7 - 0s - loss: 7.1255 - hamming_loss: 0.4643 - val_loss: 7.4323 - val_hamming_loss: 0.4495\n",
      "Epoch 83/100\n",
      "7/7 - 0s - loss: 7.1195 - hamming_loss: 0.4611 - val_loss: 7.4274 - val_hamming_loss: 0.4447\n",
      "Epoch 84/100\n",
      "7/7 - 0s - loss: 7.1135 - hamming_loss: 0.4580 - val_loss: 7.4223 - val_hamming_loss: 0.4391\n",
      "Epoch 85/100\n",
      "7/7 - 0s - loss: 7.1075 - hamming_loss: 0.4564 - val_loss: 7.4172 - val_hamming_loss: 0.4351\n",
      "Epoch 86/100\n",
      "7/7 - 0s - loss: 7.1014 - hamming_loss: 0.4496 - val_loss: 7.4123 - val_hamming_loss: 0.4335\n",
      "Epoch 87/100\n",
      "7/7 - 0s - loss: 7.0956 - hamming_loss: 0.4461 - val_loss: 7.4073 - val_hamming_loss: 0.4303\n",
      "Epoch 88/100\n",
      "7/7 - 0s - loss: 7.0895 - hamming_loss: 0.4445 - val_loss: 7.4027 - val_hamming_loss: 0.4255\n",
      "Epoch 89/100\n",
      "7/7 - 0s - loss: 7.0840 - hamming_loss: 0.4385 - val_loss: 7.3983 - val_hamming_loss: 0.4223\n",
      "Epoch 90/100\n",
      "7/7 - 0s - loss: 7.0785 - hamming_loss: 0.4350 - val_loss: 7.3942 - val_hamming_loss: 0.4215\n",
      "Epoch 91/100\n",
      "7/7 - 0s - loss: 7.0733 - hamming_loss: 0.4318 - val_loss: 7.3898 - val_hamming_loss: 0.4199\n",
      "Epoch 92/100\n",
      "7/7 - 0s - loss: 7.0677 - hamming_loss: 0.4294 - val_loss: 7.3859 - val_hamming_loss: 0.4183\n",
      "Epoch 93/100\n",
      "7/7 - 0s - loss: 7.0628 - hamming_loss: 0.4259 - val_loss: 7.3824 - val_hamming_loss: 0.4183\n",
      "Epoch 94/100\n",
      "7/7 - 0s - loss: 7.0585 - hamming_loss: 0.4239 - val_loss: 7.3784 - val_hamming_loss: 0.4183\n",
      "Epoch 95/100\n",
      "7/7 - 0s - loss: 7.0536 - hamming_loss: 0.4231 - val_loss: 7.3749 - val_hamming_loss: 0.4175\n",
      "Epoch 96/100\n",
      "7/7 - 0s - loss: 7.0487 - hamming_loss: 0.4219 - val_loss: 7.3715 - val_hamming_loss: 0.4175\n",
      "Epoch 97/100\n",
      "7/7 - 0s - loss: 7.0446 - hamming_loss: 0.4219 - val_loss: 7.3683 - val_hamming_loss: 0.4175\n",
      "Epoch 98/100\n",
      "7/7 - 0s - loss: 7.0401 - hamming_loss: 0.4215 - val_loss: 7.3653 - val_hamming_loss: 0.4175\n",
      "Epoch 99/100\n",
      "7/7 - 0s - loss: 7.0358 - hamming_loss: 0.4211 - val_loss: 7.3622 - val_hamming_loss: 0.4175\n",
      "Epoch 100/100\n",
      "7/7 - 0s - loss: 7.0320 - hamming_loss: 0.4215 - val_loss: 7.3589 - val_hamming_loss: 0.4175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20da7de4fa0>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "model_biLSTM.fit(train_padded, Y_train, epochs = 100, validation_data = (test_padded, Y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41746794871794873"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using a constant 0.5 threshold function, get the hamming loss for the trained network on the test set\n",
    "predictions = model_biLSTM.predict(test_padded)\n",
    "predictions_binary = model_biLSTM.predict(test_padded)\n",
    "for i in range(Y_test.shape[0]):\n",
    "    for j in range(Y_test.shape[1]):\n",
    "        if predictions_binary[i, j] > 0.5:\n",
    "            predictions_binary[i, j] = 1\n",
    "        else:\n",
    "            predictions_binary[i, j] = 0\n",
    "\n",
    "# Get the hamming loss\n",
    "metrics.hamming_loss(Y_test, predictions_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2155448717948718"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Learn a Threshold Function\n",
    "Y_train_pred = model_biLSTM.predict(train_padded)\n",
    "Y_test_pred = model_biLSTM.predict(test_padded)\n",
    "t_range = (0, 1)\n",
    "\n",
    "test_labels_binary, threshold_function = predict_test_labels_binary(Y_train_pred, Y_train, Y_test_pred, t_range)\n",
    "metrics.hamming_loss(Y_test, test_labels_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_binary[0,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train With BPMLL Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reduce the data to cases with at least one label\n",
    "## These are the rows that don't have any associated labels\n",
    "Y_gz = Y > 0\n",
    "no_labels_id = np.where(~Y_gz.any(axis=1))[0]\n",
    "#no_labels_id\n",
    "\n",
    "atleast_one_label_ids = paragraph_data.index\n",
    "atleast_one_label_ids = atleast_one_label_ids.difference(no_labels_id)\n",
    "X_hasLabel = X[atleast_one_label_ids, ]\n",
    "Y_hasLabel = Y[atleast_one_label_ids, ]\n",
    "X_train_hasLabel, X_test_hasLabel, Y_train_hasLabel, Y_test_hasLabel = train_test_split(X_hasLabel, Y_hasLabel, \n",
    "                                                                                        test_size = 0.33, random_state = 321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_hasLabel = counter_word(paragraph_data.iloc[atleast_one_label_ids].full_text)\n",
    "num_unique_words = len(counter)\n",
    "\n",
    "## vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer_hasLabel = Tokenizer(num_words = num_unique_words)\n",
    "tokenizer_hasLabel.fit_on_texts(X_train_hasLabel) # fit only to training\n",
    "word_index_hasLabel = tokenizer_hasLabel.word_index\n",
    "\n",
    "train_sequences_hasLabel = tokenizer_hasLabel.texts_to_sequences(X_train_hasLabel)\n",
    "test_sequences_hasLabel = tokenizer_hasLabel.texts_to_sequences(X_test_hasLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_par_length_hasLabel = 0\n",
    "for par in train_sequences_hasLabel:\n",
    "    if len(par) > max_par_length_hasLabel:\n",
    "        max_par_length_hasLabel = len(par)\n",
    "        \n",
    "max_par_length_hasLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((176, 100), (88, 100))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max number of words in a sequence\n",
    "max_length_hasLabel = 100\n",
    "\n",
    "train_padded_hasLabel = pad_sequences(train_sequences_hasLabel, maxlen = max_length_hasLabel, padding = \"post\", truncating = \"post\")\n",
    "test_padded_hasLabel = pad_sequences(test_sequences_hasLabel, maxlen = max_length_hasLabel, padding = \"post\", truncating = \"post\")\n",
    "train_padded_hasLabel.shape, test_padded_hasLabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export reduced train and test data to json file\n",
    "RNN_data_dict_reduced = {'train_padded_hasLabel' : train_padded_hasLabel.tolist(), \n",
    "                 'test_padded_hasLabel' : test_padded_hasLabel.tolist(),\n",
    "                 'Y_train_hasLabel' : Y_train_hasLabel.tolist(), \n",
    "                 'Y_test_hasLabel' : Y_test_hasLabel.tolist()}\n",
    "\n",
    "RNN_data_dict_reduced_json = json.dumps(RNN_data_dict_reduced)\n",
    "with open(\"RNN_data_dict_reduced.json\", \"w\") as outfile: \n",
    "    json.dump(RNN_data_dict_reduced_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[293, 166, 83, 43, 770, 167, 294, 771, 772, 773, 33, 774, 31, 6, 1, 775, 295, 776, 452, 777, 107, 168, 453, 33, 50, 454, 108]\n",
      "invoked defense production act compel general motors accept perform prioritize federal contracts ventilators trump said invocation dpa demonstrate clearly hesitate use full authority federal government combat crisis\n"
     ]
    }
   ],
   "source": [
    "# Check reversing the indices\n",
    "# flip (key, value)\n",
    "reverse_word_index_hasLabel = dict([(idx, word) for (word, idx) in word_index_hasLabel.items()])\n",
    "\n",
    "def decode_hasLabel(sequence):\n",
    "    return \" \".join([reverse_word_index_hasLabel.get(idx, \"?\") for idx in sequence])\n",
    "\n",
    "decoded_text_hasLabel = decode_hasLabel(train_sequences_hasLabel[0])\n",
    "\n",
    "print(train_sequences_hasLabel[0])\n",
    "print(decoded_text_hasLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the simple RNN architecture\n",
    "model_simpleRNN_bpmll = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(num_unique_words, 32, input_length = max_length),\n",
    "    tf.keras.layers.SimpleRNN(16, return_sequences = False, return_state = False),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_labels, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "optim_bpmll = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "metric = tfa.metrics.HammingLoss(mode = 'multilabel', threshold = 0.5)\n",
    "model_simpleRNN_bpmll.compile(loss = bp_mll_loss, optimizer = optim_bpmll, metrics = metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 - 2s - loss: 1.0189 - hamming_loss: 0.5520 - val_loss: 1.0122 - val_hamming_loss: 0.5472\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 1.0136 - hamming_loss: 0.5240 - val_loss: 1.0076 - val_hamming_loss: 0.5236\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 1.0072 - hamming_loss: 0.5022 - val_loss: 1.0032 - val_hamming_loss: 0.5052\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 1.0001 - hamming_loss: 0.4768 - val_loss: 0.9987 - val_hamming_loss: 0.4851\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.9924 - hamming_loss: 0.4327 - val_loss: 0.9936 - val_hamming_loss: 0.4545\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.9851 - hamming_loss: 0.4248 - val_loss: 0.9884 - val_hamming_loss: 0.4563\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.9794 - hamming_loss: 0.4222 - val_loss: 0.9844 - val_hamming_loss: 0.4353\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.9752 - hamming_loss: 0.3995 - val_loss: 0.9816 - val_hamming_loss: 0.4205\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.9713 - hamming_loss: 0.3737 - val_loss: 0.9794 - val_hamming_loss: 0.4047\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.9675 - hamming_loss: 0.3510 - val_loss: 0.9775 - val_hamming_loss: 0.3986\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.9634 - hamming_loss: 0.3330 - val_loss: 0.9760 - val_hamming_loss: 0.3890\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.9587 - hamming_loss: 0.3208 - val_loss: 0.9750 - val_hamming_loss: 0.3820\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.9534 - hamming_loss: 0.3134 - val_loss: 0.9745 - val_hamming_loss: 0.3872\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.9472 - hamming_loss: 0.2976 - val_loss: 0.9747 - val_hamming_loss: 0.3864\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.9402 - hamming_loss: 0.2863 - val_loss: 0.9751 - val_hamming_loss: 0.3846\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.9331 - hamming_loss: 0.2788 - val_loss: 0.9758 - val_hamming_loss: 0.3846\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.9266 - hamming_loss: 0.2745 - val_loss: 0.9764 - val_hamming_loss: 0.3872\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.9208 - hamming_loss: 0.2723 - val_loss: 0.9769 - val_hamming_loss: 0.4038\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.9154 - hamming_loss: 0.2679 - val_loss: 0.9769 - val_hamming_loss: 0.4056\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.9104 - hamming_loss: 0.2679 - val_loss: 0.9766 - val_hamming_loss: 0.4047\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.9060 - hamming_loss: 0.2622 - val_loss: 0.9761 - val_hamming_loss: 0.4126\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.9018 - hamming_loss: 0.2517 - val_loss: 0.9756 - val_hamming_loss: 0.4152\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.8978 - hamming_loss: 0.2483 - val_loss: 0.9753 - val_hamming_loss: 0.4196\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.8939 - hamming_loss: 0.2439 - val_loss: 0.9748 - val_hamming_loss: 0.4213\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.8905 - hamming_loss: 0.2399 - val_loss: 0.9746 - val_hamming_loss: 0.4187\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.8872 - hamming_loss: 0.2356 - val_loss: 0.9743 - val_hamming_loss: 0.4170\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.8839 - hamming_loss: 0.2303 - val_loss: 0.9742 - val_hamming_loss: 0.4205\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.8805 - hamming_loss: 0.2286 - val_loss: 0.9741 - val_hamming_loss: 0.4187\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.8774 - hamming_loss: 0.2229 - val_loss: 0.9740 - val_hamming_loss: 0.4170\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.8743 - hamming_loss: 0.2172 - val_loss: 0.9740 - val_hamming_loss: 0.4178\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.8713 - hamming_loss: 0.2177 - val_loss: 0.9742 - val_hamming_loss: 0.4178\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.8685 - hamming_loss: 0.2111 - val_loss: 0.9742 - val_hamming_loss: 0.4196\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.8655 - hamming_loss: 0.2028 - val_loss: 0.9743 - val_hamming_loss: 0.4222\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.8628 - hamming_loss: 0.2019 - val_loss: 0.9744 - val_hamming_loss: 0.4213\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.8596 - hamming_loss: 0.1989 - val_loss: 0.9747 - val_hamming_loss: 0.4231\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.8569 - hamming_loss: 0.2006 - val_loss: 0.9745 - val_hamming_loss: 0.4248\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.8538 - hamming_loss: 0.1932 - val_loss: 0.9750 - val_hamming_loss: 0.4274\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.8512 - hamming_loss: 0.2015 - val_loss: 0.9749 - val_hamming_loss: 0.4205\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.8483 - hamming_loss: 0.1967 - val_loss: 0.9761 - val_hamming_loss: 0.4292\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.8459 - hamming_loss: 0.2146 - val_loss: 0.9762 - val_hamming_loss: 0.4178\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.8419 - hamming_loss: 0.2076 - val_loss: 0.9758 - val_hamming_loss: 0.4248\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.8383 - hamming_loss: 0.2098 - val_loss: 0.9764 - val_hamming_loss: 0.4240\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.8355 - hamming_loss: 0.2111 - val_loss: 0.9759 - val_hamming_loss: 0.4240\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.8327 - hamming_loss: 0.2076 - val_loss: 0.9762 - val_hamming_loss: 0.4231\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.8300 - hamming_loss: 0.2111 - val_loss: 0.9758 - val_hamming_loss: 0.4205\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.8275 - hamming_loss: 0.2085 - val_loss: 0.9766 - val_hamming_loss: 0.4231\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.8252 - hamming_loss: 0.2159 - val_loss: 0.9759 - val_hamming_loss: 0.4248\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.8232 - hamming_loss: 0.2146 - val_loss: 0.9768 - val_hamming_loss: 0.4222\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.8218 - hamming_loss: 0.2190 - val_loss: 0.9760 - val_hamming_loss: 0.4222\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.8195 - hamming_loss: 0.2094 - val_loss: 0.9763 - val_hamming_loss: 0.4205\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.8169 - hamming_loss: 0.2102 - val_loss: 0.9762 - val_hamming_loss: 0.4196\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.8151 - hamming_loss: 0.2124 - val_loss: 0.9767 - val_hamming_loss: 0.4213\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.8131 - hamming_loss: 0.2102 - val_loss: 0.9766 - val_hamming_loss: 0.4205\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.8109 - hamming_loss: 0.2098 - val_loss: 0.9769 - val_hamming_loss: 0.4187\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.8090 - hamming_loss: 0.2067 - val_loss: 0.9763 - val_hamming_loss: 0.4213\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.8072 - hamming_loss: 0.2076 - val_loss: 0.9769 - val_hamming_loss: 0.4213\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.8054 - hamming_loss: 0.2094 - val_loss: 0.9761 - val_hamming_loss: 0.4266\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.8035 - hamming_loss: 0.2037 - val_loss: 0.9771 - val_hamming_loss: 0.4222\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.8022 - hamming_loss: 0.2032 - val_loss: 0.9756 - val_hamming_loss: 0.4318\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.8007 - hamming_loss: 0.2028 - val_loss: 0.9776 - val_hamming_loss: 0.4248\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.8005 - hamming_loss: 0.2111 - val_loss: 0.9754 - val_hamming_loss: 0.4274\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.7999 - hamming_loss: 0.2076 - val_loss: 0.9773 - val_hamming_loss: 0.4231\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.7994 - hamming_loss: 0.2137 - val_loss: 0.9766 - val_hamming_loss: 0.4257\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.7973 - hamming_loss: 0.2050 - val_loss: 0.9768 - val_hamming_loss: 0.4213\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.7947 - hamming_loss: 0.2085 - val_loss: 0.9766 - val_hamming_loss: 0.4222\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.7925 - hamming_loss: 0.2063 - val_loss: 0.9761 - val_hamming_loss: 0.4248\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.7910 - hamming_loss: 0.2080 - val_loss: 0.9759 - val_hamming_loss: 0.4222\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.7896 - hamming_loss: 0.2107 - val_loss: 0.9745 - val_hamming_loss: 0.4231\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.7887 - hamming_loss: 0.2028 - val_loss: 0.9752 - val_hamming_loss: 0.4257\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.7881 - hamming_loss: 0.2085 - val_loss: 0.9746 - val_hamming_loss: 0.4196\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.7869 - hamming_loss: 0.2032 - val_loss: 0.9749 - val_hamming_loss: 0.4248\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.7851 - hamming_loss: 0.2076 - val_loss: 0.9748 - val_hamming_loss: 0.4240\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.7836 - hamming_loss: 0.2037 - val_loss: 0.9745 - val_hamming_loss: 0.4213\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.7818 - hamming_loss: 0.2050 - val_loss: 0.9742 - val_hamming_loss: 0.4266\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 0.7806 - hamming_loss: 0.2024 - val_loss: 0.9749 - val_hamming_loss: 0.4248\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.7796 - hamming_loss: 0.2080 - val_loss: 0.9747 - val_hamming_loss: 0.4257\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.7776 - hamming_loss: 0.2041 - val_loss: 0.9747 - val_hamming_loss: 0.4266\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.7766 - hamming_loss: 0.2032 - val_loss: 0.9732 - val_hamming_loss: 0.4292\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 0.7757 - hamming_loss: 0.1997 - val_loss: 0.9752 - val_hamming_loss: 0.4344\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.7746 - hamming_loss: 0.1984 - val_loss: 0.9732 - val_hamming_loss: 0.4292\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 0.7731 - hamming_loss: 0.1997 - val_loss: 0.9752 - val_hamming_loss: 0.4283\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 0.7740 - hamming_loss: 0.1984 - val_loss: 0.9745 - val_hamming_loss: 0.4301\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 0.7714 - hamming_loss: 0.2024 - val_loss: 0.9727 - val_hamming_loss: 0.4283\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 0.7703 - hamming_loss: 0.1932 - val_loss: 0.9744 - val_hamming_loss: 0.4327\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 0.7691 - hamming_loss: 0.1984 - val_loss: 0.9727 - val_hamming_loss: 0.4274\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.7691 - hamming_loss: 0.1980 - val_loss: 0.9742 - val_hamming_loss: 0.4327\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.7684 - hamming_loss: 0.2002 - val_loss: 0.9733 - val_hamming_loss: 0.4309\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 0.7661 - hamming_loss: 0.1945 - val_loss: 0.9717 - val_hamming_loss: 0.4283\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 0.7645 - hamming_loss: 0.2015 - val_loss: 0.9724 - val_hamming_loss: 0.4318\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 0.7630 - hamming_loss: 0.1892 - val_loss: 0.9717 - val_hamming_loss: 0.4283\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 0.7623 - hamming_loss: 0.1906 - val_loss: 0.9725 - val_hamming_loss: 0.4274\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.7616 - hamming_loss: 0.1871 - val_loss: 0.9704 - val_hamming_loss: 0.4240\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.7608 - hamming_loss: 0.1888 - val_loss: 0.9712 - val_hamming_loss: 0.4257\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 0.7596 - hamming_loss: 0.1897 - val_loss: 0.9710 - val_hamming_loss: 0.4301\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 0.7596 - hamming_loss: 0.1962 - val_loss: 0.9706 - val_hamming_loss: 0.4283\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.7575 - hamming_loss: 0.1892 - val_loss: 0.9706 - val_hamming_loss: 0.4240\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.7564 - hamming_loss: 0.1901 - val_loss: 0.9693 - val_hamming_loss: 0.4266\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 0.7553 - hamming_loss: 0.1831 - val_loss: 0.9704 - val_hamming_loss: 0.4266\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.7546 - hamming_loss: 0.1866 - val_loss: 0.9694 - val_hamming_loss: 0.4248\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.7541 - hamming_loss: 0.1849 - val_loss: 0.9705 - val_hamming_loss: 0.4257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e16500550>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "model_simpleRNN_bpmll.fit(train_padded_hasLabel, Y_train_hasLabel, epochs = 100, \n",
    "                          validation_data = (test_padded_hasLabel, Y_test_hasLabel), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2902097902097902"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Learn a Threshold Function\n",
    "Y_train_pred = model_simpleRNN_bpmll.predict(train_padded_hasLabel)\n",
    "Y_test_pred = model_simpleRNN_bpmll.predict(test_padded_hasLabel)\n",
    "t_range = (0, 1)\n",
    "\n",
    "test_labels_binary, threshold_function = predict_test_labels_binary(Y_train_pred, Y_train_hasLabel, Y_test_pred, t_range)\n",
    "metrics.hamming_loss(Y_test_hasLabel, test_labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the LSTM RNN architecture\n",
    "model_LSTM_bpmll = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(num_unique_words, 32, input_length = max_length),\n",
    "    tf.keras.layers.LSTM(16, return_sequences = False, return_state = False),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_labels, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "optim_bpmll = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "#optim_func_LSTM = tf.keras.optimizers.Adagrad(\n",
    "#    learning_rate = 0.001, initial_accumulator_value = 0.1, epsilon = 1e-07,\n",
    "#    name = 'Adagrad')\n",
    "\n",
    "metric = tfa.metrics.HammingLoss(mode = 'multilabel', threshold = 0.5)\n",
    "model_LSTM_bpmll.compile(loss = bp_mll_loss, optimizer = optim_bpmll, metrics = metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 - 4s - loss: 1.0040 - hamming_loss: 0.6106 - val_loss: 1.0029 - val_hamming_loss: 0.5997\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 1.0033 - hamming_loss: 0.6106 - val_loss: 1.0022 - val_hamming_loss: 0.5997\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 1.0026 - hamming_loss: 0.6106 - val_loss: 1.0015 - val_hamming_loss: 0.5997\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 1.0018 - hamming_loss: 0.5800 - val_loss: 1.0008 - val_hamming_loss: 0.4895\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 1.0011 - hamming_loss: 0.4996 - val_loss: 1.0002 - val_hamming_loss: 0.4895\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 1.0004 - hamming_loss: 0.4419 - val_loss: 0.9995 - val_hamming_loss: 0.3829\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.9997 - hamming_loss: 0.3894 - val_loss: 0.9988 - val_hamming_loss: 0.3829\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.9990 - hamming_loss: 0.3894 - val_loss: 0.9981 - val_hamming_loss: 0.3829\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.9983 - hamming_loss: 0.3894 - val_loss: 0.9974 - val_hamming_loss: 0.3829\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.9975 - hamming_loss: 0.3894 - val_loss: 0.9967 - val_hamming_loss: 0.3829\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.9967 - hamming_loss: 0.3894 - val_loss: 0.9959 - val_hamming_loss: 0.3829\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.9960 - hamming_loss: 0.3894 - val_loss: 0.9952 - val_hamming_loss: 0.3829\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.9951 - hamming_loss: 0.3894 - val_loss: 0.9944 - val_hamming_loss: 0.3829\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.9943 - hamming_loss: 0.3894 - val_loss: 0.9935 - val_hamming_loss: 0.3829\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.9934 - hamming_loss: 0.3955 - val_loss: 0.9927 - val_hamming_loss: 0.4528\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.9925 - hamming_loss: 0.4545 - val_loss: 0.9918 - val_hamming_loss: 0.4528\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.9916 - hamming_loss: 0.4541 - val_loss: 0.9908 - val_hamming_loss: 0.4056\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.9906 - hamming_loss: 0.4052 - val_loss: 0.9898 - val_hamming_loss: 0.4056\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.9895 - hamming_loss: 0.4052 - val_loss: 0.9888 - val_hamming_loss: 0.4056\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.9883 - hamming_loss: 0.4056 - val_loss: 0.9876 - val_hamming_loss: 0.4161\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.9871 - hamming_loss: 0.4087 - val_loss: 0.9864 - val_hamming_loss: 0.4161\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.9858 - hamming_loss: 0.4082 - val_loss: 0.9851 - val_hamming_loss: 0.4161\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.9844 - hamming_loss: 0.4082 - val_loss: 0.9836 - val_hamming_loss: 0.4161\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.9828 - hamming_loss: 0.4082 - val_loss: 0.9820 - val_hamming_loss: 0.4161\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.9811 - hamming_loss: 0.4082 - val_loss: 0.9803 - val_hamming_loss: 0.4161\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.9792 - hamming_loss: 0.4082 - val_loss: 0.9783 - val_hamming_loss: 0.4161\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.9771 - hamming_loss: 0.4082 - val_loss: 0.9762 - val_hamming_loss: 0.4161\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.9748 - hamming_loss: 0.4082 - val_loss: 0.9739 - val_hamming_loss: 0.4161\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.9723 - hamming_loss: 0.4082 - val_loss: 0.9712 - val_hamming_loss: 0.4161\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.9693 - hamming_loss: 0.4078 - val_loss: 0.9682 - val_hamming_loss: 0.4161\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.9661 - hamming_loss: 0.4082 - val_loss: 0.9648 - val_hamming_loss: 0.4161\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.9623 - hamming_loss: 0.4082 - val_loss: 0.9609 - val_hamming_loss: 0.4161\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.9583 - hamming_loss: 0.4082 - val_loss: 0.9564 - val_hamming_loss: 0.4161\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.9532 - hamming_loss: 0.4082 - val_loss: 0.9515 - val_hamming_loss: 0.3584\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.9480 - hamming_loss: 0.3510 - val_loss: 0.9458 - val_hamming_loss: 0.3584\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.9418 - hamming_loss: 0.3505 - val_loss: 0.9394 - val_hamming_loss: 0.3584\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.9351 - hamming_loss: 0.3505 - val_loss: 0.9326 - val_hamming_loss: 0.3584\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.9276 - hamming_loss: 0.3497 - val_loss: 0.9254 - val_hamming_loss: 0.3584\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.9202 - hamming_loss: 0.3497 - val_loss: 0.9178 - val_hamming_loss: 0.3584\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.9122 - hamming_loss: 0.3497 - val_loss: 0.9105 - val_hamming_loss: 0.3584\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.9045 - hamming_loss: 0.3221 - val_loss: 0.9034 - val_hamming_loss: 0.2972\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.8973 - hamming_loss: 0.2946 - val_loss: 0.8968 - val_hamming_loss: 0.2972\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.8905 - hamming_loss: 0.2946 - val_loss: 0.8909 - val_hamming_loss: 0.2972\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.8846 - hamming_loss: 0.2657 - val_loss: 0.8854 - val_hamming_loss: 0.2587\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.8791 - hamming_loss: 0.2487 - val_loss: 0.8806 - val_hamming_loss: 0.2587\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.8741 - hamming_loss: 0.2487 - val_loss: 0.8763 - val_hamming_loss: 0.2587\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.8697 - hamming_loss: 0.2487 - val_loss: 0.8725 - val_hamming_loss: 0.2587\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.8659 - hamming_loss: 0.2487 - val_loss: 0.8689 - val_hamming_loss: 0.2587\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.8623 - hamming_loss: 0.2487 - val_loss: 0.8656 - val_hamming_loss: 0.2587\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.8589 - hamming_loss: 0.2487 - val_loss: 0.8627 - val_hamming_loss: 0.2587\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.8560 - hamming_loss: 0.2487 - val_loss: 0.8599 - val_hamming_loss: 0.2587\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.8532 - hamming_loss: 0.2487 - val_loss: 0.8574 - val_hamming_loss: 0.2587\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.8505 - hamming_loss: 0.2487 - val_loss: 0.8549 - val_hamming_loss: 0.2587\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.8480 - hamming_loss: 0.2487 - val_loss: 0.8526 - val_hamming_loss: 0.2587\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.8458 - hamming_loss: 0.2487 - val_loss: 0.8505 - val_hamming_loss: 0.2587\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.8435 - hamming_loss: 0.2487 - val_loss: 0.8484 - val_hamming_loss: 0.2587\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.8414 - hamming_loss: 0.2487 - val_loss: 0.8464 - val_hamming_loss: 0.2587\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.8394 - hamming_loss: 0.2487 - val_loss: 0.8445 - val_hamming_loss: 0.2587\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.8374 - hamming_loss: 0.2487 - val_loss: 0.8427 - val_hamming_loss: 0.2587\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.8355 - hamming_loss: 0.2487 - val_loss: 0.8409 - val_hamming_loss: 0.2587\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.8337 - hamming_loss: 0.2487 - val_loss: 0.8392 - val_hamming_loss: 0.2587\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.8320 - hamming_loss: 0.2487 - val_loss: 0.8375 - val_hamming_loss: 0.2587\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.8303 - hamming_loss: 0.2487 - val_loss: 0.8359 - val_hamming_loss: 0.2587\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.8286 - hamming_loss: 0.2487 - val_loss: 0.8343 - val_hamming_loss: 0.2587\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.8270 - hamming_loss: 0.2487 - val_loss: 0.8328 - val_hamming_loss: 0.2587\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.8255 - hamming_loss: 0.2487 - val_loss: 0.8313 - val_hamming_loss: 0.2587\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.8240 - hamming_loss: 0.2487 - val_loss: 0.8298 - val_hamming_loss: 0.2587\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.8225 - hamming_loss: 0.2487 - val_loss: 0.8284 - val_hamming_loss: 0.2587\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.8210 - hamming_loss: 0.2487 - val_loss: 0.8269 - val_hamming_loss: 0.2587\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.8196 - hamming_loss: 0.2487 - val_loss: 0.8255 - val_hamming_loss: 0.2587\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.8182 - hamming_loss: 0.2487 - val_loss: 0.8241 - val_hamming_loss: 0.2587\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.8168 - hamming_loss: 0.2487 - val_loss: 0.8228 - val_hamming_loss: 0.2587\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.8154 - hamming_loss: 0.2487 - val_loss: 0.8215 - val_hamming_loss: 0.2587\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.8141 - hamming_loss: 0.2487 - val_loss: 0.8201 - val_hamming_loss: 0.2587\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 0.8128 - hamming_loss: 0.2487 - val_loss: 0.8188 - val_hamming_loss: 0.2587\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.8115 - hamming_loss: 0.2487 - val_loss: 0.8175 - val_hamming_loss: 0.2587\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.8102 - hamming_loss: 0.2487 - val_loss: 0.8162 - val_hamming_loss: 0.2587\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.8089 - hamming_loss: 0.2487 - val_loss: 0.8149 - val_hamming_loss: 0.2587\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 0.8076 - hamming_loss: 0.2487 - val_loss: 0.8137 - val_hamming_loss: 0.2587\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.8064 - hamming_loss: 0.2487 - val_loss: 0.8123 - val_hamming_loss: 0.2587\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 0.8051 - hamming_loss: 0.2487 - val_loss: 0.8110 - val_hamming_loss: 0.2587\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 0.8038 - hamming_loss: 0.2487 - val_loss: 0.8098 - val_hamming_loss: 0.2587\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 0.8026 - hamming_loss: 0.2487 - val_loss: 0.8085 - val_hamming_loss: 0.2587\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 0.8013 - hamming_loss: 0.2487 - val_loss: 0.8072 - val_hamming_loss: 0.2587\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 0.8001 - hamming_loss: 0.2487 - val_loss: 0.8059 - val_hamming_loss: 0.2587\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.7988 - hamming_loss: 0.2487 - val_loss: 0.8046 - val_hamming_loss: 0.2587\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.7976 - hamming_loss: 0.1962 - val_loss: 0.8032 - val_hamming_loss: 0.1888\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 0.7963 - hamming_loss: 0.1831 - val_loss: 0.8019 - val_hamming_loss: 0.1888\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 0.7950 - hamming_loss: 0.1831 - val_loss: 0.8006 - val_hamming_loss: 0.1888\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 0.7937 - hamming_loss: 0.1831 - val_loss: 0.7992 - val_hamming_loss: 0.1888\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 0.7924 - hamming_loss: 0.1831 - val_loss: 0.7978 - val_hamming_loss: 0.1888\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.7911 - hamming_loss: 0.1831 - val_loss: 0.7965 - val_hamming_loss: 0.1888\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.7899 - hamming_loss: 0.1831 - val_loss: 0.7950 - val_hamming_loss: 0.1888\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 0.7886 - hamming_loss: 0.1831 - val_loss: 0.7937 - val_hamming_loss: 0.1888\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 0.7873 - hamming_loss: 0.1831 - val_loss: 0.7922 - val_hamming_loss: 0.1888\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.7860 - hamming_loss: 0.1831 - val_loss: 0.7908 - val_hamming_loss: 0.1888\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.7847 - hamming_loss: 0.1980 - val_loss: 0.7895 - val_hamming_loss: 0.2308\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 0.7834 - hamming_loss: 0.2155 - val_loss: 0.7881 - val_hamming_loss: 0.2308\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.7822 - hamming_loss: 0.2325 - val_loss: 0.7868 - val_hamming_loss: 0.2535\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.7810 - hamming_loss: 0.2487 - val_loss: 0.7855 - val_hamming_loss: 0.2535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e17233520>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "model_LSTM_bpmll.fit(train_padded_hasLabel, Y_train_hasLabel, epochs = 100, \n",
    "                     validation_data = (test_padded_hasLabel, Y_test_hasLabel), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18006993006993008"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Learn a Threshold Function\n",
    "Y_train_pred = model_LSTM_bpmll.predict(train_padded_hasLabel)\n",
    "Y_test_pred = model_LSTM_bpmll.predict(test_padded_hasLabel)\n",
    "t_range = (0, 1)\n",
    "\n",
    "test_labels_binary, threshold_function = predict_test_labels_binary(Y_train_pred, Y_train_hasLabel, Y_test_pred, t_range)\n",
    "metrics.hamming_loss(Y_test_hasLabel, test_labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the bidirectional LSTM RNN architecture\n",
    "model_biLSTM_bpmll = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(num_unique_words, 32, input_length = max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences = False, return_state = False)),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_labels, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "optim_bpmll = tf.keras.optimizers.Adam(lr=0.001)\n",
    "#optim_bpmll = tf.keras.optimizers.Adagrad(\n",
    "#    learning_rate = 0.1, initial_accumulator_value = 0.1, epsilon = 1e-07,\n",
    "#    name = 'Adagrad')\n",
    "\n",
    "#optim = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum = 0.8, epsilon=1e-07,)\n",
    "\n",
    "metric = tfa.metrics.HammingLoss(mode = 'multilabel', threshold = 0.5)\n",
    "model_biLSTM_bpmll.compile(loss = bp_mll_loss, optimizer = optim_bpmll, metrics = metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 - 4s - loss: 0.9963 - hamming_loss: 0.4738 - val_loss: 0.9916 - val_hamming_loss: 0.3759\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.9869 - hamming_loss: 0.3431 - val_loss: 0.9820 - val_hamming_loss: 0.3549\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.9746 - hamming_loss: 0.3475 - val_loss: 0.9679 - val_hamming_loss: 0.3584\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 0.9561 - hamming_loss: 0.3479 - val_loss: 0.9450 - val_hamming_loss: 0.3514\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.9259 - hamming_loss: 0.3230 - val_loss: 0.9044 - val_hamming_loss: 0.3042\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.8743 - hamming_loss: 0.3046 - val_loss: 0.8490 - val_hamming_loss: 0.3007\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.8240 - hamming_loss: 0.2985 - val_loss: 0.8097 - val_hamming_loss: 0.3007\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.7903 - hamming_loss: 0.2985 - val_loss: 0.7843 - val_hamming_loss: 0.3007\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.7678 - hamming_loss: 0.2985 - val_loss: 0.7658 - val_hamming_loss: 0.3007\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.7521 - hamming_loss: 0.2985 - val_loss: 0.7528 - val_hamming_loss: 0.3007\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.7408 - hamming_loss: 0.2985 - val_loss: 0.7438 - val_hamming_loss: 0.3007\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.7329 - hamming_loss: 0.2985 - val_loss: 0.7375 - val_hamming_loss: 0.3007\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.7267 - hamming_loss: 0.2985 - val_loss: 0.7325 - val_hamming_loss: 0.3007\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.7216 - hamming_loss: 0.2985 - val_loss: 0.7290 - val_hamming_loss: 0.3007\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.7173 - hamming_loss: 0.2985 - val_loss: 0.7253 - val_hamming_loss: 0.3007\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.7132 - hamming_loss: 0.2981 - val_loss: 0.7225 - val_hamming_loss: 0.3007\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.7090 - hamming_loss: 0.2946 - val_loss: 0.7198 - val_hamming_loss: 0.2998\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.7048 - hamming_loss: 0.2920 - val_loss: 0.7166 - val_hamming_loss: 0.2981\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.7004 - hamming_loss: 0.2902 - val_loss: 0.7141 - val_hamming_loss: 0.2981\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.6962 - hamming_loss: 0.2858 - val_loss: 0.7120 - val_hamming_loss: 0.2928\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.6916 - hamming_loss: 0.2592 - val_loss: 0.7090 - val_hamming_loss: 0.2771\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.6874 - hamming_loss: 0.2469 - val_loss: 0.7089 - val_hamming_loss: 0.2509\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.6834 - hamming_loss: 0.2356 - val_loss: 0.7086 - val_hamming_loss: 0.2465\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.6791 - hamming_loss: 0.2225 - val_loss: 0.7035 - val_hamming_loss: 0.2386\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.6738 - hamming_loss: 0.2128 - val_loss: 0.6994 - val_hamming_loss: 0.2360\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.6689 - hamming_loss: 0.2102 - val_loss: 0.7021 - val_hamming_loss: 0.2369\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.6644 - hamming_loss: 0.2067 - val_loss: 0.6952 - val_hamming_loss: 0.2351\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.6586 - hamming_loss: 0.1954 - val_loss: 0.6979 - val_hamming_loss: 0.2220\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.6533 - hamming_loss: 0.1731 - val_loss: 0.6938 - val_hamming_loss: 0.2142\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.6480 - hamming_loss: 0.1608 - val_loss: 0.6948 - val_hamming_loss: 0.2072\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.6428 - hamming_loss: 0.1565 - val_loss: 0.6915 - val_hamming_loss: 0.2150\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.6378 - hamming_loss: 0.1556 - val_loss: 0.6924 - val_hamming_loss: 0.2019\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.6332 - hamming_loss: 0.1534 - val_loss: 0.6900 - val_hamming_loss: 0.1993\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.6291 - hamming_loss: 0.1547 - val_loss: 0.6900 - val_hamming_loss: 0.2002\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.6248 - hamming_loss: 0.1582 - val_loss: 0.6929 - val_hamming_loss: 0.1949\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.6217 - hamming_loss: 0.1565 - val_loss: 0.6951 - val_hamming_loss: 0.1993\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.6182 - hamming_loss: 0.1543 - val_loss: 0.6962 - val_hamming_loss: 0.1976\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.6144 - hamming_loss: 0.1521 - val_loss: 0.6888 - val_hamming_loss: 0.2002\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.6106 - hamming_loss: 0.1521 - val_loss: 0.6866 - val_hamming_loss: 0.1984\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.6072 - hamming_loss: 0.1486 - val_loss: 0.6869 - val_hamming_loss: 0.2002\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.6041 - hamming_loss: 0.1486 - val_loss: 0.6909 - val_hamming_loss: 0.1932\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.6010 - hamming_loss: 0.1499 - val_loss: 0.6886 - val_hamming_loss: 0.1941\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.5980 - hamming_loss: 0.1486 - val_loss: 0.6884 - val_hamming_loss: 0.1976\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.5953 - hamming_loss: 0.1538 - val_loss: 0.6907 - val_hamming_loss: 0.1932\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.5933 - hamming_loss: 0.1565 - val_loss: 0.6943 - val_hamming_loss: 0.1949\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.5932 - hamming_loss: 0.1482 - val_loss: 0.6854 - val_hamming_loss: 0.1914\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.5898 - hamming_loss: 0.1521 - val_loss: 0.6937 - val_hamming_loss: 0.2133\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.5900 - hamming_loss: 0.1552 - val_loss: 0.6898 - val_hamming_loss: 0.1897\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.5859 - hamming_loss: 0.1490 - val_loss: 0.6921 - val_hamming_loss: 0.1879\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.5830 - hamming_loss: 0.1477 - val_loss: 0.6887 - val_hamming_loss: 0.1923\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.5805 - hamming_loss: 0.1508 - val_loss: 0.6881 - val_hamming_loss: 0.1906\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.5781 - hamming_loss: 0.1473 - val_loss: 0.6904 - val_hamming_loss: 0.1862\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.5760 - hamming_loss: 0.1451 - val_loss: 0.6875 - val_hamming_loss: 0.1906\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.5738 - hamming_loss: 0.1499 - val_loss: 0.6858 - val_hamming_loss: 0.1888\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.5716 - hamming_loss: 0.1464 - val_loss: 0.6877 - val_hamming_loss: 0.1871\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.5695 - hamming_loss: 0.1469 - val_loss: 0.6861 - val_hamming_loss: 0.1906\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.5674 - hamming_loss: 0.1442 - val_loss: 0.6876 - val_hamming_loss: 0.1897\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.5652 - hamming_loss: 0.1429 - val_loss: 0.6862 - val_hamming_loss: 0.1879\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.5631 - hamming_loss: 0.1416 - val_loss: 0.6857 - val_hamming_loss: 0.1888\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.5609 - hamming_loss: 0.1407 - val_loss: 0.6873 - val_hamming_loss: 0.1888\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.5587 - hamming_loss: 0.1381 - val_loss: 0.6854 - val_hamming_loss: 0.1879\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.5565 - hamming_loss: 0.1320 - val_loss: 0.6858 - val_hamming_loss: 0.1862\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.5544 - hamming_loss: 0.1307 - val_loss: 0.6832 - val_hamming_loss: 0.1853\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.5522 - hamming_loss: 0.1311 - val_loss: 0.6825 - val_hamming_loss: 0.1853\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.5502 - hamming_loss: 0.1307 - val_loss: 0.6848 - val_hamming_loss: 0.1836\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.5481 - hamming_loss: 0.1307 - val_loss: 0.6868 - val_hamming_loss: 0.1853\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.5460 - hamming_loss: 0.1281 - val_loss: 0.6838 - val_hamming_loss: 0.1836\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.5441 - hamming_loss: 0.1259 - val_loss: 0.6874 - val_hamming_loss: 0.1888\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.5421 - hamming_loss: 0.1237 - val_loss: 0.6880 - val_hamming_loss: 0.1844\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.5417 - hamming_loss: 0.1241 - val_loss: 0.6878 - val_hamming_loss: 0.1766\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.5396 - hamming_loss: 0.1206 - val_loss: 0.6957 - val_hamming_loss: 0.1888\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.5378 - hamming_loss: 0.1193 - val_loss: 0.6890 - val_hamming_loss: 0.1809\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.5361 - hamming_loss: 0.1136 - val_loss: 0.6818 - val_hamming_loss: 0.1897\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.5347 - hamming_loss: 0.1171 - val_loss: 0.6881 - val_hamming_loss: 0.1801\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 0.5317 - hamming_loss: 0.1084 - val_loss: 0.6916 - val_hamming_loss: 0.1801\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.5306 - hamming_loss: 0.1062 - val_loss: 0.6953 - val_hamming_loss: 0.1827\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.5288 - hamming_loss: 0.1036 - val_loss: 0.6929 - val_hamming_loss: 0.1836\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.5267 - hamming_loss: 0.1001 - val_loss: 0.6913 - val_hamming_loss: 0.1853\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 0.5247 - hamming_loss: 0.0948 - val_loss: 0.6890 - val_hamming_loss: 0.1783\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.5230 - hamming_loss: 0.0913 - val_loss: 0.6905 - val_hamming_loss: 0.1844\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 0.5212 - hamming_loss: 0.0896 - val_loss: 0.6870 - val_hamming_loss: 0.1792\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 0.5198 - hamming_loss: 0.0870 - val_loss: 0.6904 - val_hamming_loss: 0.1844\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 0.5180 - hamming_loss: 0.0865 - val_loss: 0.6857 - val_hamming_loss: 0.1774\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 0.5166 - hamming_loss: 0.0839 - val_loss: 0.6914 - val_hamming_loss: 0.1827\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 0.5151 - hamming_loss: 0.0848 - val_loss: 0.6873 - val_hamming_loss: 0.1748\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.5133 - hamming_loss: 0.0822 - val_loss: 0.6886 - val_hamming_loss: 0.1818\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.5118 - hamming_loss: 0.0809 - val_loss: 0.6867 - val_hamming_loss: 0.1748\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 0.5102 - hamming_loss: 0.0787 - val_loss: 0.6891 - val_hamming_loss: 0.1801\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 0.5088 - hamming_loss: 0.0787 - val_loss: 0.6851 - val_hamming_loss: 0.1705\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 0.5073 - hamming_loss: 0.0756 - val_loss: 0.6883 - val_hamming_loss: 0.1783\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 0.5060 - hamming_loss: 0.0774 - val_loss: 0.6857 - val_hamming_loss: 0.1713\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.5044 - hamming_loss: 0.0752 - val_loss: 0.6882 - val_hamming_loss: 0.1801\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.5029 - hamming_loss: 0.0752 - val_loss: 0.6835 - val_hamming_loss: 0.1696\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 0.5017 - hamming_loss: 0.0747 - val_loss: 0.6875 - val_hamming_loss: 0.1774\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 0.5001 - hamming_loss: 0.0721 - val_loss: 0.6853 - val_hamming_loss: 0.1705\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.4986 - hamming_loss: 0.0708 - val_loss: 0.6853 - val_hamming_loss: 0.1748\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.4969 - hamming_loss: 0.0712 - val_loss: 0.6884 - val_hamming_loss: 0.1731\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 0.4955 - hamming_loss: 0.0699 - val_loss: 0.6830 - val_hamming_loss: 0.1722\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.4942 - hamming_loss: 0.0677 - val_loss: 0.6890 - val_hamming_loss: 0.1722\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.4930 - hamming_loss: 0.0730 - val_loss: 0.6858 - val_hamming_loss: 0.1722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20db3ec8af0>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "model_biLSTM_bpmll.fit(train_padded_hasLabel, Y_train_hasLabel, epochs = 100, \n",
    "                       validation_data = (test_padded_hasLabel, Y_test_hasLabel), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x20dafbad640>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## OPTIONAL CELL FOR IF WE HAVE PRE-TRAINED WEIGHTS\n",
    "## Restore the weights\n",
    "model_biLSTM_bpmll.load_weights('Models/BPMLL/biLSTM_bpmll_weights_manyEpochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15034965034965034"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using a constant 0.5 threshold function, get the hamming loss for the trained network on the test set\n",
    "predictions = model_biLSTM_bpmll.predict(test_padded_hasLabel)\n",
    "predictions_binary = model_biLSTM_bpmll.predict(test_padded_hasLabel)\n",
    "for i in range(Y_test_hasLabel.shape[0]):\n",
    "    for j in range(Y_test_hasLabel.shape[1]):\n",
    "        if predictions_binary[i, j] > 0.5:\n",
    "            predictions_binary[i, j] = 1\n",
    "        else:\n",
    "            predictions_binary[i, j] = 0\n",
    "\n",
    "# Get the hamming loss\n",
    "metrics.hamming_loss(Y_test_hasLabel, predictions_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20716783216783216"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Learn a Threshold Function\n",
    "Y_train_pred = model_biLSTM_bpmll.predict(train_padded_hasLabel)\n",
    "Y_test_pred = model_biLSTM_bpmll.predict(test_padded_hasLabel)\n",
    "t_range = (0, 1)\n",
    "\n",
    "test_labels_binary, threshold_function = predict_test_labels_binary(Y_train_pred, Y_train_hasLabel, Y_test_pred, t_range)\n",
    "metrics.hamming_loss(Y_test_hasLabel, test_labels_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model weights (no need to save learned threshold, in this case)\n",
    "model_biLSTM_bpmll_filepath = 'Models/BPMLL/biLSTM_bpmll_weights_manyEpochs'\n",
    "model_biLSTM_bpmll.save_weights(model_biLSTM_bpmll_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
