{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Filename: RNN_Paragraph_Classification.ipynb\n",
    "#\n",
    "# Purpose: Multi-label Text-categorization, using recurrent neural networks, for paragraph-level\n",
    "#          data as part of STAT 6500 final project.\n",
    "\n",
    "# Author(s): Bobby (Robert) Lumpkin\n",
    "#\n",
    "# Library Dependencies: numpy, pandas, tensorflow, bpmll\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph Classification Using RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from bpmll import bp_mll_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import sys\n",
    "sys.path.append('../ThresholdFunctionLearning')    ## Append path to the ThresholdFunctionLearning directory to the interpreters\n",
    "                                                   ## search path\n",
    "from threshold_learning import predict_test_labels_binary    ## Import the 'predict_test_labels_binary()' function from the \n",
    "                                                             ## threshold_learning library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>threats/impacts</th>\n",
       "      <th>responses/actions</th>\n",
       "      <th>severity</th>\n",
       "      <th>susceptibility</th>\n",
       "      <th>self-efficacy</th>\n",
       "      <th>external-efficacy</th>\n",
       "      <th>response efficacy</th>\n",
       "      <th>public health</th>\n",
       "      <th>...</th>\n",
       "      <th>prosper</th>\n",
       "      <th>preview</th>\n",
       "      <th>moor</th>\n",
       "      <th>coverag</th>\n",
       "      <th>glow</th>\n",
       "      <th>profil</th>\n",
       "      <th>clash</th>\n",
       "      <th>incumb</th>\n",
       "      <th>frequent</th>\n",
       "      <th>unfound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>214236</td>\n",
       "      <td>MURPHY: Again Martha we are defacto staying at...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214232</td>\n",
       "      <td>GOV. PHIL MURPHY, (D-NJ): Yes. Good to be back...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214266</td>\n",
       "      <td>BEAUMONT (ON SCREEN UPPER LEFT - \"FRIDAY MARCH...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214246</td>\n",
       "      <td>But in the meantime, my message to Louisiana i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214238</td>\n",
       "      <td>MURPHY: Yeah listen, we had gotten another shi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   para_id                                          full_text  \\\n",
       "0   214236  MURPHY: Again Martha we are defacto staying at...   \n",
       "1   214232  GOV. PHIL MURPHY, (D-NJ): Yes. Good to be back...   \n",
       "2   214266  BEAUMONT (ON SCREEN UPPER LEFT - \"FRIDAY MARCH...   \n",
       "3   214246  But in the meantime, my message to Louisiana i...   \n",
       "4   214238  MURPHY: Yeah listen, we had gotten another shi...   \n",
       "\n",
       "   threats/impacts  responses/actions  severity  susceptibility  \\\n",
       "0                1                  1         0               1   \n",
       "1                1                  1         1               1   \n",
       "2                0                  1         0               0   \n",
       "3                1                  1         1               0   \n",
       "4                0                  1         0               0   \n",
       "\n",
       "   self-efficacy  external-efficacy  response efficacy  public health  ...  \\\n",
       "0              1                  0                  1              1  ...   \n",
       "1              1                  0                  1              1  ...   \n",
       "2              0                  1                  0              1  ...   \n",
       "3              1                  0                  1              1  ...   \n",
       "4              0                  1                  0              1  ...   \n",
       "\n",
       "   prosper  preview  moor  coverag  glow  profil  clash  incumb  frequent  \\\n",
       "0      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "1      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "2      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "3      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "4      0.0      0.0   0.0      0.0   0.0     0.0    0.0     0.0       0.0   \n",
       "\n",
       "  unfound  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "\n",
       "[5 rows x 2119 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load 'content_paragraphs_ready.csv' into a pandas dataframe\n",
    "data_filepath = \"..\\..\\dataset\\content_paragraphs_ready.csv\"\n",
    "paragraph_data = pd.read_csv(data_filepath)\n",
    "paragraph_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>threats/impacts</th>\n",
       "      <th>responses/actions</th>\n",
       "      <th>severity</th>\n",
       "      <th>susceptibility</th>\n",
       "      <th>self-efficacy</th>\n",
       "      <th>external-efficacy</th>\n",
       "      <th>response efficacy</th>\n",
       "      <th>public health</th>\n",
       "      <th>economy</th>\n",
       "      <th>education</th>\n",
       "      <th>political evaluation</th>\n",
       "      <th>racial conflict</th>\n",
       "      <th>international ralations/foreign policies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>214236</td>\n",
       "      <td>MURPHY: Again Martha we are defacto staying at...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214232</td>\n",
       "      <td>GOV. PHIL MURPHY, (D-NJ): Yes. Good to be back...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214266</td>\n",
       "      <td>BEAUMONT (ON SCREEN UPPER LEFT - \"FRIDAY MARCH...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214246</td>\n",
       "      <td>But in the meantime, my message to Louisiana i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214238</td>\n",
       "      <td>MURPHY: Yeah listen, we had gotten another shi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   para_id                                          full_text  \\\n",
       "0   214236  MURPHY: Again Martha we are defacto staying at...   \n",
       "1   214232  GOV. PHIL MURPHY, (D-NJ): Yes. Good to be back...   \n",
       "2   214266  BEAUMONT (ON SCREEN UPPER LEFT - \"FRIDAY MARCH...   \n",
       "3   214246  But in the meantime, my message to Louisiana i...   \n",
       "4   214238  MURPHY: Yeah listen, we had gotten another shi...   \n",
       "\n",
       "   threats/impacts  responses/actions  severity  susceptibility  \\\n",
       "0                1                  1         0               1   \n",
       "1                1                  1         1               1   \n",
       "2                0                  1         0               0   \n",
       "3                1                  1         1               0   \n",
       "4                0                  1         0               0   \n",
       "\n",
       "   self-efficacy  external-efficacy  response efficacy  public health  \\\n",
       "0              1                  0                  1              1   \n",
       "1              1                  0                  1              1   \n",
       "2              0                  1                  0              1   \n",
       "3              1                  0                  1              1   \n",
       "4              0                  1                  0              1   \n",
       "\n",
       "   economy  education  political evaluation  racial conflict  \\\n",
       "0        0          0                     0                0   \n",
       "1        0          0                     0                0   \n",
       "2        0          0                     0                0   \n",
       "3        0          0                     0                0   \n",
       "4        0          0                     0                0   \n",
       "\n",
       "   international ralations/foreign policies  \n",
       "0                                         0  \n",
       "1                                         0  \n",
       "2                                         0  \n",
       "3                                         0  \n",
       "4                                         0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Keep only the paragraph id, paragraph text, and labels\n",
    "to_keep = paragraph_data.columns[0:15]\n",
    "#to_keep\n",
    "paragraph_data = paragraph_data[to_keep]\n",
    "paragraph_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "# https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate/34294022\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"https?://(\\S+|www)\\.\\S+\")\n",
    "for t in paragraph_data.full_text:\n",
    "    matches = pattern.findall(t)\n",
    "    for match in matches:\n",
    "        print(t)\n",
    "        print(match)\n",
    "        print(pattern.sub(r\"\", t))\n",
    "    if len(matches) > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_data[\"full_text\"] = paragraph_data.full_text.map(remove_URL) # map(lambda x: remove_URL(x))\n",
    "paragraph_data[\"full_text\"] = paragraph_data.full_text.map(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine\n",
    "# has been programmed to ignore, both when indexing entries for searching and when retrieving them \n",
    "# as the result of a search query.\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_data[\"full_text\"] = paragraph_data.full_text.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "counter = counter_word(paragraph_data.full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 69), ('people', 61), ('trump', 49), ('new', 48), ('tests', 47)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unique_words = len(counter)\n",
    "counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the X and Y train and test matrices\n",
    "covariate_cols = ['full_text']\n",
    "label_cols = paragraph_data.columns.difference(['para_id'] + covariate_cols)\n",
    "\n",
    "X = paragraph_data.full_text.to_numpy()\n",
    "Y = paragraph_data[label_cols].to_numpy().astype(float)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words = num_unique_words)\n",
    "tokenizer.fit_on_texts(X_train) # fit only to training\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "johnson asked 70 local savannah religious leaders keep worship centers closed none leaders said would reopen johnson told religious leaders understood financial burden religious institutions closed said reach god without going building”\n",
      "[478, 72, 829, 830, 831, 316, 178, 114, 832, 228, 229, 833, 178, 1, 23, 230, 478, 57, 316, 178, 834, 144, 835, 316, 836, 229, 1, 479, 837, 179, 3, 838]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_par_length = 0\n",
    "for par in train_sequences:\n",
    "    if len(par) > max_par_length:\n",
    "        max_par_length = len(par)\n",
    "        \n",
    "max_par_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((194, 100), (96, 100))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Max number of words in a sequence\n",
    "max_length = 100\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen = max_length, padding = \"post\", truncating = \"post\")\n",
    "test_padded = pad_sequences(test_sequences, maxlen = max_length, padding = \"post\", truncating = \"post\")\n",
    "train_padded.shape, test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reversing the indices\n",
    "# flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n",
    "\n",
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[478, 72, 829, 830, 831, 316, 178, 114, 832, 228, 229, 833, 178, 1, 23, 230, 478, 57, 316, 178, 834, 144, 835, 316, 836, 229, 1, 479, 837, 179, 3, 838]\n",
      "johnson asked 70 local savannah religious leaders keep worship centers closed none leaders said would reopen johnson told religious leaders understood financial burden religious institutions closed said reach god without going building”\n"
     ]
    }
   ],
   "source": [
    "decoded_text = decode(train_sequences[0])\n",
    "\n",
    "print(train_sequences[0])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the simple RNN architecture\n",
    "num_labels = len(label_cols)\n",
    "\n",
    "model_simpleRNN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(num_unique_words, 32, input_length = max_length),\n",
    "    tf.keras.layers.SimpleRNN(16, return_sequences = False, return_state = False),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_labels, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "#metrics = tfa.metrics.hamming_loss_fn(mode = 'multi-label')\n",
    "model_simpleRNN.compile(loss = 'categorical_crossentropy', optimizer = optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 - 2s - loss: 7.6402 - val_loss: 7.8881\n",
      "Epoch 2/100\n",
      "7/7 - 0s - loss: 7.6057 - val_loss: 7.8699\n",
      "Epoch 3/100\n",
      "7/7 - 0s - loss: 7.5747 - val_loss: 7.8509\n",
      "Epoch 4/100\n",
      "7/7 - 0s - loss: 7.5452 - val_loss: 7.8327\n",
      "Epoch 5/100\n",
      "7/7 - 0s - loss: 7.5174 - val_loss: 7.8150\n",
      "Epoch 6/100\n",
      "7/7 - 0s - loss: 7.4897 - val_loss: 7.7966\n",
      "Epoch 7/100\n",
      "7/7 - 0s - loss: 7.4609 - val_loss: 7.7773\n",
      "Epoch 8/100\n",
      "7/7 - 0s - loss: 7.4315 - val_loss: 7.7573\n",
      "Epoch 9/100\n",
      "7/7 - 0s - loss: 7.4005 - val_loss: 7.7378\n",
      "Epoch 10/100\n",
      "7/7 - 0s - loss: 7.3696 - val_loss: 7.7201\n",
      "Epoch 11/100\n",
      "7/7 - 0s - loss: 7.3370 - val_loss: 7.6999\n",
      "Epoch 12/100\n",
      "7/7 - 0s - loss: 7.3020 - val_loss: 7.6820\n",
      "Epoch 13/100\n",
      "7/7 - 0s - loss: 7.2682 - val_loss: 7.6658\n",
      "Epoch 14/100\n",
      "7/7 - 0s - loss: 7.2312 - val_loss: 7.6518\n",
      "Epoch 15/100\n",
      "7/7 - 0s - loss: 7.1934 - val_loss: 7.6442\n",
      "Epoch 16/100\n",
      "7/7 - 0s - loss: 7.1578 - val_loss: 7.6425\n",
      "Epoch 17/100\n",
      "7/7 - 0s - loss: 7.1240 - val_loss: 7.6437\n",
      "Epoch 18/100\n",
      "7/7 - 0s - loss: 7.0898 - val_loss: 7.6363\n",
      "Epoch 19/100\n",
      "7/7 - 0s - loss: 7.0493 - val_loss: 7.6261\n",
      "Epoch 20/100\n",
      "7/7 - 0s - loss: 7.0058 - val_loss: 7.6140\n",
      "Epoch 21/100\n",
      "7/7 - 0s - loss: 6.9725 - val_loss: 7.6129\n",
      "Epoch 22/100\n",
      "7/7 - 0s - loss: 6.9464 - val_loss: 7.6038\n",
      "Epoch 23/100\n",
      "7/7 - 0s - loss: 6.9113 - val_loss: 7.5920\n",
      "Epoch 24/100\n",
      "7/7 - 0s - loss: 6.8791 - val_loss: 7.5956\n",
      "Epoch 25/100\n",
      "7/7 - 0s - loss: 6.8521 - val_loss: 7.5851\n",
      "Epoch 26/100\n",
      "7/7 - 0s - loss: 6.8225 - val_loss: 7.5739\n",
      "Epoch 27/100\n",
      "7/7 - 0s - loss: 6.7947 - val_loss: 7.5546\n",
      "Epoch 28/100\n",
      "7/7 - 0s - loss: 6.7708 - val_loss: 7.5508\n",
      "Epoch 29/100\n",
      "7/7 - 0s - loss: 6.7456 - val_loss: 7.5502\n",
      "Epoch 30/100\n",
      "7/7 - 0s - loss: 6.7266 - val_loss: 7.5472\n",
      "Epoch 31/100\n",
      "7/7 - 0s - loss: 6.6983 - val_loss: 7.5419\n",
      "Epoch 32/100\n",
      "7/7 - 0s - loss: 6.6835 - val_loss: 7.5238\n",
      "Epoch 33/100\n",
      "7/7 - 0s - loss: 6.6588 - val_loss: 7.4841\n",
      "Epoch 34/100\n",
      "7/7 - 0s - loss: 6.6412 - val_loss: 7.4807\n",
      "Epoch 35/100\n",
      "7/7 - 0s - loss: 6.6281 - val_loss: 7.4903\n",
      "Epoch 36/100\n",
      "7/7 - 0s - loss: 6.6235 - val_loss: 7.4856\n",
      "Epoch 37/100\n",
      "7/7 - 0s - loss: 6.5931 - val_loss: 7.4716\n",
      "Epoch 38/100\n",
      "7/7 - 0s - loss: 6.5774 - val_loss: 7.4726\n",
      "Epoch 39/100\n",
      "7/7 - 0s - loss: 6.5698 - val_loss: 7.4780\n",
      "Epoch 40/100\n",
      "7/7 - 0s - loss: 6.5600 - val_loss: 7.4679\n",
      "Epoch 41/100\n",
      "7/7 - 0s - loss: 6.5343 - val_loss: 7.4617\n",
      "Epoch 42/100\n",
      "7/7 - 0s - loss: 6.5255 - val_loss: 7.4636\n",
      "Epoch 43/100\n",
      "7/7 - 0s - loss: 6.5114 - val_loss: 7.4507\n",
      "Epoch 44/100\n",
      "7/7 - 0s - loss: 6.4979 - val_loss: 7.4548\n",
      "Epoch 45/100\n",
      "7/7 - 0s - loss: 6.4949 - val_loss: 7.4542\n",
      "Epoch 46/100\n",
      "7/7 - 0s - loss: 6.4754 - val_loss: 7.4551\n",
      "Epoch 47/100\n",
      "7/7 - 0s - loss: 6.4688 - val_loss: 7.4516\n",
      "Epoch 48/100\n",
      "7/7 - 0s - loss: 6.4544 - val_loss: 7.4336\n",
      "Epoch 49/100\n",
      "7/7 - 0s - loss: 6.4407 - val_loss: 7.4355\n",
      "Epoch 50/100\n",
      "7/7 - 0s - loss: 6.4422 - val_loss: 7.4273\n",
      "Epoch 51/100\n",
      "7/7 - 0s - loss: 6.4242 - val_loss: 7.4215\n",
      "Epoch 52/100\n",
      "7/7 - 0s - loss: 6.4153 - val_loss: 7.4319\n",
      "Epoch 53/100\n",
      "7/7 - 0s - loss: 6.4219 - val_loss: 7.4333\n",
      "Epoch 54/100\n",
      "7/7 - 0s - loss: 6.4034 - val_loss: 7.4187\n",
      "Epoch 55/100\n",
      "7/7 - 0s - loss: 6.3906 - val_loss: 7.4263\n",
      "Epoch 56/100\n",
      "7/7 - 0s - loss: 6.3899 - val_loss: 7.4259\n",
      "Epoch 57/100\n",
      "7/7 - 0s - loss: 6.3792 - val_loss: 7.4204\n",
      "Epoch 58/100\n",
      "7/7 - 0s - loss: 6.3728 - val_loss: 7.4164\n",
      "Epoch 59/100\n",
      "7/7 - 0s - loss: 6.3600 - val_loss: 7.4176\n",
      "Epoch 60/100\n",
      "7/7 - 0s - loss: 6.3628 - val_loss: 7.4111\n",
      "Epoch 61/100\n",
      "7/7 - 0s - loss: 6.3415 - val_loss: 7.4117\n",
      "Epoch 62/100\n",
      "7/7 - 0s - loss: 6.3439 - val_loss: 7.4134\n",
      "Epoch 63/100\n",
      "7/7 - 0s - loss: 6.3302 - val_loss: 7.4112\n",
      "Epoch 64/100\n",
      "7/7 - 0s - loss: 6.3242 - val_loss: 7.4122\n",
      "Epoch 65/100\n",
      "7/7 - 0s - loss: 6.3162 - val_loss: 7.4099\n",
      "Epoch 66/100\n",
      "7/7 - 0s - loss: 6.3104 - val_loss: 7.4153\n",
      "Epoch 67/100\n",
      "7/7 - 0s - loss: 6.3092 - val_loss: 7.3989\n",
      "Epoch 68/100\n",
      "7/7 - 0s - loss: 6.2889 - val_loss: 7.4061\n",
      "Epoch 69/100\n",
      "7/7 - 0s - loss: 6.2999 - val_loss: 7.3916\n",
      "Epoch 70/100\n",
      "7/7 - 0s - loss: 6.2779 - val_loss: 7.4016\n",
      "Epoch 71/100\n",
      "7/7 - 0s - loss: 6.2848 - val_loss: 7.3873\n",
      "Epoch 72/100\n",
      "7/7 - 0s - loss: 6.2669 - val_loss: 7.3946\n",
      "Epoch 73/100\n",
      "7/7 - 0s - loss: 6.2755 - val_loss: 7.3925\n",
      "Epoch 74/100\n",
      "7/7 - 0s - loss: 6.2557 - val_loss: 7.3885\n",
      "Epoch 75/100\n",
      "7/7 - 0s - loss: 6.2465 - val_loss: 7.3935\n",
      "Epoch 76/100\n",
      "7/7 - 0s - loss: 6.2469 - val_loss: 7.3912\n",
      "Epoch 77/100\n",
      "7/7 - 0s - loss: 6.2417 - val_loss: 7.4009\n",
      "Epoch 78/100\n",
      "7/7 - 0s - loss: 6.2399 - val_loss: 7.3896\n",
      "Epoch 79/100\n",
      "7/7 - 0s - loss: 6.2253 - val_loss: 7.3992\n",
      "Epoch 80/100\n",
      "7/7 - 0s - loss: 6.2249 - val_loss: 7.3758\n",
      "Epoch 81/100\n",
      "7/7 - 0s - loss: 6.2075 - val_loss: 7.3883\n",
      "Epoch 82/100\n",
      "7/7 - 0s - loss: 6.2184 - val_loss: 7.3615\n",
      "Epoch 83/100\n",
      "7/7 - 0s - loss: 6.2053 - val_loss: 7.3776\n",
      "Epoch 84/100\n",
      "7/7 - 0s - loss: 6.2032 - val_loss: 7.3575\n",
      "Epoch 85/100\n",
      "7/7 - 0s - loss: 6.1825 - val_loss: 7.3701\n",
      "Epoch 86/100\n",
      "7/7 - 0s - loss: 6.2000 - val_loss: 7.3567\n",
      "Epoch 87/100\n",
      "7/7 - 0s - loss: 6.1912 - val_loss: 7.3741\n",
      "Epoch 88/100\n",
      "7/7 - 0s - loss: 6.2011 - val_loss: 7.3541\n",
      "Epoch 89/100\n",
      "7/7 - 0s - loss: 6.1791 - val_loss: 7.3695\n",
      "Epoch 90/100\n",
      "7/7 - 0s - loss: 6.1847 - val_loss: 7.3412\n",
      "Epoch 91/100\n",
      "7/7 - 0s - loss: 6.1621 - val_loss: 7.3568\n",
      "Epoch 92/100\n",
      "7/7 - 0s - loss: 6.1771 - val_loss: 7.3459\n",
      "Epoch 93/100\n",
      "7/7 - 0s - loss: 6.1550 - val_loss: 7.3524\n",
      "Epoch 94/100\n",
      "7/7 - 0s - loss: 6.1640 - val_loss: 7.3547\n",
      "Epoch 95/100\n",
      "7/7 - 0s - loss: 6.1497 - val_loss: 7.3578\n",
      "Epoch 96/100\n",
      "7/7 - 0s - loss: 6.1560 - val_loss: 7.3705\n",
      "Epoch 97/100\n",
      "7/7 - 0s - loss: 6.1603 - val_loss: 7.3544\n",
      "Epoch 98/100\n",
      "7/7 - 0s - loss: 6.1501 - val_loss: 7.3633\n",
      "Epoch 99/100\n",
      "7/7 - 0s - loss: 6.1585 - val_loss: 7.3516\n",
      "Epoch 100/100\n",
      "7/7 - 0s - loss: 6.1373 - val_loss: 7.3472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d17dad02e0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "model_simpleRNN.fit(train_padded, Y_train, epochs = 100, validation_data = (test_padded, Y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4583333333333333"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using a constant 0.5 threshold function, get the hamming loss for the trained network on the test set\n",
    "predictions = model_simpleRNN.predict(test_padded)\n",
    "predictions_binary = model_simpleRNN.predict(test_padded)\n",
    "for i in range(Y_test.shape[0]):\n",
    "    for j in range(Y_test.shape[1]):\n",
    "        if predictions_binary[i, j] > 0.5:\n",
    "            predictions_binary[i, j] = 1\n",
    "        else:\n",
    "            predictions_binary[i, j] = 0\n",
    "\n",
    "# Get the hamming loss\n",
    "metrics.hamming_loss(Y_test, predictions_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23557692307692307"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Learn a Threshold Function\n",
    "Y_train_pred = model_simpleRNN.predict(train_padded)\n",
    "Y_test_pred = model_simpleRNN.predict(test_padded)\n",
    "t_range = (0, 1)\n",
    "\n",
    "test_labels_binary, threshold_function = predict_test_labels_binary(Y_train_pred, Y_train, Y_test_pred, t_range)\n",
    "metrics.hamming_loss(Y_test, test_labels_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_binary[0,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the LSTM RNN architecture\n",
    "num_labels = len(label_cols)\n",
    "\n",
    "model_LSTM = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(num_unique_words, 32, input_length = max_length),\n",
    "    tf.keras.layers.LSTM(16, return_sequences = False, return_state = False),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_labels, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "#optim_func_LSTM = tf.keras.optimizers.Adagrad(\n",
    "#    learning_rate = 0.001, initial_accumulator_value = 0.1, epsilon = 1e-07,\n",
    "#    name = 'Adagrad')\n",
    "\n",
    "#metrics = tfa.metrics.hamming_loss_fn(mode = 'multi-label')\n",
    "model_LSTM.compile(loss = 'categorical_crossentropy', optimizer = optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 - 3s - loss: 7.7051 - val_loss: 7.9704\n",
      "Epoch 2/50\n",
      "7/7 - 0s - loss: 7.6989 - val_loss: 7.9639\n",
      "Epoch 3/50\n",
      "7/7 - 0s - loss: 7.6925 - val_loss: 7.9572\n",
      "Epoch 4/50\n",
      "7/7 - 0s - loss: 7.6863 - val_loss: 7.9505\n",
      "Epoch 5/50\n",
      "7/7 - 0s - loss: 7.6800 - val_loss: 7.9439\n",
      "Epoch 6/50\n",
      "7/7 - 0s - loss: 7.6740 - val_loss: 7.9373\n",
      "Epoch 7/50\n",
      "7/7 - 0s - loss: 7.6677 - val_loss: 7.9303\n",
      "Epoch 8/50\n",
      "7/7 - 0s - loss: 7.6611 - val_loss: 7.9231\n",
      "Epoch 9/50\n",
      "7/7 - 0s - loss: 7.6544 - val_loss: 7.9161\n",
      "Epoch 10/50\n",
      "7/7 - 0s - loss: 7.6479 - val_loss: 7.9089\n",
      "Epoch 11/50\n",
      "7/7 - 0s - loss: 7.6411 - val_loss: 7.9015\n",
      "Epoch 12/50\n",
      "7/7 - 0s - loss: 7.6341 - val_loss: 7.8939\n",
      "Epoch 13/50\n",
      "7/7 - 0s - loss: 7.6268 - val_loss: 7.8856\n",
      "Epoch 14/50\n",
      "7/7 - 0s - loss: 7.6192 - val_loss: 7.8770\n",
      "Epoch 15/50\n",
      "7/7 - 0s - loss: 7.6114 - val_loss: 7.8686\n",
      "Epoch 16/50\n",
      "7/7 - 0s - loss: 7.6035 - val_loss: 7.8596\n",
      "Epoch 17/50\n",
      "7/7 - 0s - loss: 7.5947 - val_loss: 7.8491\n",
      "Epoch 18/50\n",
      "7/7 - 0s - loss: 7.5855 - val_loss: 7.8379\n",
      "Epoch 19/50\n",
      "7/7 - 0s - loss: 7.5748 - val_loss: 7.8259\n",
      "Epoch 20/50\n",
      "7/7 - 0s - loss: 7.5637 - val_loss: 7.8131\n",
      "Epoch 21/50\n",
      "7/7 - 0s - loss: 7.5517 - val_loss: 7.7992\n",
      "Epoch 22/50\n",
      "7/7 - 0s - loss: 7.5388 - val_loss: 7.7836\n",
      "Epoch 23/50\n",
      "7/7 - 0s - loss: 7.5241 - val_loss: 7.7663\n",
      "Epoch 24/50\n",
      "7/7 - 0s - loss: 7.5078 - val_loss: 7.7460\n",
      "Epoch 25/50\n",
      "7/7 - 0s - loss: 7.4888 - val_loss: 7.7230\n",
      "Epoch 26/50\n",
      "7/7 - 0s - loss: 7.4685 - val_loss: 7.6990\n",
      "Epoch 27/50\n",
      "7/7 - 0s - loss: 7.4469 - val_loss: 7.6714\n",
      "Epoch 28/50\n",
      "7/7 - 0s - loss: 7.4204 - val_loss: 7.6416\n",
      "Epoch 29/50\n",
      "7/7 - 0s - loss: 7.3934 - val_loss: 7.6076\n",
      "Epoch 30/50\n",
      "7/7 - 0s - loss: 7.3617 - val_loss: 7.5681\n",
      "Epoch 31/50\n",
      "7/7 - 0s - loss: 7.3255 - val_loss: 7.5240\n",
      "Epoch 32/50\n",
      "7/7 - 0s - loss: 7.2888 - val_loss: 7.4765\n",
      "Epoch 33/50\n",
      "7/7 - 0s - loss: 7.2464 - val_loss: 7.4293\n",
      "Epoch 34/50\n",
      "7/7 - 0s - loss: 7.2055 - val_loss: 7.3821\n",
      "Epoch 35/50\n",
      "7/7 - 0s - loss: 7.1656 - val_loss: 7.3344\n",
      "Epoch 36/50\n",
      "7/7 - 0s - loss: 7.1281 - val_loss: 7.2891\n",
      "Epoch 37/50\n",
      "7/7 - 0s - loss: 7.0902 - val_loss: 7.2463\n",
      "Epoch 38/50\n",
      "7/7 - 0s - loss: 7.0547 - val_loss: 7.2036\n",
      "Epoch 39/50\n",
      "7/7 - 0s - loss: 7.0201 - val_loss: 7.1635\n",
      "Epoch 40/50\n",
      "7/7 - 0s - loss: 6.9876 - val_loss: 7.1250\n",
      "Epoch 41/50\n",
      "7/7 - 0s - loss: 6.9539 - val_loss: 7.0887\n",
      "Epoch 42/50\n",
      "7/7 - 0s - loss: 6.9237 - val_loss: 7.0550\n",
      "Epoch 43/50\n",
      "7/7 - 0s - loss: 6.8954 - val_loss: 7.0251\n",
      "Epoch 44/50\n",
      "7/7 - 0s - loss: 6.8704 - val_loss: 6.9995\n",
      "Epoch 45/50\n",
      "7/7 - 0s - loss: 6.8487 - val_loss: 6.9761\n",
      "Epoch 46/50\n",
      "7/7 - 0s - loss: 6.8292 - val_loss: 6.9569\n",
      "Epoch 47/50\n",
      "7/7 - 0s - loss: 6.8129 - val_loss: 6.9396\n",
      "Epoch 48/50\n",
      "7/7 - 0s - loss: 6.7982 - val_loss: 6.9249\n",
      "Epoch 49/50\n",
      "7/7 - 0s - loss: 6.7852 - val_loss: 6.9117\n",
      "Epoch 50/50\n",
      "7/7 - 0s - loss: 6.7750 - val_loss: 6.9010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d11c7a0f10>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "model_LSTM.fit(train_padded, Y_train, epochs = 50, validation_data = (test_padded, Y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5024038461538461"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using a constant 0.5 threshold function, get the hamming loss for the trained network on the test set\n",
    "predictions = model_LSTM.predict(test_padded)\n",
    "predictions_binary = model_LSTM.predict(test_padded)\n",
    "for i in range(Y_test.shape[0]):\n",
    "    for j in range(Y_test.shape[1]):\n",
    "        if predictions_binary[i, j] > 0.5:\n",
    "            predictions_binary[i, j] = 1\n",
    "        else:\n",
    "            predictions_binary[i, j] = 0\n",
    "\n",
    "# Get the hamming loss\n",
    "metrics.hamming_loss(Y_test, predictions_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17708333333333334"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Learn a Threshold Function\n",
    "Y_train_pred = model_LSTM.predict(train_padded)\n",
    "Y_test_pred = model_LSTM.predict(test_padded)\n",
    "t_range = (0, 1)\n",
    "\n",
    "test_labels_binary, threshold_function = predict_test_labels_binary(Y_train_pred, Y_train, Y_test_pred, t_range)\n",
    "metrics.hamming_loss(Y_test, test_labels_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_binary[0,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the LSTM RNN architecture\n",
    "num_labels = len(label_cols)\n",
    "\n",
    "model_biLSTM = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(num_unique_words, 32, input_length = max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences = False, return_state = False)),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_labels, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "#optim = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "optim = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate = 0.001, initial_accumulator_value = 0.1, epsilon = 1e-07,\n",
    "    name = 'Adagrad')\n",
    "\n",
    "#optim = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum = 0.8, epsilon=1e-07,)\n",
    "\n",
    "#metrics = tfa.metrics.hamming_loss_fn(mode = 'multi-label')\n",
    "model_biLSTM.compile(loss = 'categorical_crossentropy', optimizer = optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 - 9s - loss: 7.6779 - val_loss: 7.9388\n",
      "Epoch 2/100\n",
      "7/7 - 0s - loss: 7.6626 - val_loss: 7.9254\n",
      "Epoch 3/100\n",
      "7/7 - 0s - loss: 7.6504 - val_loss: 7.9143\n",
      "Epoch 4/100\n",
      "7/7 - 0s - loss: 7.6400 - val_loss: 7.9048\n",
      "Epoch 5/100\n",
      "7/7 - 0s - loss: 7.6311 - val_loss: 7.8965\n",
      "Epoch 6/100\n",
      "7/7 - 0s - loss: 7.6233 - val_loss: 7.8883\n",
      "Epoch 7/100\n",
      "7/7 - 0s - loss: 7.6155 - val_loss: 7.8802\n",
      "Epoch 8/100\n",
      "7/7 - 0s - loss: 7.6077 - val_loss: 7.8737\n",
      "Epoch 9/100\n",
      "7/7 - 0s - loss: 7.6014 - val_loss: 7.8673\n",
      "Epoch 10/100\n",
      "7/7 - 0s - loss: 7.5951 - val_loss: 7.8610\n",
      "Epoch 11/100\n",
      "7/7 - 0s - loss: 7.5889 - val_loss: 7.8552\n",
      "Epoch 12/100\n",
      "7/7 - 0s - loss: 7.5834 - val_loss: 7.8489\n",
      "Epoch 13/100\n",
      "7/7 - 0s - loss: 7.5771 - val_loss: 7.8428\n",
      "Epoch 14/100\n",
      "7/7 - 0s - loss: 7.5714 - val_loss: 7.8382\n",
      "Epoch 15/100\n",
      "7/7 - 0s - loss: 7.5669 - val_loss: 7.8332\n",
      "Epoch 16/100\n",
      "7/7 - 0s - loss: 7.5620 - val_loss: 7.8271\n",
      "Epoch 17/100\n",
      "7/7 - 0s - loss: 7.5561 - val_loss: 7.8220\n",
      "Epoch 18/100\n",
      "7/7 - 0s - loss: 7.5511 - val_loss: 7.8169\n",
      "Epoch 19/100\n",
      "7/7 - 0s - loss: 7.5464 - val_loss: 7.8123\n",
      "Epoch 20/100\n",
      "7/7 - 0s - loss: 7.5418 - val_loss: 7.8072\n",
      "Epoch 21/100\n",
      "7/7 - 0s - loss: 7.5368 - val_loss: 7.8021\n",
      "Epoch 22/100\n",
      "7/7 - 0s - loss: 7.5319 - val_loss: 7.7971\n",
      "Epoch 23/100\n",
      "7/7 - 0s - loss: 7.5269 - val_loss: 7.7914\n",
      "Epoch 24/100\n",
      "7/7 - 0s - loss: 7.5214 - val_loss: 7.7857\n",
      "Epoch 25/100\n",
      "7/7 - 0s - loss: 7.5159 - val_loss: 7.7814\n",
      "Epoch 26/100\n",
      "7/7 - 0s - loss: 7.5117 - val_loss: 7.7764\n",
      "Epoch 27/100\n",
      "7/7 - 0s - loss: 7.5069 - val_loss: 7.7716\n",
      "Epoch 28/100\n",
      "7/7 - 0s - loss: 7.5021 - val_loss: 7.7668\n",
      "Epoch 29/100\n",
      "7/7 - 0s - loss: 7.4972 - val_loss: 7.7614\n",
      "Epoch 30/100\n",
      "7/7 - 0s - loss: 7.4919 - val_loss: 7.7561\n",
      "Epoch 31/100\n",
      "7/7 - 0s - loss: 7.4867 - val_loss: 7.7509\n",
      "Epoch 32/100\n",
      "7/7 - 0s - loss: 7.4816 - val_loss: 7.7457\n",
      "Epoch 33/100\n",
      "7/7 - 0s - loss: 7.4766 - val_loss: 7.7404\n",
      "Epoch 34/100\n",
      "7/7 - 0s - loss: 7.4712 - val_loss: 7.7346\n",
      "Epoch 35/100\n",
      "7/7 - 0s - loss: 7.4657 - val_loss: 7.7292\n",
      "Epoch 36/100\n",
      "7/7 - 0s - loss: 7.4605 - val_loss: 7.7237\n",
      "Epoch 37/100\n",
      "7/7 - 0s - loss: 7.4550 - val_loss: 7.7178\n",
      "Epoch 38/100\n",
      "7/7 - 0s - loss: 7.4493 - val_loss: 7.7123\n",
      "Epoch 39/100\n",
      "7/7 - 0s - loss: 7.4440 - val_loss: 7.7066\n",
      "Epoch 40/100\n",
      "7/7 - 0s - loss: 7.4385 - val_loss: 7.7005\n",
      "Epoch 41/100\n",
      "7/7 - 0s - loss: 7.4325 - val_loss: 7.6946\n",
      "Epoch 42/100\n",
      "7/7 - 0s - loss: 7.4267 - val_loss: 7.6891\n",
      "Epoch 43/100\n",
      "7/7 - 0s - loss: 7.4212 - val_loss: 7.6835\n",
      "Epoch 44/100\n",
      "7/7 - 0s - loss: 7.4157 - val_loss: 7.6775\n",
      "Epoch 45/100\n",
      "7/7 - 0s - loss: 7.4099 - val_loss: 7.6725\n",
      "Epoch 46/100\n",
      "7/7 - 0s - loss: 7.4050 - val_loss: 7.6660\n",
      "Epoch 47/100\n",
      "7/7 - 0s - loss: 7.3986 - val_loss: 7.6597\n",
      "Epoch 48/100\n",
      "7/7 - 0s - loss: 7.3923 - val_loss: 7.6530\n",
      "Epoch 49/100\n",
      "7/7 - 0s - loss: 7.3858 - val_loss: 7.6461\n",
      "Epoch 50/100\n",
      "7/7 - 0s - loss: 7.3791 - val_loss: 7.6399\n",
      "Epoch 51/100\n",
      "7/7 - 0s - loss: 7.3730 - val_loss: 7.6327\n",
      "Epoch 52/100\n",
      "7/7 - 0s - loss: 7.3659 - val_loss: 7.6271\n",
      "Epoch 53/100\n",
      "7/7 - 0s - loss: 7.3604 - val_loss: 7.6200\n",
      "Epoch 54/100\n",
      "7/7 - 0s - loss: 7.3535 - val_loss: 7.6137\n",
      "Epoch 55/100\n",
      "7/7 - 0s - loss: 7.3473 - val_loss: 7.6073\n",
      "Epoch 56/100\n",
      "7/7 - 0s - loss: 7.3411 - val_loss: 7.5996\n",
      "Epoch 57/100\n",
      "7/7 - 0s - loss: 7.3337 - val_loss: 7.5919\n",
      "Epoch 58/100\n",
      "7/7 - 0s - loss: 7.3261 - val_loss: 7.5842\n",
      "Epoch 59/100\n",
      "7/7 - 0s - loss: 7.3186 - val_loss: 7.5776\n",
      "Epoch 60/100\n",
      "7/7 - 0s - loss: 7.3120 - val_loss: 7.5700\n",
      "Epoch 61/100\n",
      "7/7 - 0s - loss: 7.3045 - val_loss: 7.5628\n",
      "Epoch 62/100\n",
      "7/7 - 0s - loss: 7.2976 - val_loss: 7.5558\n",
      "Epoch 63/100\n",
      "7/7 - 0s - loss: 7.2906 - val_loss: 7.5475\n",
      "Epoch 64/100\n",
      "7/7 - 0s - loss: 7.2825 - val_loss: 7.5396\n",
      "Epoch 65/100\n",
      "7/7 - 0s - loss: 7.2749 - val_loss: 7.5311\n",
      "Epoch 66/100\n",
      "7/7 - 0s - loss: 7.2666 - val_loss: 7.5245\n",
      "Epoch 67/100\n",
      "7/7 - 0s - loss: 7.2599 - val_loss: 7.5166\n",
      "Epoch 68/100\n",
      "7/7 - 0s - loss: 7.2523 - val_loss: 7.5085\n",
      "Epoch 69/100\n",
      "7/7 - 0s - loss: 7.2442 - val_loss: 7.4994\n",
      "Epoch 70/100\n",
      "7/7 - 0s - loss: 7.2353 - val_loss: 7.4905\n",
      "Epoch 71/100\n",
      "7/7 - 0s - loss: 7.2264 - val_loss: 7.4817\n",
      "Epoch 72/100\n",
      "7/7 - 0s - loss: 7.2178 - val_loss: 7.4724\n",
      "Epoch 73/100\n",
      "7/7 - 0s - loss: 7.2086 - val_loss: 7.4628\n",
      "Epoch 74/100\n",
      "7/7 - 0s - loss: 7.1990 - val_loss: 7.4536\n",
      "Epoch 75/100\n",
      "7/7 - 0s - loss: 7.1901 - val_loss: 7.4445\n",
      "Epoch 76/100\n",
      "7/7 - 0s - loss: 7.1813 - val_loss: 7.4359\n",
      "Epoch 77/100\n",
      "7/7 - 0s - loss: 7.1728 - val_loss: 7.4274\n",
      "Epoch 78/100\n",
      "7/7 - 0s - loss: 7.1645 - val_loss: 7.4185\n",
      "Epoch 79/100\n",
      "7/7 - 0s - loss: 7.1557 - val_loss: 7.4093\n",
      "Epoch 80/100\n",
      "7/7 - 0s - loss: 7.1461 - val_loss: 7.3994\n",
      "Epoch 81/100\n",
      "7/7 - 0s - loss: 7.1364 - val_loss: 7.3897\n",
      "Epoch 82/100\n",
      "7/7 - 0s - loss: 7.1269 - val_loss: 7.3809\n",
      "Epoch 83/100\n",
      "7/7 - 0s - loss: 7.1182 - val_loss: 7.3722\n",
      "Epoch 84/100\n",
      "7/7 - 0s - loss: 7.1094 - val_loss: 7.3626\n",
      "Epoch 85/100\n",
      "7/7 - 0s - loss: 7.0999 - val_loss: 7.3530\n",
      "Epoch 86/100\n",
      "7/7 - 0s - loss: 7.0902 - val_loss: 7.3438\n",
      "Epoch 87/100\n",
      "7/7 - 0s - loss: 7.0811 - val_loss: 7.3339\n",
      "Epoch 88/100\n",
      "7/7 - 0s - loss: 7.0711 - val_loss: 7.3250\n",
      "Epoch 89/100\n",
      "7/7 - 0s - loss: 7.0621 - val_loss: 7.3154\n",
      "Epoch 90/100\n",
      "7/7 - 0s - loss: 7.0524 - val_loss: 7.3063\n",
      "Epoch 91/100\n",
      "7/7 - 0s - loss: 7.0434 - val_loss: 7.2964\n",
      "Epoch 92/100\n",
      "7/7 - 0s - loss: 7.0332 - val_loss: 7.2869\n",
      "Epoch 93/100\n",
      "7/7 - 0s - loss: 7.0237 - val_loss: 7.2784\n",
      "Epoch 94/100\n",
      "7/7 - 0s - loss: 7.0152 - val_loss: 7.2693\n",
      "Epoch 95/100\n",
      "7/7 - 0s - loss: 7.0060 - val_loss: 7.2596\n",
      "Epoch 96/100\n",
      "7/7 - 0s - loss: 6.9962 - val_loss: 7.2518\n",
      "Epoch 97/100\n",
      "7/7 - 0s - loss: 6.9878 - val_loss: 7.2424\n",
      "Epoch 98/100\n",
      "7/7 - 0s - loss: 6.9783 - val_loss: 7.2329\n",
      "Epoch 99/100\n",
      "7/7 - 0s - loss: 6.9685 - val_loss: 7.2239\n",
      "Epoch 100/100\n",
      "7/7 - 0s - loss: 6.9594 - val_loss: 7.2151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d19d7271c0>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "model_biLSTM.fit(train_padded, Y_train, epochs = 100, validation_data = (test_padded, Y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5208333333333334"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using a constant 0.5 threshold function, get the hamming loss for the trained network on the test set\n",
    "predictions = model_biLSTM.predict(test_padded)\n",
    "predictions_binary = model_biLSTM.predict(test_padded)\n",
    "for i in range(Y_test.shape[0]):\n",
    "    for j in range(Y_test.shape[1]):\n",
    "        if predictions_binary[i, j] > 0.5:\n",
    "            predictions_binary[i, j] = 1\n",
    "        else:\n",
    "            predictions_binary[i, j] = 0\n",
    "\n",
    "# Get the hamming loss\n",
    "metrics.hamming_loss(Y_test, predictions_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20112179487179488"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Learn a Threshold Function\n",
    "Y_train_pred = model_biLSTM.predict(train_padded)\n",
    "Y_test_pred = model_biLSTM.predict(test_padded)\n",
    "t_range = (0, 1)\n",
    "\n",
    "test_labels_binary, threshold_function = predict_test_labels_binary(Y_train_pred, Y_train, Y_test_pred, t_range)\n",
    "metrics.hamming_loss(Y_test, test_labels_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_binary[0,]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
