setwd("C:/Users/li.7232/OneDrive - The Ohio State University/NewsArticleClassification")
knitr::opts_chunk$set(echo = TRUE)
library(quanteda)
library(readtext)
library(stringr)
data <- readtext("dataframe_with_article_data.csv", text_field = c("title", "Article_text"))
View(data)
data[["text"]] <-  stringr::str_replace_all(data[["text"]],"[^a-zA-Z\\s]", " ")
data[["text"]] <- stringr::str_replace_all(data[["text"]],"[\\s]+", " ")
titles_corpus <- corpus(data)
summary(titles_corpus)
kwic(titles_corpus, "trump")
titles_tokens <- tokens(titles_corpus, remove_numbers = TRUE, remove_symbols = TRUE, remove_punct = TRUE, remove_separators = TRUE, include_docvars = TRUE)
View(titles_tokens)
titles_tokens[["dataframe_with_article_data.csv.1"]]
View(data)
raw <- read.csv("dataframe_with_article_data.csv", sep = ",", header = TRUE)
raw$text <- with(raw, paste(title, Article_text))
View(raw)
head(raw)
write.csv(raw,"dataframe_with_article_data.csv")
data <- readtext("dataframe_with_article_data.csv", text_field = c("text"))
data[["text"]] <-  stringr::str_replace_all(data[["text"]],"[^a-zA-Z\\s]", " ")
data[["text"]] <- stringr::str_replace_all(data[["text"]],"[\\s]+", " ")
View(data)
corpus <- corpus(data)
wh_corpus <- corpus(data)
wh_tokens <- tokens(titles_corpus, remove_numbers = TRUE, remove_symbols = TRUE, remove_punct = TRUE, remove_separators = TRUE, include_docvars = TRUE)
wh_tokens <- tokens(wh_corpus, remove_numbers = TRUE, remove_symbols = TRUE, remove_punct = TRUE, remove_separators = TRUE, include_docvars = TRUE)
wh_tokens <- tokens_tolower(wh_tokens)
wh_tokens <- tokens_remove(wh_tokens, stopwords("english"))
wh_tokens <- tokens_wordstem(wh_tokens)
wh_dfm <- dfm(wh_tokens)
wh_dfm_trim <- dfm_trim(wh_dfm, min_docfreq = 0.025, max_docfreq = 0.975, docfreq_type = "prop")
head(dfm_sort(wh_dfm_trim, decreasing =TRUE, margin = "both"), n = 10, nf = 10)
wh_dataframe <- convert(wh_dfm, to = "data.frame", docid_field = "doc_id")
ready_to_use <- merge(x = wh_dataframe, y = data, by = "doc_id", all = TRUE)
View(ready_to_use)
View(data)
View(ready_to_use)
original_cols <- colnames(data)
drop_cols <- original_cols[!(original_cols %in% c('doc_id', 'related', 'domain'))]
ready_to_use_NoOriginalData <- ready_to_use[ , !(names(ready_to_use) %in% drop_cols)]
dim(ready_to_use_NoOriginalData)
names(ready_to_use_NoOriginalData)[0:50]
table(ready_to_use_NoOriginalData$related)
table(ready_to_use_NoOriginalData$related) / nrow(ready_to_use)
related_cases <- ready_to_use_NoOriginalData[ready_to_use_NoOriginalData$related == 1, ][,-1]
related_cases <- related_cases[ , which(colnames(related_cases) != 'related')]
View(related_cases)
related_cases_means <- colMeans(related_cases)
View(ready_to_use_NoOriginalData)
original_cols <- colnames(data)
original_cols <- colnames(data)
original_cols <- colnames(data)
drop_cols <- original_cols[!(original_cols %in% c('doc_id', 'related'))]
ready_to_use_NoOriginalData <- ready_to_use[ , !(names(ready_to_use) %in% drop_cols)]
# Check the data dimension
dim(ready_to_use_NoOriginalData)
# See some of the variable names
names(ready_to_use_NoOriginalData)[0:50]
# Distribution of the response
table(ready_to_use_NoOriginalData$related)
# Sample proportion
table(ready_to_use_NoOriginalData$related) / nrow(ready_to_use)
#Below, we look to see what proportion of "related" articles have a given word in their title, what proportion of "non-related" articles have a word in their title, and the words with largest absolute difference between those two groups. These words might be most useful as features.
##```{r examining predictors}
## proportion of related articles that include words in title
related_cases <- ready_to_use_NoOriginalData[ready_to_use_NoOriginalData$related == 1, ][,-1]
related_cases <- related_cases[ , which(colnames(related_cases) != 'related')]
related_cases_means <- colMeans(related_cases)
View(related_cases)
View(related_cases)
str(ready_to_use_NoOriginalData)
test <- str(ready_to_use_NoOriginalData)
data <- readtext("dataframe_with_article_data.csv", text_field = c("text"))
#clean text
data[["text"]] <-  stringr::str_replace_all(data[["text"]],"[^a-zA-Z\\s]", " ")
data[["text"]] <- stringr::str_replace_all(data[["text"]],"[\\s]+", " ")
wh_corpus <- corpus(data)
wh_tokens <- tokens(wh_corpus, remove_numbers = TRUE, remove_symbols = TRUE, remove_punct = TRUE, remove_separators = TRUE, include_docvars = TRUE)
wh_tokens <- tokens_tolower(wh_tokens)
#remove stopwords, such as "a" "an" "the" "and" etc
wh_tokens <- tokens_remove(wh_tokens, stopwords("english"))
wh_tokens <- tokens_wordstem(wh_tokens)
#create document-feature matrix
wh_dfm <- dfm(wh_tokens)
wh_dataframe <- convert(wh_dfm, to = "data.frame", docid_field = "doc_id")
#add the document-level variables to the data frame
ready_to_use <- merge(x = wh_dataframe, y = data, by = "doc_id", all = TRUE)
# remove the non-binary data from original data set (what existed before pre-processing)
original_cols <- colnames(data)
drop_cols <- original_cols[!(original_cols %in% c('doc_id', 'related'))]
ready_to_use_NoOriginalData <- ready_to_use[ , !(names(ready_to_use) %in% drop_cols)]
# Check the data dimension
dim(ready_to_use_NoOriginalData)
# See some of the variable names
names(ready_to_use_NoOriginalData)[0:50]
# Distribution of the response
table(ready_to_use_NoOriginalData$related)
# Sample proportion
table(ready_to_use_NoOriginalData$related) / nrow(ready_to_use)
#Below, we look to see what proportion of "related" articles have a given word in their title, what proportion of "non-related" articles have a word in their title, and the words with largest absolute difference between those two groups. These words might be most useful as features.
##```{r examining predictors}
## proportion of related articles that include words in title
related_cases <- ready_to_use_NoOriginalData[ready_to_use_NoOriginalData$related == 1, ][,-1]
related_cases <- related_cases[ , which(colnames(related_cases) != 'related')]
related_cases_means <- colMeans(related_cases)
View(ready_to_use_NoOriginalData)
View(related_cases)
drop_cols
View(ready_to_use)
drop_cols <- c("text.y", "V1", "id.y", "title", "url", "seendate", "socialimage", "domain.y", "language", "sourcecountry", "Index", "Article_text")
ready_to_use_NoOriginalData <- ready_to_use[ , !(names(ready_to_use) %in% drop_cols)]
dim(ready_to_use_NoOriginalData)
names(ready_to_use_NoOriginalData)[0:50]
table(ready_to_use_NoOriginalData$related)
table(ready_to_use_NoOriginalData$related) / nrow(ready_to_use)
related_cases <- ready_to_use_NoOriginalData[ready_to_use_NoOriginalData$related == 1, ][,-1]
related_cases <- related_cases[ , which(colnames(related_cases) != 'related')]
related_cases_means <- colMeans(related_cases)
related_cases_means[1:10]
## proportion of non-related articles that include words in title
non_related_cases <- ready_to_use_NoOriginalData[ready_to_use_NoOriginalData$related == 0, ][,-1]
non_related_cases <- non_related_cases[ , which(colnames(non_related_cases) != 'related')]
non_related_cases_means <- colMeans(non_related_cases)
non_related_cases_means[1:10]
## Absolute difference between them
abs_diff_means <- abs(related_cases_means - non_related_cases_means)
## Round and sort the differences
sorted_diffs <- round(sort(abs_diff_means, decreasing = TRUE), digits = 4)
sorted_diffs[1:10]
tail(sort_diffs, 10)
tail(sorted_diffs, 10)
head(sorted_diffs, 10)
