---
title: "EDA Report"
author: "Bobby Lumpkin"
date: 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Text Preprocessing

```{r preprocessing}
####################
#text preprocessing#
####################
#instal packages if you did not use them before. Ignore this step if you have these packages installed. 
#install.packages("quanteda")
#install.packages("readtext")
#install.packages("stringr")

#load the library
library(quanteda)
library(readtext)
library(stringr)

#read in the file using the readtext package
data <- readtext("dataframe_with_article_data.csv", text_field = c("text"))

#clean text
data[["text"]] <-  stringr::str_replace_all(data[["text"]],"[^a-zA-Z\\s]", " ")
data[["text"]] <- stringr::str_replace_all(data[["text"]],"[\\s]+", " ")

#build a corpus, that is a bag of words
wh_corpus <- corpus(data)

#explore the corpus texts a little bit. You can skip this step.
kwic(wh_corpus, "trump")

#create tokens (https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/advancing-text-mining/#supervised)
wh_tokens <- tokens(wh_corpus, remove_numbers = TRUE, remove_symbols = TRUE, remove_punct = TRUE, remove_separators = TRUE, include_docvars = TRUE)

#lower case the tokens
wh_tokens <- tokens_tolower(wh_tokens)

#remove stopwords, such as "a" "an" "the" "and" etc
wh_tokens <- tokens_remove(wh_tokens, stopwords("english"))

#stem the tokens (e.g., learning -> learn, learned -> learn)
wh_tokens <- tokens_wordstem(wh_tokens)

#create document-feature matrix
wh_dfm <- dfm(wh_tokens)

#trim the text with dfm_trim. filter out words that appear less than 2.5% and more than 97.5% because these words may be less useful for prediction.
wh_dfm_trim <- dfm_trim(wh_dfm, min_docfreq = 0.025, max_docfreq = 0.975, docfreq_type = "prop")

#take a look at the dfm.
head(dfm_sort(wh_dfm_trim, decreasing =TRUE, margin = "both"), n = 10, nf = 10)

#create tf-idf (term-frequency-inverse-ducument-frequency), which is often used as a weighting factor in search for important words to a document in a collection or corpus. (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
#you can skip this step for now.
titles_tfidf <- dfm_tfidf(wh_dfm_trim)

#convert the dfm to a data frame. We can convert the trimmed dfm or the raw dfm.
wh_dataframe <- convert(wh_dfm, to = "data.frame", docid_field = "doc_id")

#add the document-level variables to the data frame
ready_to_use <- merge(x = wh_dataframe, y = data, by = "doc_id", all = TRUE)
```

## Exploring the Data

```{r initial look}
# remove the non-binary data from original data set (what existed before pre-processing)
drop_cols <- c("text.y", "V1", "id.y", "title", "url", "seendate", "socialimage", "domain.y", "language", "sourcecountry", "Index", "Article_text")
ready_to_use_NoOriginalData <- ready_to_use[ , !(names(ready_to_use) %in% drop_cols)]

# Check the data dimension
dim(ready_to_use_NoOriginalData)

# See some of the variable names
names(ready_to_use_NoOriginalData)[0:50]

# Distribution of the response
table(ready_to_use_NoOriginalData$related)

# Sample proportion
table(ready_to_use_NoOriginalData$related) / nrow(ready_to_use)

#Below, we look to see what proportion of "related" articles have a given word in their title, what proportion of "non-related" articles have a word in their title, and the words with largest absolute difference between those two groups. These words might be most useful as features.
##```{r examining predictors}
## proportion of related articles that include words in title
related_cases <- ready_to_use_NoOriginalData[ready_to_use_NoOriginalData$related == 1, ][,-1]
related_cases <- related_cases[ , which(colnames(related_cases) != 'related')]

related_cases_means <- colMeans(related_cases)
related_cases_means[1:10]

## proportion of non-related articles that include words in title
non_related_cases <- ready_to_use_NoOriginalData[ready_to_use_NoOriginalData$related == 0, ][,-1]
non_related_cases <- non_related_cases[ , which(colnames(non_related_cases) != 'related')]

non_related_cases_means <- colMeans(non_related_cases)
non_related_cases_means[1:10]

## Absolute difference between them
abs_diff_means <- abs(related_cases_means - non_related_cases_means)

## Round and sort the differences
sorted_diffs <- round(sort(abs_diff_means, decreasing = TRUE), digits = 4)
head(sorted_diffs, 10)

##Plot section by Haihang Wu
# difference between related case and non relative case
diff_means <- related_cases_means - non_related_cases_means
round(sort(diff_means, decreasing = TRUE), digits = 4)
head(round(sort(diff_means, decreasing = TRUE), digits = 4), 10)
# visualize the differences
plot(sort(diff_means, decreasing = TRUE), xlab = "", ylab = "related-unrelated",
main = "Difference in average percentage")
abline(h=0, lty=2)
plot(sorted_diffs, xlab = "", ylab = "related-unrelated",
main = "Absolute Difference in average percentage")
abline(h=0, lty=2)

#We will consider the top 50 predictors in absolute difference value for the following analysis; we draw box plot to visualize it
#top 1-5 variable that has positive effect in related
par(mfrow = c(1,5))
boxplot(trump ~ related, main = "trump", data = ready_to_use_NoOriginalData)
boxplot(presid  ~ related, main = "presid", data = ready_to_use_NoOriginalData)
boxplot(hous ~ related, main = "hous", data = ready_to_use_NoOriginalData)
boxplot(white ~ related, main = "white", data = ready_to_use_NoOriginalData)
boxplot(american ~ related, main = "american", data = ready_to_use_NoOriginalData)

dev.new()
#top 5-10 variable that has positive effect in related; smallest difference value is 0.0238
par(mfrow = c(1,5))
boxplot(us ~ related, main = "us", data = ready_to_use_NoOriginalData)
boxplot(fauci ~ related, main = "fauci", data = ready_to_use_NoOriginalData)
boxplot(test ~ related, main = "test", data = ready_to_use_NoOriginalData)
boxplot(said ~ related, main = "said", data = ready_to_use_NoOriginalData)
boxplot(administr ~ related, main = "administr", data = ready_to_use_NoOriginalData)

dev.new()
#top 1-5 variable that has negative effect in related;
par(mfrow = c(1,5))
boxplot(state ~ related, main = "state", data = ready_to_use_NoOriginalData)
boxplot(counti ~ related, main = "counti", data = ready_to_use_NoOriginalData)
boxplot(citi ~ related, main = "citi", data = ready_to_use_NoOriginalData)
boxplot(order ~ related, main = "order", data = ready_to_use_NoOriginalData)
boxplot(case ~ related, main = "case", data = ready_to_use_NoOriginalData)

dev.new()
#top 5-10 variable that has negative effect in related; smallest difference value is -0.0199
par(mfrow = c(1,5))
boxplot(new ~ related, main = "new", data = ready_to_use_NoOriginalData)
boxplot(gov ~ related, main = "gov", data = ready_to_use_NoOriginalData)
boxplot(home ~ related, main = "home", data = ready_to_use_NoOriginalData)
boxplot(cavuto ~ related, main = "cavuto", data = ready_to_use_NoOriginalData)
boxplot(polic ~ related, main = "polic", data = ready_to_use_NoOriginalData)

```

```{r PCA}
###Archit's changes.
##Just replotting the box plots here to visualize the previous plots. Seemed to be some problem and i wasn't able to visualize all of them. 

# Analyze the 20 most influential variables, by plotting their distributions and performing a PCA. 
influential_variables <- names(sorted_diffs)[1:20]

dev.new()
par(mfrow = c(1,3), cex=0.8, mar=c(3,3,0.5,0.5) )

#Strong positive
boxplot(trump ~ related, main = "trump", data = ready_to_use_NoOriginalData)
boxplot(coronavirus ~ related, main = "presid", data = ready_to_use_NoOriginalData)
boxplot(respons ~ related, main = "hous", data = ready_to_use_NoOriginalData)
boxplot(fauci ~ related, main = "white", data = ready_to_use_NoOriginalData)
boxplot(check ~ related, main = "american", data = ready_to_use_NoOriginalData)

#Mild positive.
boxplot(white ~ related, main = "us", data = ready_to_use_NoOriginalData)
boxplot(hous ~ related, main = "fauci", data = ready_to_use_NoOriginalData)
boxplot(brief ~ related, main = "test", data = ready_to_use_NoOriginalData)
boxplot(fact ~ related, main = "said", data = ready_to_use_NoOriginalData)
boxplot(dr ~ related, main = "administr", data = ready_to_use_NoOriginalData)

#mild negative
boxplot(state ~ related, main = "state", data = ready_to_use_NoOriginalData)
boxplot(case ~ related, main = "counti", data = ready_to_use_NoOriginalData)
boxplot(quarantin ~ related, main = "citi", data = ready_to_use_NoOriginalData)
boxplot(close ~ related, main = "order", data = ready_to_use_NoOriginalData)
boxplot(home ~ related, main = "case", data = ready_to_use_NoOriginalData)

#Strong negative.
boxplot(senat ~ related, main = "new", data = ready_to_use_NoOriginalData)
boxplot(covid ~ related, main = "gov", data = ready_to_use_NoOriginalData)
boxplot(around ~ related, main = "home", data = ready_to_use_NoOriginalData)
boxplot(cuomo ~ related, main = "cavuto", data = ready_to_use_NoOriginalData)
boxplot(south ~ related, main = "polic", data = ready_to_use_NoOriginalData)
```
\
From the foregoing plots, we  can see that most variables are taking values between 0 and 1. Most of them don't show appreciable differenes between the means of the two groups. 

This might be because we are just using words from the title, which might too small a text for sentiment analysis. \

```{r PCA, fig.show="hold", out.width=0.5}
#pr <- prcomp(ready_to_use_NoOriginalData[, influential_variables], center=TRUE, #scale.=TRUE)
pr <- prcomp(ready_to_use_NoOriginalData[, names(sorted_diffs)[1:50]], center=TRUE, scale.=TRUE, rank=5)
pr$sdev
pr$sdev^2 / sum(pr$sdev^2) ## To show the percetntages
pr$rotation
dev.new()

par(mfrow = c(1,3), cex=0.8, mar=c(4,4,0.5,0.5) )

plot(pr$x[,1], pr$x[,2], xlab="PC-1", ylab="PC-2")
```

From the PCA, we can see that the variances for the first 5 components represent > 50% of the data, but those from the other components cannot be neglected. \
Also, it is, in general, hard to draw inferences about the first few components from the words as features. The first component, however, reprensents the sum of the features associated with all the words, while the second component represents the differenes in the features associated with America-specific words such as "trump", "presid", "hous", "white", "fauci" and more general words like "state", "citi", "coronavirus", "virus", "public".   

