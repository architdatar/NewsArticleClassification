\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{biblatex}
%% Language and font encodings
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{csquotes}
%%\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241} 

%% Useful packages
\usepackage{amsmath}\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{enumitem}
\usepackage{mleftright}
\usepackage{setspace}
\usepackage{wrapfig}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }

\usepackage{natbib}
\bibliographystyle{unsrtnat}


\title{\Huge News Article Classification}
\author{Lauren Contard, Archit Datar \\ 
Yue Li, Robert Lumpkin, Haihang Wu}
\date{}

\addbibresource{FinalReport.bib}
\doublespacing

\begin{document}

\begin{singlespace}
    \maketitle
\end{singlespace}


\section{Introduction and Problem Statement}

\section{Methods}

\subsection{ML-kNN}
ML-kNN (Multi-label k nearest neighbors) is derived from the traditional k nearest neighbors (kNN), except for the multi-label case. While the goal of the traditional kNN algorithm is to predict whether class of the test sample based on the classes of its k nearest neighbors, the goal of ML-kNN is to predict multiple classes based on the classes of the k nearest neighbors of the test point. For the unseen data point, its nearest neighbors are identified. Then, based on the number of neighboring instances belonging to each possible class, maximum a posteriori (MAP) principle is utilized to determine the label set for the unseen instance. 

ML-kNN is used in a variety of problems, such as, text categorization \autocite{McCallum99multi-labeltext}, where each document may belong to several topics, such as the use case for our project. Apart from this, it can also be useful in areas such as functional genomics where each gene may be associated with a set of functional classes \autocite{KernelMulti-labelClassification}, and in image classification, where each image could have multiple genres.\autocite{Boutell04learningmulti-label}  

%%Questions: 1. How were $P(H_1) and P(E | H) derived? Wasn't able to understand very well. 2. What would be wrong with simply considering this as Q separate classification problems? What exactly do we mean by "correlations between labels" and how do they manifest themselves mathematically?$ 
%%Comments: Somehow, the order of citations is reversed. Not sure why that is happening. Probably due to some argument of BibLatex? 

\subsection{Linear Dimension Reduction (PCA)}

\subsection{Nonlinear Dimension Reduction (ANN Autoencoder)}

In addition to reductions in dimension due to PCA, we also implement an ANN autoencoder. Autoencoders can learn data projections with suitable dimensionality and sparsity limitations that are more useful than other fundamental methods such as PCA, which only allow for linear data representations \autocite{Alkhayrat}.

\begin{wrapfigure}{r}{0.5\textwidth}
    \begin{center}
        \includegraphics[width=0.48\textwidth]{autoencoder_diagram.png}
    \end{center}
\end{wrapfigure}

This nonlinear dimension reduction is done by by training a feed forward neural
network to perform the identity mapping, where the network inputs are reproduced
at the output layer. The network contains an internal “bottleneck” layer (containing fewer nodes than input or output layers), which forces the network to develop a compact representation of the input data, and two additional hidden layers \autocite{Kramer}. Look to the diagram to the right, for a visualisation of this set-up. 

The particular network that we trained had three hidden layers, as in the diagram. The first, second and third hidden layers are of dimensions $128$, $64$, and $128$, and use the activations tanh, ReLu, and sigmoid, respectively. Training was performed using Adam optimization, MSE loss, and over $400$ epochs. After training, the generated encodings were used to repeat our model fitting procedures for both the binary-relevance KNN and ML-KNN algorithms. 


\subsection{Artificial Neural Networks (Feed-Forward \& Recurrent)}

\section{Results}

\subsection{ML-KNN Results}

\subsection{Artificial Neural Network Results}

\section{Discussion \& Conclusions}

\newpage
\printbibliography

\end{document}

