\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{biblatex}
%% Language and font encodings
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{csquotes}
%%\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241} 

%% Useful packages
\usepackage{amsmath}\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{enumitem}
\usepackage{mleftright}
\usepackage{setspace}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }



\title{\Huge News Article Classification}
\author{Lauren Contard, Archit Datar \\ 
Yue Li, Robert Lumpkin, Haihang Wu}
\date{}

\addbibresource{FinalReport.bib}
\doublespacing

\begin{document}

\begin{singlespace}
    \maketitle
\end{singlespace}


\section{Introduction and Problem Statement}

\section{Methods}

\subsection{ML-kNN}
ML-kNN (Multi-label k nearest neighbors) is derived from the traditional k nearest neighbors (kNN), except for the multi-label case. While the goal of the traditional kNN algorithm is to predict whether class of the test sample based on the classes of its k nearest neighbors, the goal of ML-kNN is to predict multiple classes based on the classes of the k nearest neighbors of the test point. For the unseen data point, its nearest neighbors are identified. Then, based on the number of neighboring instances belonging to each possible class, maximum a posteriori (MAP) principle is utilized to determine the label set for the unseen instance. 

ML-kNN is used in a variety of problems, such as, text categorization \autocite{McCallum99multi-labeltext}, where each document may belong to several topics, such as the use case for our project. Apart from this, it can also be useful in areas such as functional genomics where each gene may be associated with a set of functional classes \autocite{KernelMulti-labelClassification}, and in image classification, where each image could have multiple genres.\autocite{Boutell04learningmulti-label}  

%%Questions: 1. How were $P(H_1) and P(E | H) derived? Wasn't able to understand very well. 2. What would be wrong with simply considering this as Q separate classification problems? What exactly do we mean by "correlations between labels" and how do they manifest themselves mathematically?$ 
%%Comments: Somehow, the order of citations is reversed. Not sure why that is happening. Probably due to some argument of BibLatex? 

\subsection{Linear Dimension Reduction (PCA)}

\subsection{Nonlinear Dimension Reduction (ANN Autoencoder)}

\subsection{Artificial Neural Networks (Feed-Forward \& Recurrent)}

\section{Results}

\subsection{ML-KNN Results}

\subsection{Artificial Neural Network Results}

\section{Discussion \& Conclusions}

\newpage
\printbibliography

\end{document}

