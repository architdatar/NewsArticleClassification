@INPROCEEDINGS{McCallum99multi-labeltext,
    author = {Andrew Kachites McCallum},
    title = {Multi-label text classification with a mixture model trained by EM},
    booktitle = { AAAI 99 Workshop on Text Learning},
    year = {1999},
    publisher = {}
}
@inproceedings{KernelMulti-labelClassification,
	author = {Elisseeff, Andr\'{e} and Weston, Jason},
	title = {A Kernel Method for Multi-Labelled Classification},
	year = {2001},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	abstract = {This article presents a Support Vector Machine (SVM) like learning system to handle multi-label problems. Such problems are usually decomposed into many two-class problems but the expressive power of such a system can be weak [5, 7]. We explore a new direct approach. It is based on a large margin ranking system that shares a lot of common properties with SVMs. We tested it on a Yeast gene functional classification problem with positive results.},
	booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
	pages = {681â€“687},
	numpages = {7},
	location = {Vancouver, British Columbia, Canada},
	series = {NIPS'01}
}
@MISC{Boutell04learningmulti-label,
	author = {Matthew R. Boutell and Jiebo Luo and Xipeng Shen and Christopher M. Brown},
	title = {Learning multi-label scene classification },
	year = {2004}
}
@article{ZhangMulti-labelLazy,
	title = {ML-KNN: A lazy learning approach to multi-label learning},
	journal = {Pattern Recognition},
	volume = {40},
	number = {7},
	pages = {2038-2048},
	year = {2007},
	issn = {0031-3203},
	doi = {https://doi.org/10.1016/j.patcog.2006.12.019},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320307000027},
	author = {Min-Ling Zhang and Zhi-Hua Zhou},
	keywords = {Machine learning, Multi-label learning, Lazy learning, -nearest neighbor, Functional genomics, Natural scene classification, Text categorization},
	abstract = {Multi-label learning originated from the investigation of text categorization problem, where each document may belong to several predefined topics simultaneously. In multi-label learning, the training set is composed of instances each associated with a set of labels, and the task is to predict the label sets of unseen instances through analyzing training instances with known label sets. In this paper, a multi-label lazy learning approach named ML-KNN is presented, which is derived from the traditional K-nearest neighbor (KNN) algorithm. In detail, for each unseen instance, its K nearest neighbors in the training set are firstly identified. After that, based on statistical information gained from the label sets of these neighboring instances, i.e. the number of neighboring instances belonging to each possible class, maximum a posteriori (MAP) principle is utilized to determine the label set for the unseen instance. Experiments on three different real-world multi-label learning problems, i.e. Yeast gene functional analysis, natural scene classification and automatic web page categorization, show that ML-KNN achieves superior performance to some well-established multi-label learning algorithms.}
}